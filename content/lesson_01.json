{
  "id": "01",
  "title": "Lesson 01: The Deep Learning Revolution",
  "lesson_title": "The Deep Learning Revolution",
  "objectives": [
    "Understand the relationship between machine learning, AI, and deep learning",
    "Distinguish between supervised, unsupervised, and self-supervised learning",
    "Understand classification vs regression problems",
    "Learn the concepts of overfitting, regularization, and model selection",
    "Understand the historical development of neural networks"
  ],
  "cards": [
    {
      "uid": "deep-learning-foundations-and-concepts-01-001",
      "front": "What is the relationship between the terms 'machine learning' and 'AI'?",
      "back": "The terms machine learning and AI are often used interchangeably. Many AI systems in current use represent applications of machine learning designed to solve specific problems.",
      "tags": [
        "ch01",
        "terminology",
        "definition"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-002",
      "front": "What are the adjustable parameters in a deep neural network called?",
      "back": "They are called <b>weights</b>.",
      "tags": [
        "ch01",
        "neural-networks",
        "terminology"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-003",
      "front": "What is the process of setting parameter values from data called in machine learning?",
      "back": "It is called <b>learning</b> or <b>training</b>.",
      "tags": [
        "ch01",
        "training",
        "definition"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-004",
      "front": "What is a classification problem?",
      "back": "A <b>classification problem</b> is one where each input must be assigned to a discrete set of classes (e.g., benign or malignant).",
      "tags": [
        "ch01",
        "classification",
        "supervised-learning"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-005",
      "front": "What is a regression problem?",
      "back": "A <b>regression problem</b> is one where the output consists of one or more continuous variables (e.g., predicting the yield in a chemical process given temperature and pressure).",
      "tags": [
        "ch01",
        "regression",
        "supervised-learning"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-006",
      "front": "What is supervised learning?",
      "back": "<b>Supervised learning</b> is when the network is told the correct label for each training example. The training data consists of input-output pairs.",
      "tags": [
        "ch01",
        "supervised-learning",
        "definition"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-007",
      "front": "What is unsupervised learning?",
      "back": "<b>Unsupervised learning</b> is when the training data consists of unlabelled examples, and the goal is to discover structure or patterns in the data (e.g., generating new images that share statistical properties with training images).",
      "tags": [
        "ch01",
        "unsupervised-learning",
        "definition"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-008",
      "front": "What is self-supervised learning?",
      "back": "<b>Self-supervised learning</b> is when a function from inputs to outputs is learned, but the labelled outputs are obtained automatically from the input training data without needing separate human-derived labels.<br>Example: predicting the next word in a sequence, where the target is derived from the text itself.",
      "tags": [
        "ch01",
        "self-supervised-learning",
        "definition"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-009",
      "front": "What is generalization in machine learning?",
      "back": "<b>Generalization</b> is the ability to make accurate predictions on previously unseen inputs. It is a key goal in machine learning.",
      "tags": [
        "ch01",
        "generalization",
        "definition"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-010",
      "front": "What is a generative model?",
      "back": "A <b>generative model</b> can generate new output examples that differ from those used to train the model but share the same statistical properties.",
      "tags": [
        "ch01",
        "generative-models",
        "definition"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-013",
      "front": "What is overfitting?",
      "back": "<b>Overfitting</b> occurs when a model fits the training data very well (possibly exactly) but gives poor predictions on new, unseen data. The model has essentially memorized the noise in the training data rather than learning the underlying pattern.",
      "tags": [
        "ch01",
        "overfitting",
        "generalization"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-014",
      "front": "What is a test set used for?",
      "back": "A <b>test set</b> is a separate set of data used to evaluate how well a model generalizes to new data. It measures the model's predictive performance on data not used during training.",
      "tags": [
        "ch01",
        "test-set",
        "evaluation"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-020",
      "front": "What is a hyperparameter?",
      "back": "A <b>hyperparameter</b> is a parameter whose value is fixed during training (not learned from data). Examples include:<br><ul><li>The regularization coefficient \\( \\lambda \\)</li><li>The order \\( M \\) of a polynomial</li><li>Learning rate</li></ul>",
      "tags": [
        "ch01",
        "hyperparameters",
        "definition"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-021",
      "front": "What is the difference between a training set and a validation set?",
      "back": "<ul><li><b>Training set</b>: Used to determine the model parameters (weights)</li><li><b>Validation set</b> (also called hold-out or development set): Used to select hyperparameters and model architecture</li></ul>A third <b>test set</b> may be kept aside for final evaluation.",
      "tags": [
        "ch01",
        "data-splits",
        "model-selection"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-022",
      "front": "What is S-fold cross-validation?",
      "back": "<b>S-fold cross-validation</b> partitions data into S groups. For each run:<br><ul><li>Train on S-1 groups</li><li>Evaluate on the remaining group</li></ul>Repeat for all S choices of held-out group and average the scores. This uses (S-1)/S of data for training while evaluating on all data.",
      "tags": [
        "ch01",
        "cross-validation",
        "model-selection"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-023",
      "front": "What is the perceptron and what is its historical significance?",
      "back": "The <b>perceptron</b> (Rosenblatt, 1962) is a single-layer neural network with a step activation function:<br>\\( f(a) = \\begin{cases} 0, & \\text{if } a \\leq 0 \\\\ 1, & \\text{if } a > 0 \\end{cases} \\)<br>It was one of the first trainable neural network models, but Minsky and Papert (1969) gave formal proofs of its limited capabilities.",
      "tags": [
        "ch01",
        "perceptron",
        "history"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-024",
      "front": "What is a multilayer perceptron (MLP)?",
      "back": "A <b>multilayer perceptron (MLP)</b> is a modern neural network with multiple layers of learnable parameters. Despite the name, it uses continuous differentiable activation functions (not step functions like the original perceptron).",
      "tags": [
        "ch01",
        "mlp",
        "neural-networks"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-025",
      "front": "What two key changes enabled training of multilayer neural networks?",
      "back": "<ol><li>Replace the step function with <b>continuous differentiable activation functions</b> having non-zero gradients</li><li>Introduce <b>differentiable error functions</b> that define how well parameter values predict target variables</li></ol>",
      "tags": [
        "ch01",
        "backpropagation",
        "history"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-028",
      "front": "What is error backpropagation?",
      "back": "<b>Error backpropagation</b> (or just backprop) is an efficient algorithm for computing the derivatives of the error function with respect to all network parameters. Information flows backwards through the network from outputs towards inputs.",
      "tags": [
        "ch01",
        "backpropagation",
        "training"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-029",
      "front": "What is stochastic gradient descent (SGD)?",
      "back": "<b>Stochastic gradient descent</b> is an optimization algorithm that updates parameters iteratively using gradients computed on small subsets (or single examples) of training data, rather than the entire dataset.<br><b>Formula</b>: \\( \\vec{w}^{(\\tau+1)} = \\vec{w}^{(\\tau)} - \\eta \\nabla E_n \\) where \\( \\eta \\) is the learning rate.<br><b>Advantages</b>:<br><ul><li>Can escape local minima (due to noise)</li><li>Efficient for large datasets</li><li>Online learning capability</li></ul>",
      "tags": [
        "ch01",
        "sgd",
        "optimization"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-031",
      "front": "What is feature extraction in traditional machine learning?",
      "back": "<b>Feature extraction</b> is hand-crafted pre-processing that transforms input variables into a new space where the machine learning problem is easier to solve. Deep learning largely eliminates the need for this by learning features directly from data.",
      "tags": [
        "ch01",
        "feature-extraction",
        "history"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-032",
      "front": "What are deep neural networks?",
      "back": "<b>Deep neural networks</b> are networks with many layers of learnable weights. The sub-field of machine learning focusing on such networks is called <b>deep learning</b>.",
      "tags": [
        "ch01",
        "deep-learning",
        "definition"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-033",
      "front": "What role do GPUs play in deep learning?",
      "back": "<b>GPUs (Graphics Processing Units)</b> are specialist processors originally developed for graphics rendering. They are well-suited for training neural networks because units in one layer can be evaluated in parallel, mapping well onto the massive parallelism of GPUs.",
      "tags": [
        "ch01",
        "gpu",
        "hardware"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-034",
      "front": "How did compute requirements for state-of-the-art neural networks change before and after 2012?",
      "back": "<ul><li><b>Before 2012 (perceptron era)</b>: Compute doubled roughly every 2 years (following Moore's law)</li><li><b>After 2012 (deep learning era)</b>: Doubling time reduced to 3.4 months (10x increase per year)</li></ul>",
      "tags": [
        "ch01",
        "compute",
        "history"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-035",
      "front": "What is representation learning?",
      "back": "<b>Representation learning</b> views hidden layers as transforming input data into new representations that are semantically meaningful, creating an easier problem for final layers to solve. These learned representations can be repurposed for related problems via transfer learning.",
      "tags": [
        "ch01",
        "representation-learning",
        "deep-learning"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-039",
      "front": "How many neurons and synapses does a human brain contain?",
      "back": "A human brain contains:<br><ul><li>Around <b>90 billion neurons</b></li><li>Each neuron has on average several thousand synapses</li><li>Total of around <b>100 trillion (10^14) synapses</b></li></ul>",
      "tags": [
        "ch01",
        "neuroscience",
        "background"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-040",
      "front": "What is the mathematical model of a single artificial neuron?",
      "back": "Pre-activation: \\( a = \\sum_{i=1}^{M} w_i x_i \\)<br>Activation: \\( y = f(a) \\)<br>where:<ul><li>\\( x_1, \\ldots, x_M \\) are inputs</li><li>\\( w_1, \\ldots, w_M \\) are weights (synapse strengths)</li><li>\\( f(\\cdot) \\) is the activation function</li></ul>",
      "tags": [
        "ch01",
        "neural-networks",
        "mathematics"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-042",
      "front": "Why can't we determine hyperparameters by minimizing the training error jointly with model parameters?",
      "back": "Minimizing training error jointly would lead to extreme values:<br><ul><li>Regularization \\( \\lambda \\to 0 \\) (no regularization)</li><li>Polynomial order \\( M \\to \\) large values</li></ul>Both result in overfitting with small/zero training error but poor generalization.",
      "tags": [
        "ch01",
        "hyperparameters",
        "overfitting"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-043",
      "front": "What is data leakage in machine learning?",
      "back": "<b>Data leakage</b> is when information that would not be available at prediction time (often from the validation/test set, or from the future) influences training.<br><br><b>Why it matters</b>: It makes evaluation metrics look much better than real-world performance.<br><br><b>Common leak patterns</b>:<ul><li><b>Preprocessing leakage</b>: Fitting normalization/feature selection on all data (train+val+test) instead of training data only</li><li><b>Target leakage</b>: A feature directly/indirectly encodes the label (e.g., using \"refund issued\" when predicting fraud)</li><li><b>Temporal leakage</b>: Using future information when predicting the past</li></ul><b>Fix</b>: Fit every transform strictly on the training split, then apply to validation/test.",
      "tags": [
        "ch01",
        "data-leakage",
        "evaluation"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-044",
      "front": "What is dataset shift (distribution shift) and what are the main types?",
      "back": "<b>Dataset shift</b> means the data distribution changes between training and deployment, so training performance no longer predicts real-world performance.<br><br><b>Common types</b>:<ul><li><b>Covariate shift</b>: \\( p(\\vec{x}) \\) changes but \\( p(y|\\vec{x}) \\) stays the same (inputs look different)</li><li><b>Label shift</b>: \\( p(y) \\) changes but \\( p(\\vec{x}|y) \\) stays the same (class proportions change)</li><li><b>Concept drift</b>: \\( p(y|\\vec{x}) \\) changes (the underlying relationship changes)</li></ul>",
      "tags": [
        "ch01",
        "dataset-shift",
        "generalization"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-045",
      "front": "When is a random train/validation/test split a bad idea?",
      "back": "Random splitting is risky when it breaks the assumptions of independence between splits.<br><br><b>Common cases</b>:<ul><li><b>Time series</b>: Random split causes temporal leakage; prefer chronological splits</li><li><b>Grouped data</b> (e.g., multiple samples per user/patient): Random split leaks identity; use group-based splits</li><li><b>Near-duplicates</b> (e.g., similar frames/images): Duplicates across splits inflate metrics; deduplicate or split by source</li></ul>",
      "tags": [
        "ch01",
        "data-splits",
        "evaluation"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-046",
      "front": "What is the correct role of the test set in model development?",
      "back": "The <b>test set</b> should be used <b>only once</b>, at the end, to estimate generalization after all modeling decisions are fixed.<br><br><b>Rule</b>: If you look at test performance and then change anything (features, preprocessing, architecture, hyperparameters), the test set has become part of model selection and your evaluation is biased.<br><br>Use <b>validation</b> (or cross-validation) for all choices; reserve the test set for the final report.",
      "tags": [
        "ch01",
        "test-set",
        "model-selection"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-047",
      "front": "What is a stratified split and when should you use it?",
      "back": "A <b>stratified split</b> preserves the class proportions across train/validation/test splits.<br><br><b>When to use</b>: In classification, especially with <b>imbalanced classes</b>, stratification reduces the chance that one split has too few positives (which makes metrics noisy and misleading).",
      "tags": [
        "ch01",
        "data-splits",
        "imbalanced-classes"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-01-055",
      "front": "What is the difference between causal and predictive modeling?",
      "back": "<b>Predictive modeling</b>: Find \\( P(Y|X) \\) to predict Y from X.<ul><li>Correlation is sufficient</li><li>Spurious correlations can help prediction</li><li>Fails under distribution shift</li></ul><b>Causal modeling</b>: Understand how interventions on X affect Y.<ul><li>Requires \\( P(Y|do(X)) \\), not just \\( P(Y|X) \\)</li><li>Robust to certain distribution shifts</li><li>Enables answering counterfactual questions</li></ul><b>Key distinction</b>: Prediction asks 'what will Y be if I observe X?'. Causation asks 'what will Y be if I <i>set</i> X?'<br><br><b>Testing causality from observational data</b>:<ul><li>Randomized experiments (gold standard)</li><li>Instrumental variables</li><li>Regression discontinuity</li><li>Difference-in-differences</li><li>Causal discovery algorithms (PC, FCI)</li></ul><b>Challenges</b>: Confounders, selection bias, unobserved variables. Observational data alone rarely proves causation without strong assumptions.",
      "tags": [
        "ch01",
        "causality",
        "prediction"
      ]
    }
  ]
}
