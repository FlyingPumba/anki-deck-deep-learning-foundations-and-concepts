{
  "id": "06",
  "title": "Lesson 06: Deep Neural Networks",
  "lesson_title": "Deep Neural Networks",
  "objectives": [
    "Understand feed-forward network architecture",
    "Learn about error functions and output activations",
    "Master gradient-based optimization: batch, SGD, mini-batch",
    "Understand weight initialization and symmetry breaking",
    "Learn optimization algorithms: momentum, RMSProp, Adam",
    "Understand normalization techniques: input, batch, layer normalization"
  ],
  "cards": [
    {
      "uid": "06-001",
      "front": "What is few-shot learning?",
      "back": "Learning from only a few labelled examples per class.<br>When only a <b>single labelled example</b> is available per class, it's called <b>one-shot learning</b>.<br>Requires models that can generalize from very limited data.",
      "tags": [
        "ch06",
        "few-shot",
        "learning-paradigms"
      ]
    },
    {
      "uid": "06-003",
      "front": "What is a feed-forward architecture in neural networks?",
      "back": "A network where information flows in <b>one direction only</b>: from input to output, with no cycles or loops.<br>Connections go from layer \\( l \\) to layer \\( l+1 \\), never backwards.<br>Contrast with recurrent networks that have feedback connections.",
      "tags": [
        "ch06",
        "feed-forward",
        "architecture"
      ]
    },
    {
      "uid": "06-004",
      "front": "What are tensors in machine learning?",
      "back": "Higher-dimensional arrays:<br><ul><li>Scalar: 0D tensor (single number)</li><li>Vector: 1D tensor</li><li>Matrix: 2D tensor</li><li>3D+ arrays: higher-order tensors</li></ul>Used to represent batches of data, images (height x width x channels), etc.",
      "tags": [
        "ch06",
        "tensors",
        "data-structures"
      ]
    },
    {
      "uid": "06-005",
      "front": "In machine learning, do we typically maximize likelihood or minimize a loss?",
      "back": "We typically <b>minimize a loss/error function</b>.<br>The error function is the <b>negative log likelihood</b>.<br>Maximizing likelihood = Minimizing negative log likelihood.<br>This convention aligns with optimization literature.",
      "tags": [
        "ch06",
        "loss-function",
        "optimization"
      ]
    },
    {
      "uid": "06-006",
      "front": "What error function matches Gaussian noise assumption in regression?",
      "back": "<b>Sum-of-squares error</b>:<br>\\( E = \\frac{1}{2}\\sum_{n=1}^{N}(y_n - t_n)^2 \\)<br>Maximizing likelihood under Gaussian noise is equivalent to minimizing sum-of-squares.<br>Use with <b>linear output activations</b>.",
      "tags": [
        "ch06",
        "regression",
        "error-function"
      ]
    },
    {
      "uid": "06-007",
      "front": "What error function should be used for classification problems?",
      "back": "<b>Cross-entropy error</b>:<br>\\( E = -\\sum_{n=1}^{N}\\sum_{k=1}^{K} t_{nk} \\ln y_{nk} \\)<br>Using cross-entropy instead of sum-of-squares for classification leads to <b>faster training</b> and <b>better generalization</b>.",
      "tags": [
        "ch06",
        "classification",
        "cross-entropy"
      ]
    },
    {
      "uid": "06-008",
      "front": "What is the natural pairing of output activation and error function for different tasks?",
      "back": "<b>Regression</b>: Linear outputs + sum-of-squares error<br><b>Binary classification</b>: Sigmoid output + binary cross-entropy<br><b>Multi-class classification</b>: Softmax outputs + categorical cross-entropy<br>These pairings come from maximum likelihood under appropriate distributions.",
      "tags": [
        "ch06",
        "output-activation",
        "error-function"
      ]
    },
    {
      "uid": "06-009",
      "front": "What is a mixture density network?",
      "back": "A neural network that outputs parameters of a <b>mixture model</b> (e.g., Gaussian mixture).<br>Outputs: mixing coefficients \\( \\pi_k \\), means \\( \\mu_k \\), variances \\( \\sigma_k^2 \\)<br>Useful for multi-modal distributions where a single prediction is insufficient.",
      "tags": [
        "ch06",
        "mixture-density",
        "probabilistic"
      ]
    },
    {
      "uid": "06-010",
      "front": "Why can neural network error functions be optimized efficiently?",
      "back": "Because:<br><ol><li>The network function is <b>differentiable by design</b> (smooth activation functions)</li><li>The error function is also <b>differentiable</b></li><li><b>Backpropagation</b> efficiently computes gradients</li></ol>This enables gradient-based optimization.",
      "tags": [
        "ch06",
        "differentiable",
        "optimization"
      ]
    },
    {
      "uid": "06-011",
      "front": "What is the goal when training a neural network?",
      "back": "Find weights and biases that achieve <b>good generalization</b> on unseen data.<br>Not just minimizing training error, but finding parameters that work well on new inputs.<br>This typically involves finding a point where \\( \\nabla E(\\vec{w}) = 0 \\).",
      "tags": [
        "ch06",
        "training",
        "generalization"
      ]
    },
    {
      "uid": "06-012",
      "front": "What are stationary points in optimization?",
      "back": "Points where the gradient vanishes: \\( \\nabla E(\\vec{w}) = 0 \\)<br>Types of stationary points:<br><ul><li><b>Local minima</b>: Lower than all nearby points</li><li><b>Local maxima</b>: Higher than all nearby points</li><li><b>Saddle points</b>: Minimum in some directions, maximum in others</li></ul>",
      "tags": [
        "ch06",
        "stationary-points",
        "optimization"
      ]
    },
    {
      "uid": "06-013",
      "front": "What is the difference between global and local minima?",
      "back": "<b>Global minimum</b>: The smallest value of the error function across the entire weight space.<br><b>Local minimum</b>: A point lower than all nearby points, but not necessarily the lowest overall.<br>Neural networks typically have many local minima.",
      "tags": [
        "ch06",
        "minima",
        "optimization"
      ]
    },
    {
      "uid": "06-046",
      "front": "What is the Hessian matrix?",
      "back": "The <b>Hessian</b> is the matrix of second derivatives of a function:<br>\\( H_{ij} = \\frac{\\partial^2 E}{\\partial w_i \\partial w_j} \\)<br><b>Intuition</b>: Analogous to the second derivative \\( f''(x) \\) in single-variable calculus, but for multiple dimensions. While the gradient tells you the slope, the Hessian tells you the <i>curvature</i> - how the slope is changing.<br><b>Uses</b>:<ul><li>Determines if a stationary point is a min, max, or saddle</li><li>Enables second-order optimization (Newton's method)</li></ul>",
      "tags": [
        "ch06",
        "hessian",
        "optimization"
      ]
    },
    {
      "uid": "06-014",
      "front": "When is a matrix H positive definite?",
      "back": "When \\( \\vec{v}^T H \\vec{v} > 0 \\) for all non-zero vectors \\( \\vec{v} \\).<br>Equivalently: all eigenvalues are positive.<br>At a local minimum, the Hessian matrix must be positive definite.",
      "tags": [
        "ch06",
        "positive-definite",
        "hessian"
      ]
    },
    {
      "uid": "06-015",
      "front": "What conditions must hold for a point to be a local minimum?",
      "back": "Necessary and sufficient conditions:<br><ol><li>Gradient is zero: \\( \\nabla E(\\vec{w}^*) = 0 \\)</li><li>Hessian is <b>positive definite</b>: all eigenvalues > 0</li></ol>If the Hessian has negative eigenvalues, it's a saddle point or maximum.",
      "tags": [
        "ch06",
        "local-minimum",
        "conditions"
      ]
    },
    {
      "uid": "06-016",
      "front": "Why does the solution found by gradient descent depend on initialization?",
      "back": "Because the error surface has <b>multiple local minima</b>.<br>Gradient descent finds a local minimum near the starting point.<br>Different initializations \\( \\vec{w}^{(0)} \\) lead to different final solutions.",
      "tags": [
        "ch06",
        "initialization",
        "local-minima"
      ]
    },
    {
      "uid": "06-017",
      "front": "Why is gradient information essential for training neural networks?",
      "back": "The gradient \\( \\nabla E \\) is a vector of length W (number of weights).<br>Each evaluation provides <b>W pieces of information</b> about the error surface.<br>This makes gradient-based methods far more efficient than gradient-free approaches.",
      "tags": [
        "ch06",
        "gradient",
        "efficiency"
      ]
    },
    {
      "uid": "06-018",
      "front": "What is the gradient descent update rule?",
      "back": "\\( \\vec{w}^{(\\tau+1)} = \\vec{w}^{(\\tau)} - \\eta \\nabla E(\\vec{w}^{(\\tau)}) \\)<br>Where:<br><ul><li>\\( \\eta > 0 \\) is the <b>learning rate</b></li><li>\\( \\tau \\) is the iteration number</li></ul>Takes a small step in the direction of steepest descent.",
      "tags": [
        "ch06",
        "gradient-descent",
        "update-rule"
      ]
    },
    {
      "uid": "06-019",
      "front": "What are batch methods in optimization?",
      "back": "Methods that use the <b>whole dataset at once</b> to compute the gradient.<br>Advantages: Stable gradient estimates<br>Disadvantages: Computationally expensive for large datasets, requires all data in memory.",
      "tags": [
        "ch06",
        "batch",
        "optimization"
      ]
    },
    {
      "uid": "06-021",
      "front": "What is a training epoch?",
      "back": "A <b>complete pass through the entire training set</b>.<br>In SGD, one epoch = N weight updates (for N data points).<br>Training typically runs for many epochs until convergence.",
      "tags": [
        "ch06",
        "epoch",
        "training"
      ]
    },
    {
      "uid": "06-022",
      "front": "What is online gradient descent?",
      "back": "SGD applied to streaming data where examples arrive one at a time and may not be stored.<br>Useful when:<br><ul><li>Data arrives continuously</li><li>Dataset doesn't fit in memory</li><li>Real-time adaptation is needed</li></ul>",
      "tags": [
        "ch06",
        "online-learning",
        "sgd"
      ]
    },
    {
      "uid": "06-023",
      "front": "How can SGD escape local minima?",
      "back": "The <b>stochasticity</b> (randomness) in gradient estimates acts like noise.<br>This noise can push the optimization out of shallow local minima.<br>Batch gradient descent follows the exact gradient and gets trapped more easily.",
      "tags": [
        "ch06",
        "sgd",
        "local-minima"
      ]
    },
    {
      "uid": "06-024",
      "front": "What is a downside of pure SGD (single-point updates)?",
      "back": "The gradient estimate from one data point is <b>noisy</b> (high variance).<br>This can cause:<br><ul><li>Erratic updates</li><li>Slower convergence</li><li>Difficulty reaching precise minima</li></ul>",
      "tags": [
        "ch06",
        "sgd",
        "variance"
      ]
    },
    {
      "uid": "06-025",
      "front": "What is mini-batch gradient descent?",
      "back": "Uses a <b>small subset of data points</b> (mini-batch) to estimate the gradient:<br>\\( \\nabla E \\approx \\frac{1}{B}\\sum_{n \\in \\text{batch}} \\nabla E_n \\)<br>Balances:<br><ul><li>Computational efficiency (parallelism)</li><li>Gradient accuracy (lower variance than SGD)</li><li>Memory constraints</li></ul>",
      "tags": [
        "ch06",
        "mini-batch",
        "optimization"
      ]
    },
    {
      "uid": "06-026",
      "front": "Why are there diminishing returns from increasing mini-batch size?",
      "back": "Standard error of gradient estimate decreases as \\( 1/\\sqrt{B} \\).<br>Doubling batch size only reduces noise by \\( \\sqrt{2} \\approx 1.4 \\).<br>Computational cost increases linearly while accuracy improves sub-linearly.",
      "tags": [
        "ch06",
        "mini-batch",
        "batch-size"
      ]
    },
    {
      "uid": "06-027",
      "front": "What is symmetry breaking in neural network initialization?",
      "back": "Initializing weights <b>randomly</b> rather than identically.<br>If all weights start the same, all hidden units compute the same function and learn the same features.<br>Random initialization breaks this symmetry, allowing diverse features to emerge.",
      "tags": [
        "ch06",
        "initialization",
        "symmetry-breaking"
      ]
    },
    {
      "uid": "06-028",
      "front": "What is He initialization?",
      "back": "Weight initialization scaled by \\( \\sqrt{2/M} \\) where M is the number of input units:<br>\\( w \\sim N(0, 2/M) \\)<br>Designed for ReLU activations to maintain variance of activations across layers.<br>Prevents vanishing/exploding gradients.",
      "tags": [
        "ch06",
        "he-initialization",
        "initialization"
      ]
    },
    {
      "uid": "06-029",
      "front": "What happens if the learning rate is too large?",
      "back": "<b>Divergent oscillations</b>: steps overshoot the minimum and bounce back and forth with increasing amplitude.<br>The optimization diverges instead of converging.<br>Must keep \\( \\eta \\) small enough to avoid this.",
      "tags": [
        "ch06",
        "learning-rate",
        "divergence"
      ]
    },
    {
      "uid": "06-030",
      "front": "What is the convergence rate of gradient descent near a minimum?",
      "back": "<b>Linear convergence</b>: error decreases by a constant factor per iteration.<br>\\( E^{(\\tau+1)} - E^{*} \\approx c(E^{(\\tau)} - E^{*}) \\)<br>where \\( 0 < c < 1 \\) and \\( E^{*} \\) is the minimum error.<br>Slower than quadratic (Newton's method) but each iteration is cheaper.",
      "tags": [
        "ch06",
        "convergence",
        "gradient-descent"
      ]
    },
    {
      "uid": "06-031",
      "front": "What problem arises with elongated error surfaces (different eigenvalues)?",
      "back": "Gradient descent <b>oscillates</b> across narrow valleys while making slow progress along them.<br>The step size must be small enough for the steepest direction, causing slow convergence in shallow directions.<br>Momentum helps address this.",
      "tags": [
        "ch06",
        "eigenvalues",
        "oscillation"
      ]
    },
    {
      "uid": "06-032",
      "front": "What is momentum in gradient descent?",
      "back": "Adding a fraction of the previous update to the current update:<br>\\( \\vec{v}^{(\\tau+1)} = \\mu \\vec{v}^{(\\tau)} - \\eta \\nabla E \\)<br>\\( \\vec{w}^{(\\tau+1)} = \\vec{w}^{(\\tau)} + \\vec{v}^{(\\tau+1)} \\)<br>Effectively adds <b>inertia</b> to motion through weight space, smoothing out oscillations.",
      "tags": [
        "ch06",
        "momentum",
        "optimization"
      ]
    },
    {
      "uid": "06-033",
      "front": "How does momentum help optimization?",
      "back": "<ol><li><b>Smooths oscillations</b> across valleys</li><li><b>Accelerates</b> along consistent gradient directions</li><li>Leads to <b>faster convergence</b> without needing larger learning rates</li></ol>Momentum accumulates in directions of consistent gradient.",
      "tags": [
        "ch06",
        "momentum",
        "benefits"
      ]
    },
    {
      "uid": "06-034",
      "front": "What is the best strategy for setting the learning rate during training?",
      "back": "Use a <b>larger learning rate at the start</b> and <b>reduce it during training</b>.<br>Early: Large steps for fast initial progress<br>Late: Small steps for fine-tuning near minimum<br>Called learning rate scheduling or annealing.",
      "tags": [
        "ch06",
        "learning-rate",
        "scheduling"
      ]
    },
    {
      "uid": "06-035",
      "front": "What is Root Mean Square Propagation (RMSProp)?",
      "back": "An adaptive learning rate method.<br>Maintains moving average of squared gradients per parameter:<br>\\( v_i^{(\\tau+1)} = \\rho v_i^{(\\tau)} + (1-\\rho)(g_i^{(\\tau)})^2 \\)<br>Scales learning rate: \\( \\Delta w_i = -\\frac{\\eta}{\\sqrt{v_i + \\epsilon}} g_i \\)<br>Gives each parameter its own effective learning rate.",
      "tags": [
        "ch06",
        "rmsprop",
        "adaptive-learning"
      ]
    },
    {
      "uid": "06-036",
      "front": "What is Adam optimization?",
      "back": "<b>Adaptive Moments</b>: Combines RMSProp with momentum.<br>Maintains:<br><ul><li>First moment (momentum): \\( m_i \\) (moving average of gradients)</li><li>Second moment: \\( v_i \\) (moving average of squared gradients)</li></ul><b>Most widely adopted</b> learning algorithm in deep learning.",
      "tags": [
        "ch06",
        "adam",
        "optimization"
      ]
    },
    {
      "uid": "06-037",
      "front": "What are the three kinds of normalization in neural networks?",
      "back": "<b>Intuition</b>: Normalization keeps activations in a 'healthy' range, preventing them from exploding or vanishing as they flow through layers.<br><br><ol><li><b>Input normalization</b>: Rescale each input feature across the dataset to zero mean, unit variance. <i>When</i>: preprocessing step before training.</li><li><b>Batch normalization</b>: For each hidden unit, normalize across examples in the mini-batch. <i>When</i>: during training, uses batch statistics; at inference, uses running averages.</li><li><b>Layer normalization</b>: For each example, normalize across all hidden units in a layer. <i>When</i>: works identically at train and test time, preferred for transformers and small batches.</li></ol><b>Key difference</b>: Batch norm averages over examples (vertical slice), layer norm averages over features (horizontal slice).",
      "tags": [
        "ch06",
        "normalization",
        "types"
      ]
    },
    {
      "uid": "06-038",
      "front": "Why is input normalization beneficial?",
      "back": "When input variables span <b>very different ranges</b>, gradients can be poorly scaled.<br>Re-scaling inputs to have zero mean and unit variance:<br>\\( \\tilde{x}_i = \\frac{x_i - \\mu_i}{\\sigma_i} \\)<br>Makes optimization more efficient and stable.",
      "tags": [
        "ch06",
        "input-normalization",
        "preprocessing"
      ]
    },
    {
      "uid": "06-039",
      "front": "What is batch normalization?",
      "back": "Normalizing hidden unit activations <b>across the mini-batch</b>:<br>\\( \\hat{a}_i = \\frac{a_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\)<br>Then rescale: \\( \\tilde{a}_i = \\gamma_i \\hat{a}_i + \\beta_i \\)<br>where \\( \\gamma_i, \\beta_i \\) are learned parameters.<br>Helps with vanishing/exploding gradients.",
      "tags": [
        "ch06",
        "batch-normalization",
        "normalization"
      ]
    },
    {
      "uid": "06-040",
      "front": "What are vanishing gradients and exploding gradients?",
      "back": "In deep networks, gradients are products of many terms (chain rule).<br><b>Vanishing</b>: Product tends to 0 if terms < 1 - early layers don't learn<br><b>Exploding</b>: Product tends to infinity if terms > 1 - training becomes unstable<br>Batch normalization and careful initialization help prevent both.",
      "tags": [
        "ch06",
        "vanishing-gradients",
        "exploding-gradients"
      ]
    },
    {
      "uid": "06-041",
      "front": "Why does batch normalization include learnable scale and shift parameters?",
      "back": "The parameters \\( \\gamma_i \\) and \\( \\beta_i \\) let the network <b>undo the normalization</b> if needed.<br>Without them, normalization might remove useful information.<br>With them, the network can learn the optimal scaling while still benefiting from normalized gradients.",
      "tags": [
        "ch06",
        "batch-normalization",
        "parameters"
      ]
    },
    {
      "uid": "06-042",
      "front": "How is batch normalization applied at inference time?",
      "back": "Use <b>moving averages</b> of mean and variance computed during training, not the current batch statistics.<br>These moving averages are updated during training:<br>\\( \\mu_{\\text{running}} = \\alpha \\mu_{\\text{running}} + (1-\\alpha)\\mu_B \\)<br>This allows consistent predictions for single examples.",
      "tags": [
        "ch06",
        "batch-normalization",
        "inference"
      ]
    },
    {
      "uid": "06-043",
      "front": "What is layer normalization?",
      "back": "Normalizing <b>across hidden units for each example separately</b> (not across the batch).<br>Advantages over batch normalization:<br><ul><li>Works with any batch size (even size 1)</li><li>Same computation at training and inference</li><li>Useful in transformers and RNNs</li></ul>",
      "tags": [
        "ch06",
        "layer-normalization",
        "normalization"
      ]
    },
    {
      "uid": "06-044",
      "front": "When is layer normalization preferred over batch normalization?",
      "back": "Layer normalization is useful when:<br><ul><li><b>Small batch sizes</b> (batch norm estimates are noisy)</li><li><b>Transformers</b> (standard choice)</li><li><b>Recurrent networks</b> (variable sequence lengths)</li><li><b>Inference</b> on single examples</li></ul>Same normalization function at train and test time.",
      "tags": [
        "ch06",
        "layer-normalization",
        "applications"
      ]
    },
    {
      "uid": "06-045",
      "front": "Should batch normalization be applied before or after the activation function?",
      "back": "We have a choice: normalize <b>pre-activation values</b> \\( a_i \\) or <b>post-activation values</b> \\( z_i \\).<br>Both approaches are used in practice.<br>Original paper normalized before activation, but normalizing after also works well.",
      "tags": [
        "ch06",
        "batch-normalization",
        "placement"
      ]
    }
  ]
}
