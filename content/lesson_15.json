{
  "id": "15",
  "title": "Lesson 15: Discrete Latent Variables",
  "lesson_title": "Discrete Latent Variables",
  "objectives": [
    "Understand K-means clustering algorithm",
    "Learn Gaussian mixture models and their properties",
    "Master the Expectation-Maximization (EM) algorithm",
    "Understand the role of latent variables in mixture models",
    "Learn about the evidence lower bound (ELBO)"
  ],
  "cards": [
    {
      "uid": "15-001",
      "front": "What is K-means clustering?",
      "back": "An algorithm to partition N data points into K clusters.\n\nObjective: Minimize sum of squared distances to cluster centres:\n\n\\( J = \\sum_{n=1}^{N} \\sum_{k=1}^{K} r_{nk} ||\\mathbf{x}_n - \\boldsymbol{\\mu}_k||^2 \\)\n\nwhere \\( r_{nk} \\in \\{0,1\\} \\) indicates cluster assignment.",
      "tags": [
        "ch15",
        "k-means",
        "clustering"
      ]
    },
    {
      "uid": "15-002",
      "front": "What are the two steps of the K-means algorithm?",
      "back": "<b>E step</b>: Assign each point to nearest cluster centre\n\\( r_{nk} = 1 \\) if \\( k = \\arg\\min_j ||\\mathbf{x}_n - \\boldsymbol{\\mu}_j||^2 \\)\n\n<b>M step</b>: Update cluster centres to be mean of assigned points\n\\( \\boldsymbol{\\mu}_k = \\frac{\\sum_n r_{nk} \\mathbf{x}_n}{\\sum_n r_{nk}} \\)\n\nAlternate until convergence.",
      "tags": [
        "ch15",
        "k-means",
        "algorithm"
      ]
    },
    {
      "uid": "15-003",
      "front": "Is K-means guaranteed to converge?",
      "back": "Yes, K-means is <b>guaranteed to converge</b> in a finite number of steps.\n\nReason: Assignments are discrete, and each step cannot increase J.\n\nHowever, may converge to a <b>local minimum</b>, not global minimum.",
      "tags": [
        "ch15",
        "k-means",
        "convergence"
      ]
    },
    {
      "uid": "15-004",
      "front": "What is vector quantization?",
      "back": "Application of K-means to <b>lossy data compression</b>.\n\nThe cluster centres \\( \\{\\boldsymbol{\\mu}_k\\} \\) are called <b>codebook vectors</b>.\n\nTo compress: Replace each data point with index of nearest codebook vector.\n\nTo decompress: Look up codebook vector from index.",
      "tags": [
        "ch15",
        "vector-quantization",
        "compression"
      ]
    },
    {
      "uid": "15-005",
      "front": "What is a Gaussian mixture model?",
      "back": "A probability distribution expressed as a weighted sum of Gaussians:\n\n\\( p(\\mathbf{x}) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) \\)\n\nwhere:\n\n- \\( \\pi_k \\) are mixing coefficients (\\( \\sum_k \\pi_k = 1 \\))\n- \\( \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k \\) are component parameters",
      "tags": [
        "ch15",
        "gmm",
        "mixture-model"
      ]
    },
    {
      "uid": "15-006",
      "front": "What is the latent variable interpretation of Gaussian mixtures?",
      "back": "Introduce discrete latent variable <b>z</b> with 1-of-K encoding.\n\n\\( p(z_k = 1) = \\pi_k \\) (prior)\n\n\\( p(\\mathbf{x}|z_k = 1) = \\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) \\)\n\nFor every observed \\( \\mathbf{x}_n \\) there is a corresponding latent \\( \\mathbf{z}_n \\) indicating which component generated it.",
      "tags": [
        "ch15",
        "gmm",
        "latent-variables"
      ]
    },
    {
      "uid": "15-007",
      "front": "What is the responsibility in Gaussian mixture models?",
      "back": "The <b>posterior probability</b> that component k generated data point n:\n\n\\( \\gamma(z_{nk}) = \\frac{\\pi_k \\mathcal{N}(\\mathbf{x}_n|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}{\\sum_j \\pi_j \\mathcal{N}(\\mathbf{x}_n|\\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j)} \\)\n\nInterpretation: How \"responsible\" component k is for generating \\( \\mathbf{x}_n \\).",
      "tags": [
        "ch15",
        "gmm",
        "responsibility"
      ]
    },
    {
      "uid": "15-008",
      "front": "What is the Expectation-Maximization (EM) algorithm?",
      "back": "A general method for finding <b>maximum likelihood</b> solutions in models with latent variables.\n\nTwo alternating steps:\n\n1. <b>E step</b>: Compute posterior over latent variables using current parameters\n\n2. <b>M step</b>: Maximize expected complete-data log likelihood\n\nGuaranteed to increase likelihood at each step.",
      "tags": [
        "ch15",
        "em-algorithm",
        "maximum-likelihood"
      ]
    },
    {
      "uid": "15-009",
      "front": "What is the E step in EM for Gaussian mixtures?",
      "back": "Evaluate responsibilities using current parameters:\n\n\\( \\gamma(z_{nk}) = \\frac{\\pi_k \\mathcal{N}(\\mathbf{x}_n|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}{\\sum_j \\pi_j \\mathcal{N}(\\mathbf{x}_n|\\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j)} \\)\n\nThese soft assignments replace the hard assignments \\( r_{nk} \\) in K-means.",
      "tags": [
        "ch15",
        "em-algorithm",
        "e-step"
      ]
    },
    {
      "uid": "15-010",
      "front": "What is the M step in EM for Gaussian mixtures?",
      "back": "Re-estimate parameters using responsibilities:\n\n\\( \\boldsymbol{\\mu}_k = \\frac{1}{N_k} \\sum_n \\gamma(z_{nk}) \\mathbf{x}_n \\)\n\n\\( \\boldsymbol{\\Sigma}_k = \\frac{1}{N_k} \\sum_n \\gamma(z_{nk}) (\\mathbf{x}_n - \\boldsymbol{\\mu}_k)(\\mathbf{x}_n - \\boldsymbol{\\mu}_k)^T \\)\n\n\\( \\pi_k = \\frac{N_k}{N} \\)\n\nwhere \\( N_k = \\sum_n \\gamma(z_{nk}) \\) is effective number of points.",
      "tags": [
        "ch15",
        "em-algorithm",
        "m-step"
      ]
    },
    {
      "uid": "15-011",
      "front": "What is \\( N_k \\) in the EM algorithm for GMM?",
      "back": "The <b>effective number of points</b> assigned to cluster k:\n\n\\( N_k = \\sum_{n=1}^{N} \\gamma(z_{nk}) \\)\n\nSoft version of cluster size. Can be non-integer.\n\nUsed to normalize mean and covariance estimates.",
      "tags": [
        "ch15",
        "em-algorithm",
        "effective-count"
      ]
    },
    {
      "uid": "15-012",
      "front": "Why is EM not guaranteed to find the global maximum?",
      "back": "EM only guarantees <b>monotonic increase</b> in log likelihood.\n\nMay converge to a <b>local maximum</b>.\n\nMitigation strategies:\n\n- Run multiple times with different initializations\n- Initialize with K-means\n- Use techniques like simulated annealing",
      "tags": [
        "ch15",
        "em-algorithm",
        "limitations"
      ]
    },
    {
      "uid": "15-013",
      "front": "Why use K-means to initialize EM for Gaussian mixtures?",
      "back": "<b>K-means is much faster</b> than EM:\n\n- Fewer iterations to converge\n- Simpler computations per iteration\n\nInitialization strategy:\n\n- Run K-means to get cluster centres\n- Initialize GMM means with K-means centres\n- Initialize covariances with cluster sample covariances\n- Initialize mixing coefficients with cluster proportions",
      "tags": [
        "ch15",
        "initialization",
        "practical"
      ]
    },
    {
      "uid": "15-014",
      "front": "What is the singularity problem in GMM maximum likelihood?",
      "back": "If a Gaussian component <b>collapses</b> onto a single data point:\n\n- Variance \\( \\to 0 \\)\n- Likelihood \\( \\to \\infty \\)\n\nThis is a pathological solution.\n\nSolutions:\n\n- Detect collapse and reset parameters\n- Use regularization/priors\n- Constrain minimum variance",
      "tags": [
        "ch15",
        "gmm",
        "singularity"
      ]
    },
    {
      "uid": "15-015",
      "front": "What is the identifiability problem in mixture models?",
      "back": "For K components, there are <b>K! equivalent solutions</b> corresponding to different labelings.\n\nSwapping component labels gives same distribution.\n\nImportant for parameter interpretation, irrelevant for density modeling.",
      "tags": [
        "ch15",
        "gmm",
        "identifiability"
      ]
    },
    {
      "uid": "15-016",
      "front": "What is the complete-data log likelihood?",
      "back": "The log likelihood if we knew the latent variables Z:\n\n\\( \\ln p(\\mathbf{X}, \\mathbf{Z}|\\theta) \\)\n\nFor GMM:\n\\( \\sum_{n=1}^{N} \\sum_{k=1}^{K} z_{nk} [\\ln \\pi_k + \\ln \\mathcal{N}(\\mathbf{x}_n|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)] \\)\n\nEasier to optimize than incomplete-data likelihood.",
      "tags": [
        "ch15",
        "em-algorithm",
        "complete-data"
      ]
    },
    {
      "uid": "15-017",
      "front": "What is the Q function in the EM algorithm?",
      "back": "Expected complete-data log likelihood under posterior of latent variables:\n\n\\( Q(\\theta, \\theta^{\\text{old}}) = \\sum_{\\mathbf{Z}} p(\\mathbf{Z}|\\mathbf{X}, \\theta^{\\text{old}}) \\ln p(\\mathbf{X}, \\mathbf{Z}|\\theta) \\)\n\nE step computes this expectation.\nM step maximizes Q over \\( \\theta \\).",
      "tags": [
        "ch15",
        "em-algorithm",
        "q-function"
      ]
    },
    {
      "uid": "15-018",
      "front": "Why does the summation inside the logarithm make GMM optimization difficult?",
      "back": "The marginal likelihood involves:\n\n\\( \\ln \\sum_{\\mathbf{Z}} p(\\mathbf{X}, \\mathbf{Z}|\\theta) \\)\n\nThe <b>sum inside the log</b> prevents simple decomposition.\n\nEven if joint distribution is in exponential family, the marginal typically is not.\n\nEM sidesteps this by working with expected complete-data likelihood.",
      "tags": [
        "ch15",
        "em-algorithm",
        "difficulty"
      ]
    },
    {
      "uid": "15-019",
      "front": "What is a hidden Markov model (HMM)?",
      "back": "Extension of mixture models for <b>sequential data</b>.\n\nThe discrete latent variables are connected in a <b>Markov chain</b>:\n\n\\( p(z_1, \\ldots, z_T) = p(z_1) \\prod_{t=2}^{T} p(z_t | z_{t-1}) \\)\n\nUsed for speech recognition, genomics, time series.",
      "tags": [
        "ch15",
        "hmm",
        "sequential"
      ]
    },
    {
      "uid": "15-020",
      "front": "How does K-means relate to GMM?",
      "back": "K-means is a <b>special limit</b> of GMM where:\n\n- All covariances are \\( \\epsilon \\mathbf{I} \\) with \\( \\epsilon \\to 0 \\)\n- Responsibilities become hard assignments (0 or 1)\n- Each point assigned to exactly one cluster\n\nK-means is \"hard\" clustering, GMM is \"soft\" clustering.",
      "tags": [
        "ch15",
        "k-means",
        "gmm-relation"
      ]
    },
    {
      "uid": "15-021",
      "front": "What is the difference between hard and soft clustering?",
      "back": "<b>Hard clustering</b> (K-means):\n\n- Each point assigned to exactly one cluster\n- Binary assignments \\( r_{nk} \\in \\{0,1\\} \\)\n\n<b>Soft clustering</b> (GMM):\n\n- Points have probabilistic membership\n- Continuous responsibilities \\( \\gamma(z_{nk}) \\in [0,1] \\)\n- Accounts for uncertainty in assignments",
      "tags": [
        "ch15",
        "clustering",
        "comparison"
      ]
    },
    {
      "uid": "15-022",
      "front": "What is the evidence lower bound (ELBO)?",
      "back": "A lower bound on the log marginal likelihood:\n\n\\( \\ln p(\\mathbf{X}|\\theta) \\geq \\mathcal{L}(q, \\theta) \\)\n\nwhere \\( \\mathcal{L} \\) is the ELBO.\n\nEM can be viewed as <b>coordinate ascent</b> on the ELBO:\n\n- E step: Optimize \\( q \\) (tightens bound)\n- M step: Optimize \\( \\theta \\)",
      "tags": [
        "ch15",
        "elbo",
        "variational"
      ]
    },
    {
      "uid": "15-023",
      "front": "How can EM be extended to find MAP solutions?",
      "back": "Add a prior \\( p(\\theta) \\) over parameters.\n\nE step: Same as ML case\n\nM step: Maximize \\( Q(\\theta, \\theta^{\\text{old}}) + \\ln p(\\theta) \\)\n\nSuitable priors can <b>remove singularities</b> in GMM.",
      "tags": [
        "ch15",
        "em-algorithm",
        "map"
      ]
    },
    {
      "uid": "15-024",
      "front": "What is the relationship between data standardization and K-means?",
      "back": "K-means uses Euclidean distance, which is <b>scale-dependent</b>.\n\nBefore K-means, often apply <b>standardization</b>:\n\n- Subtract mean (zero mean)\n- Divide by standard deviation (unit variance)\n\nEnsures all features contribute equally to clustering.",
      "tags": [
        "ch15",
        "k-means",
        "preprocessing"
      ]
    },
    {
      "uid": "15-025",
      "front": "Why do mixture models require many components in high dimensions?",
      "back": "In high-dimensional spaces, data often lies on <b>complex manifolds</b>.\n\nGaussian components are <b>locally linear</b> approximations.\n\nAccurate modeling of complex distributions requires <b>exponentially many</b> components.\n\nMotivates nonlinear latent variable models (VAEs, etc.).",
      "tags": [
        "ch15",
        "gmm",
        "limitations"
      ]
    }
  ]
}
