{
  "id": "15",
  "title": "Lesson 15: Clustering & Mixture Models (EM)",
  "lesson_title": "Clustering & Mixture Models (EM)",
  "objectives": [
    "Understand K-means clustering algorithm",
    "Learn Gaussian mixture models and their properties",
    "Master the Expectation-Maximization (EM) algorithm",
    "Understand the role of latent variables in mixture models",
    "Learn about the evidence lower bound (ELBO)"
  ],
  "cards": [
    {
      "uid": "deep-learning-foundations-and-concepts-15-001",
      "front": "What is K-means clustering?",
      "back": "An algorithm to partition N data points into K clusters.<br>Objective: Minimize sum of squared distances to cluster centres:<br>\\( J = \\sum_{n=1}^{N} \\sum_{k=1}^{K} r_{nk} ||\\mathbf{x}_n - \\boldsymbol{\\mu}_k||^2 \\)<br>where \\( r_{nk} \\in \\{0,1\\} \\) indicates cluster assignment.",
      "tags": [
        "ch15",
        "k-means",
        "clustering"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-15-002",
      "front": "What are the two steps of the K-means algorithm?",
      "back": "<b>E step</b>: Assign each point to nearest cluster centre<br>\\( r_{nk} = 1 \\) if \\( k = \\arg\\min_j ||\\mathbf{x}_n - \\boldsymbol{\\mu}_j||^2 \\)<br><b>M step</b>: Update cluster centres to be mean of assigned points<br>\\( \\boldsymbol{\\mu}_k = \\frac{\\sum_n r_{nk} \\mathbf{x}_n}{\\sum_n r_{nk}} \\)<br>Alternate until convergence.",
      "tags": [
        "ch15",
        "k-means",
        "algorithm"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-15-003",
      "front": "Is K-means guaranteed to converge?",
      "back": "Yes, K-means is <b>guaranteed to converge</b> in a finite number of steps.<br>Reason: Assignments are discrete, and each step cannot increase J.<br>However, may converge to a <b>local minimum</b>, not global minimum.",
      "tags": [
        "ch15",
        "k-means",
        "convergence"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-15-004",
      "front": "What is vector quantization?",
      "back": "Application of K-means to <b>lossy data compression</b>.<br>The cluster centres \\( \\{\\boldsymbol{\\mu}_k\\} \\) are called <b>codebook vectors</b>.<br>To compress: Replace each data point with index of nearest codebook vector.<br>To decompress: Look up codebook vector from index.",
      "tags": [
        "ch15",
        "vector-quantization",
        "compression"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-15-005",
      "front": "What is a Gaussian mixture model?",
      "back": "<b>Intuition</b>: One bell curve can't capture multiple clusters. Solution: use K bell curves, each centered on a different cluster. With enough Gaussians, you can approximate almost any distribution.<br><br><b>Formula</b>:<br>\\( p(\\mathbf{x}) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) \\)<br>where:<br><ul><li>\\( \\pi_k \\) are mixing coefficients representing how \"popular\" each cluster is (must sum to 1)</li><li>\\( \\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k \\) are component parameters</li></ul>",
      "tags": [
        "ch15",
        "gmm",
        "mixture-model"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-15-006",
      "front": "What is the latent variable interpretation of Gaussian mixtures?",
      "back": "Introduce discrete latent variable <b>z</b> with 1-of-K encoding.<br>\\( p(z_k = 1) = \\pi_k \\) (prior)<br>\\( p(\\mathbf{x}|z_k = 1) = \\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) \\)<br>For every observed \\( \\mathbf{x}_n \\) there is a corresponding latent \\( \\mathbf{z}_n \\) indicating which component generated it.",
      "tags": [
        "ch15",
        "gmm",
        "latent-variables"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-15-007",
      "front": "What is the responsibility in Gaussian mixture models?",
      "back": "<b>Intuition</b>: Answers \"which cluster generated this point?\" A point near cluster 2's center will have high responsibility for cluster 2 and low for others.<br><br><b>Soft assignment</b>: Unlike hard clustering, responsibilities can be fractional (e.g., 60% cluster 1, 40% cluster 2).<br><br><b>Formula</b>:<br>\\( \\gamma(z_{nk}) = \\frac{\\pi_k \\mathcal{N}(\\mathbf{x}_n|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}{\\sum_j \\pi_j \\mathcal{N}(\\mathbf{x}_n|\\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j)} \\)",
      "tags": [
        "ch15",
        "gmm",
        "responsibility"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-15-008",
      "front": "What is the Expectation-Maximization (EM) algorithm?",
      "back": "A general method for finding <b>maximum likelihood</b> solutions in models with latent variables.<br>Two alternating steps:<br><ol><li><b>E step</b>: Compute posterior over latent variables using current parameters</li><li><b>M step</b>: Maximize expected complete-data log likelihood</li></ol>Guaranteed to increase likelihood at each step.",
      "tags": [
        "ch15",
        "em-algorithm",
        "maximum-likelihood"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-15-009",
      "front": "What is the E step in EM for Gaussian mixtures?",
      "back": "Evaluate responsibilities using current parameters:<br>\\( \\gamma(z_{nk}) = \\frac{\\pi_k \\mathcal{N}(\\mathbf{x}_n|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}{\\sum_j \\pi_j \\mathcal{N}(\\mathbf{x}_n|\\boldsymbol{\\mu}_j, \\boldsymbol{\\Sigma}_j)} \\)<br>These soft assignments replace the hard assignments \\( r_{nk} \\) in K-means.",
      "tags": [
        "ch15",
        "em-algorithm",
        "e-step"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-15-010",
      "front": "What is the M step in EM for Gaussian mixtures?",
      "back": "Re-estimate parameters using responsibilities:<br>\\( \\boldsymbol{\\mu}_k = \\frac{1}{N_k} \\sum_n \\gamma(z_{nk}) \\mathbf{x}_n \\)<br>\\( \\boldsymbol{\\Sigma}_k = \\frac{1}{N_k} \\sum_n \\gamma(z_{nk}) (\\mathbf{x}_n - \\boldsymbol{\\mu}_k)(\\mathbf{x}_n - \\boldsymbol{\\mu}_k)^T \\)<br>\\( \\pi_k = \\frac{N_k}{N} \\)<br>where \\( N_k = \\sum_n \\gamma(z_{nk}) \\) is effective number of points.",
      "tags": [
        "ch15",
        "em-algorithm",
        "m-step"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-15-011",
      "front": "What is \\( N_k \\) in the EM algorithm for GMM?",
      "back": "The <b>effective number of points</b> assigned to cluster k:<br>\\( N_k = \\sum_{n=1}^{N} \\gamma(z_{nk}) \\)<br>Soft version of cluster size. Can be non-integer.<br>Used to normalize mean and covariance estimates.",
      "tags": [
        "ch15",
        "em-algorithm",
        "effective-count"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-15-012",
      "front": "Why is EM not guaranteed to find the global maximum?",
      "back": "EM only guarantees <b>monotonic increase</b> in log likelihood.<br>May converge to a <b>local maximum</b>.<br>Mitigation strategies:<br><ul><li>Run multiple times with different initializations</li><li>Initialize with K-means</li><li>Use techniques like simulated annealing</li></ul>",
      "tags": [
        "ch15",
        "em-algorithm",
        "limitations"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-15-013",
      "front": "Why use K-means to initialize EM for Gaussian mixtures?",
      "back": "<b>K-means is much faster</b> than EM:<br><ul><li>Fewer iterations to converge</li><li>Simpler computations per iteration</li></ul>Initialization strategy:<br><ul><li>Run K-means to get cluster centres</li><li>Initialize GMM means with K-means centres</li><li>Initialize covariances with cluster sample covariances</li><li>Initialize mixing coefficients with cluster proportions</li></ul>",
      "tags": [
        "ch15",
        "initialization",
        "practical"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-15-014",
      "front": "What is the singularity problem in GMM maximum likelihood?",
      "back": "If a Gaussian component <b>collapses</b> onto a single data point:<br><ul><li>Variance \\( \\to 0 \\)</li><li>Likelihood \\( \\to \\infty \\)</li></ul>This is a pathological solution.<br>Solutions:<br><ul><li>Detect collapse and reset parameters</li><li>Use regularization/priors</li><li>Constrain minimum variance</li></ul>",
      "tags": [
        "ch15",
        "gmm",
        "singularity"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-15-015",
      "front": "What is the identifiability problem in mixture models?",
      "back": "For K components, there are <b>K! equivalent solutions</b> corresponding to different labelings.<br>Swapping component labels gives same distribution.<br>Important for parameter interpretation, irrelevant for density modeling.",
      "tags": [
        "ch15",
        "gmm",
        "identifiability"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-15-016",
      "front": "What is the complete-data log likelihood?",
      "back": "The log likelihood if we knew the latent variables Z:<br>\\( \\ln p(\\mathbf{X}, \\mathbf{Z}|\\theta) \\)<br>For GMM:<br>\\( \\sum_{n=1}^{N} \\sum_{k=1}^{K} z_{nk} [\\ln \\pi_k + \\ln \\mathcal{N}(\\mathbf{x}_n|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)] \\)<br>Easier to optimize than incomplete-data likelihood.",
      "tags": [
        "ch15",
        "em-algorithm",
        "complete-data"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-15-017",
      "front": "What is the Q function in the EM algorithm?",
      "back": "Expected complete-data log likelihood under posterior of latent variables:<br>\\( Q(\\theta, \\theta^{\\text{old}}) = \\sum_{\\mathbf{Z}} p(\\mathbf{Z}|\\mathbf{X}, \\theta^{\\text{old}}) \\ln p(\\mathbf{X}, \\mathbf{Z}|\\theta) \\)<br>E step computes this expectation.<br>M step maximizes Q over \\( \\theta \\).",
      "tags": [
        "ch15",
        "em-algorithm",
        "q-function"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-15-018",
      "front": "Why does the summation inside the logarithm make GMM optimization difficult?",
      "back": "The marginal likelihood involves:<br>\\( \\ln \\sum_{\\mathbf{Z}} p(\\mathbf{X}, \\mathbf{Z}|\\theta) \\)<br>The <b>sum inside the log</b> prevents simple decomposition.<br>Even if joint distribution is in exponential family, the marginal typically is not.<br>EM sidesteps this by working with expected complete-data likelihood.",
      "tags": [
        "ch15",
        "em-algorithm",
        "difficulty"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-15-019",
      "front": "What is a hidden Markov model (HMM)?",
      "back": "Extension of mixture models for <b>sequential data</b>.<br>The discrete latent variables are connected in a <b>Markov chain</b>:<br>\\( p(z_1, \\ldots, z_T) = p(z_1) \\prod_{t=2}^{T} p(z_t | z_{t-1}) \\)<br>Used for speech recognition, genomics, time series.",
      "tags": [
        "ch15",
        "hmm",
        "sequential"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-15-020",
      "front": "How does K-means relate to GMM?",
      "back": "K-means is a <b>special limit</b> of GMM where:<br><ul><li>All covariances are \\( \\epsilon \\mathbf{I} \\) with \\( \\epsilon \\to 0 \\)</li><li>Responsibilities become hard assignments (0 or 1)</li><li>Each point assigned to exactly one cluster</li></ul>K-means is \"hard\" clustering, GMM is \"soft\" clustering.",
      "tags": [
        "ch15",
        "k-means",
        "gmm-relation"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-15-021",
      "front": "What is the difference between hard and soft clustering?",
      "back": "<b>Hard clustering</b> (K-means):<br><ul><li>Each point assigned to exactly one cluster</li><li>Binary assignments \\( r_{nk} \\in \\{0,1\\} \\)</li></ul><b>Soft clustering</b> (GMM):<br><ul><li>Points have probabilistic membership</li><li>Continuous responsibilities \\( \\gamma(z_{nk}) \\in [0,1] \\)</li><li>Accounts for uncertainty in assignments</li></ul>",
      "tags": [
        "ch15",
        "clustering",
        "comparison"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-15-022",
      "front": "What is the evidence lower bound (ELBO)?",
      "back": "A lower bound on the log marginal likelihood:<br>\\( \\ln p(\\mathbf{X}|\\theta) \\geq \\mathcal{L}(q, \\theta) \\)<br>where \\( \\mathcal{L} \\) is the ELBO.<br>EM can be viewed as <b>coordinate ascent</b> on the ELBO:<br><ul><li>E step: Optimize \\( q \\) (tightens bound)</li><li>M step: Optimize \\( \\theta \\)</li></ul>",
      "tags": [
        "ch15",
        "elbo",
        "variational"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-15-023",
      "front": "How can EM be extended to find MAP solutions?",
      "back": "Add a prior \\( p(\\theta) \\) over parameters.<br>E step: Same as ML case<br>M step: Maximize \\( Q(\\theta, \\theta^{\\text{old}}) + \\ln p(\\theta) \\)<br>Suitable priors can <b>remove singularities</b> in GMM.",
      "tags": [
        "ch15",
        "em-algorithm",
        "map"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-15-024",
      "front": "What is the relationship between data standardization and K-means?",
      "back": "K-means uses Euclidean distance, which is <b>scale-dependent</b>.<br>Before K-means, often apply <b>standardization</b>:<br><ul><li>Subtract mean (zero mean)</li><li>Divide by standard deviation (unit variance)</li></ul>Ensures all features contribute equally to clustering.",
      "tags": [
        "ch15",
        "k-means",
        "preprocessing"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-15-025",
      "front": "Why do mixture models require many components in high dimensions?",
      "back": "In high-dimensional spaces, data often lies on <b>complex manifolds</b>.<br>Gaussian components are <b>locally linear</b> approximations.<br>Accurate modeling of complex distributions requires <b>exponentially many</b> components.<br>Motivates nonlinear latent variable models (VAEs, etc.).",
      "tags": [
        "ch15",
        "gmm",
        "limitations"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-15-026",
      "front": "What conditions guarantee identifiability in latent variable models?",
      "back": "<b>Identifiability</b>: Can we uniquely recover the true parameters from data?<br><br><b>Types of non-identifiability</b>:<ul><li><b>Label switching</b>: In mixtures, component labels are arbitrary (K! equivalent solutions)</li><li><b>Rotational</b>: In factor analysis with Gaussian factors, rotations of loadings are equivalent</li><li><b>Scale</b>: Can scale latent variables and inversely scale loadings</li></ul><b>Conditions for identifiability</b>:<ul><li><b>Non-Gaussian sources</b>: ICA requires non-Gaussian (breaks rotational symmetry)</li><li><b>Constraints on loadings</b>: Sparse, orthogonal, or structured loadings</li><li><b>Temporal structure</b>: Time-contrastive learning uses temporal dependencies</li><li><b>Auxiliary information</b>: Class labels, domains can help identify</li></ul><b>When identifiability fails</b>:<ul><li>Parameters are not uniquely determined</li><li>Interpretation of latent variables is ambiguous</li><li>But density estimation still works fine</li></ul><b>Modern view</b>: Nonlinear ICA (iVAE) shows identifiability is possible with auxiliary variables even for neural networks.",
      "tags": [
        "ch15",
        "identifiability",
        "latent-variables"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-15-027",
      "front": "What is a mixture distribution?",
      "back": "A <b>mixture distribution</b> combines simpler distributions to model complex data.<br><b>Intuition</b>: Like mixing paint colors. If your data has multiple clusters, use one \"paint\" (distribution) per cluster and blend them. Each component has a weight \\( \\pi_k \\) saying how much of that \"paint\" to use.<br><b>Formula</b>: \\( p(\\mathbf{x}) = \\sum_{k=1}^{K} \\pi_k p_k(\\mathbf{x}) \\) where weights sum to 1.",
      "tags": [
        "ch15",
        "mixture-models",
        "definition"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-15-028",
      "front": "Why is maximum likelihood for Gaussian mixtures more complex than for a single Gaussian?",
      "back": "<b>Intuition</b>: For a single Gaussian, we can solve for the best parameters directly (closed-form). For mixtures, the components are \"entangled\" - we don't know which points belong to which cluster.<br><b>The problem</b>: Log of a sum \\( \\ln(\\sum_k ...) \\) doesn't simplify nicely, unlike sum of logs.<br><b>Solution</b>: Use iterative methods like <b>EM algorithm</b> - alternate between guessing cluster assignments and updating parameters.",
      "tags": [
        "ch15",
        "gmm",
        "maximum-likelihood"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-15-029",
      "front": "What is a limitation of using a single Gaussian for multimodal data?",
      "back": "<b>Intuition</b>: A single bell curve has one peak. If your data has multiple clusters (like heights of children vs adults), one Gaussian will awkwardly sit between them.<br><b>Problems</b>:<ul><li>Misses the cluster structure entirely</li><li>Assigns high probability to the \"gap\" between clusters where no data exists</li></ul><b>Solution</b>: Use a mixture of Gaussians - one bell curve per cluster.",
      "tags": [
        "ch15",
        "gaussian-limitations",
        "multimodal"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-15-030",
      "front": "What is a mixture density network?",
      "back": "A neural network that outputs parameters of a <b>mixture model</b> (e.g., Gaussian mixture).<br>Outputs: mixing coefficients \\( \\pi_k \\), means \\( \\mu_k \\), variances \\( \\sigma_k^2 \\)<br>Useful for multi-modal distributions where a single prediction is insufficient.",
      "tags": [
        "ch15",
        "mixture-density",
        "probabilistic"
      ]
    }
  ]
}
