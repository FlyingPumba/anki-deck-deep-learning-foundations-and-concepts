{
  "id": "12",
  "title": "Lesson 12: Transformers",
  "lesson_title": "Transformers",
  "objectives": [
    "Understand attention mechanisms and self-attention",
    "Master transformer architecture: queries, keys, values",
    "Learn multi-head attention and positional encoding",
    "Understand tokenization and word embeddings",
    "Learn about decoder, encoder, and encoder-decoder transformers",
    "Understand large language models and sampling strategies"
  ],
  "cards": [
    {
      "uid": "12-001",
      "front": "What are transformers in deep learning?",
      "back": "Models that <b>transform a set of input vectors into output vectors</b> using attention mechanisms.<br>Key properties:<br><ul><li>Process sequences without recurrence</li><li>Capture long-range dependencies</li><li>Enable parallel computation</li><li>Foundation for modern NLP and beyond</li></ul>",
      "tags": [
        "ch12",
        "transformers",
        "architecture"
      ]
    },
    {
      "uid": "12-002",
      "front": "What is a foundation model?",
      "back": "A <b>large-scale model</b> trained on broad data that can be <b>adapted to solve multiple downstream tasks</b>.<br>Examples: GPT, BERT, CLIP<br>Typically:<br><ul><li>Pre-trained on massive datasets</li><li>Fine-tuned for specific applications</li><li>Transfer learning across tasks</li></ul>",
      "tags": [
        "ch12",
        "foundation-models",
        "transfer-learning"
      ]
    },
    {
      "uid": "12-003",
      "front": "How can transformers be trained without labelled data?",
      "back": "Using <b>self-supervised learning</b> on unlabelled data.<br>Examples:<br><ul><li><b>Masked language modeling</b>: Predict masked words</li><li><b>Next token prediction</b>: Predict next word</li><li><b>Contrastive learning</b>: Learn similar/different pairs</li></ul>No manual labels required.",
      "tags": [
        "ch12",
        "self-supervised",
        "training"
      ]
    },
    {
      "uid": "12-004",
      "front": "What does 'attend to' mean in transformers?",
      "back": "Using <b>context provided by other elements</b> to process the current element.<br>Attention uses <b>weighting factors</b> whose values depend on the specific input sequence.<br>Different from fixed weights - attention is <b>data-dependent</b>.",
      "tags": [
        "ch12",
        "attention",
        "mechanism"
      ]
    },
    {
      "uid": "12-005",
      "front": "What is a token in transformer terminology?",
      "back": "An individual <b>data vector</b> in the input sequence.<br>Tokens can represent:<br><ul><li>Words or subwords (NLP)</li><li>Image patches (vision)</li><li>Audio segments</li></ul>Input is a matrix X of dimensions N x D (N tokens, D features).",
      "tags": [
        "ch12",
        "tokens",
        "input"
      ]
    },
    {
      "uid": "12-006",
      "front": "What is the basic operation of a transformer layer?",
      "back": "A transformer layer has two main sub-layers:<br><br><b>1. Self-attention</b>: Each output token is a weighted combination of all input tokens:<br>\\( \\vec{y}_m = \\sum_n a_{mn} \\vec{v}_n \\)<br>where \\( a_{mn} \\) are data-dependent <b>attention weights</b>.<br><br><b>2. Feed-forward network (FFN)</b>: Applied independently to each token position.<br><br><b>Additional components</b>:<br><ul><li><b>Residual connections</b>: Add input to output of each sub-layer</li><li><b>Layer normalization</b>: Applied after each sub-layer</li></ul>",
      "tags": [
        "ch12",
        "transformer-layer",
        "operation"
      ]
    },
    {
      "uid": "12-007",
      "front": "What are attention weights?",
      "back": "Scalars that determine how much each output token attends to each input token.<br><br><b>How they're computed</b>:<br>\\( a_{mn} = \\text{softmax}\\left(\\frac{\\vec{q}_m \\cdot \\vec{k}_n}{\\sqrt{D_k}}\\right) \\)<br>The dot product measures similarity between query m and key n.<br><br><b>Properties</b>:<br><ul><li>Non-negative and sum to 1 (due to softmax)</li><li>Data-dependent: computed from the input itself</li><li>Higher weight = token m pays more attention to token n</li></ul>",
      "tags": [
        "ch12",
        "attention-weights",
        "mechanism"
      ]
    },
    {
      "uid": "12-008",
      "front": "What are queries, keys, and values in attention?",
      "back": "Movie database analogy:<br><b>Key</b>: Attributes of each item (movie metadata)<br><b>Value</b>: The actual content (movie file)<br><b>Query</b>: What you're searching for (your preferences)<br>Attention finds values whose keys match the query.",
      "tags": [
        "ch12",
        "query-key-value",
        "attention"
      ]
    },
    {
      "uid": "12-009",
      "front": "What is soft attention?",
      "back": "Attention where weights are <b>continuous values</b> (not binary).<br>Computed via softmax, producing a <b>weighted average</b> of values.<br>Contrast with hard attention which selects discrete items.<br>Soft attention is differentiable and easier to train.",
      "tags": [
        "ch12",
        "soft-attention",
        "mechanism"
      ]
    },
    {
      "uid": "12-010",
      "front": "What is self-attention?",
      "back": "Attention where queries, keys, and values all come from the <b>same sequence</b>.<br>Each position attends to all positions in the same sequence.<br>Also called <b>intra-attention</b>.<br>Contrast with cross-attention where queries come from a different sequence.",
      "tags": [
        "ch12",
        "self-attention",
        "mechanism"
      ]
    },
    {
      "uid": "12-011",
      "front": "What is dot-product self-attention?",
      "back": "Computing attention weights using dot products:<br><ol><li>Query \\( \\vec{q}_m \\) = input \\( \\vec{x}_m \\)</li><li>Key \\( \\vec{k}_n \\) = input \\( \\vec{x}_n \\)</li><li>Similarity = \\( \\vec{q}_m \\cdot \\vec{k}_n \\)</li><li>Weights = softmax over similarities</li></ol>Simplest form before adding learnable projections.",
      "tags": [
        "ch12",
        "dot-product",
        "self-attention"
      ]
    },
    {
      "uid": "12-012",
      "front": "Why use separate Q, K, V matrices in attention?",
      "back": "<ol><li><b>Feature flexibility</b>: Focus on different aspects of input</li><li><b>Asymmetry</b>: Query and key can be different (A attending to B ≠ B attending to A)</li><li><b>Learnable transformations</b>: Network learns optimal projections</li></ol>Q, K, V matrices have <b>independent learnable parameters</b>.",
      "tags": [
        "ch12",
        "qkv-matrices",
        "attention"
      ]
    },
    {
      "uid": "12-013",
      "front": "What is the computational complexity of computing QK^T?",
      "back": "<b>O(N²D)</b> for N tokens with D dimensions.<br>The N x N attention matrix is the main bottleneck for long sequences.<br>This quadratic scaling limits maximum sequence length in practice.",
      "tags": [
        "ch12",
        "complexity",
        "attention"
      ]
    },
    {
      "uid": "12-014",
      "front": "Why is scaled dot-product attention needed?",
      "back": "Without scaling, dot products can be very large, causing softmax to have <b>exponentially small gradients</b>.<br><br><b>Why?</b> If Q and K elements are independent random variables with variance 1, the dot product has variance \\( D_k \\) (the key dimension).<br><br><b>Solution</b>: Scale by \\( 1/\\sqrt{D_k} \\) to normalize variance back to 1:<br>\\( \\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{D_k}}\\right)V \\)<br><br><b>Computing the scalar</b>: \\( D_k \\) is the dimension of the key vectors. For multi-head attention with model dimension \\( D \\) and \\( H \\) heads: \\( D_k = D/H \\).",
      "tags": [
        "ch12",
        "scaled-attention",
        "numerical-stability"
      ]
    },
    {
      "uid": "12-015",
      "front": "What is multi-head attention?",
      "back": "Running <b>multiple attention operations in parallel</b> with independent parameters.<br>Each 'head' can attend to different aspects of the input.<br>Outputs are <b>concatenated</b> then linearly projected.<br>Typically \\( D_v = D/H \\) so final dimension matches input.",
      "tags": [
        "ch12",
        "multi-head",
        "attention"
      ]
    },
    {
      "uid": "12-016",
      "front": "Why use multiple attention heads?",
      "back": "A single head <b>averages</b> over different effects.<br>Multiple heads can capture:<br><ul><li>Different relationship types</li><li>Different distance ranges</li><li>Different semantic aspects</li></ul>Each head has independent learnable parameters.",
      "tags": [
        "ch12",
        "multi-head",
        "motivation"
      ]
    },
    {
      "uid": "12-017",
      "front": "What is a transformer layer (block)?",
      "back": "Combines:<br><ol><li><b>Multi-head attention</b> sublayer</li><li><b>Feed-forward network</b> sublayer</li><li><b>Layer normalization</b> after each sublayer</li><li><b>Residual connections</b> around each sublayer</li></ol>Improves training efficiency and gradient flow.",
      "tags": [
        "ch12",
        "transformer-layer",
        "architecture"
      ]
    },
    {
      "uid": "12-018",
      "front": "Why add a feed-forward network after attention?",
      "back": "Attention computes a <b>weighted sum</b> of value vectors - linear in V, but the weights come from softmax (nonlinear in Q and K).<br><br>This limits expressiveness: each output is a convex combination of values.<br><br>The feed-forward network (MLP) adds <b>position-wise non-linearity</b> to enhance flexibility.<br>Typically: Linear -> ReLU/GELU -> Linear",
      "tags": [
        "ch12",
        "ffn",
        "non-linearity"
      ]
    },
    {
      "uid": "12-019",
      "front": "Why is positional encoding needed in transformers?",
      "back": "Self-attention is <b>permutation equivariant</b>: shuffling inputs shuffles outputs.<br>It doesn't inherently know <b>position</b> or <b>order</b> of tokens.<br>Positional encodings add position information so the model can use sequence order.",
      "tags": [
        "ch12",
        "positional-encoding",
        "motivation"
      ]
    },
    {
      "uid": "12-020",
      "front": "What properties should an ideal positional encoding have?",
      "back": "<ol><li><b>Unique</b> representation for each position</li><li><b>Consistent distance</b> between adjacent positions</li><li><b>Generalize</b> to longer sequences than training</li><li>Encoding at position n+k should be <b>representable</b> as function of encoding at n</li></ol>",
      "tags": [
        "ch12",
        "positional-encoding",
        "properties"
      ]
    },
    {
      "uid": "12-021",
      "front": "How does sinusoidal positional encoding work?",
      "back": "Uses sine and cosine functions of different frequencies:<br>\\( PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d}) \\)<br>\\( PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d}) \\)<br>For any fixed offset k, encoding at position n+k is a <b>linear function</b> of encoding at n.",
      "tags": [
        "ch12",
        "sinusoidal",
        "positional-encoding"
      ]
    },
    {
      "uid": "12-022",
      "front": "What is word embedding?",
      "back": "Mapping discrete words to <b>continuous vectors</b> in a learned embedding space.<br>Embedding matrix E of size D x K (D dimensions, K vocabulary size).<br>Word n is represented by column n of E.<br>Captures semantic relationships (similar words = similar vectors).",
      "tags": [
        "ch12",
        "word-embedding",
        "representation"
      ]
    },
    {
      "uid": "12-023",
      "front": "What is the continuous bag of words (CBOW) model?",
      "back": "A word embedding training method where the <b>target is the center word</b> predicted from surrounding context words.<br>Input: Context words (before and after)<br>Output: Predict center word<br>Form of self-supervised learning.",
      "tags": [
        "ch12",
        "cbow",
        "word-embedding"
      ]
    },
    {
      "uid": "12-024",
      "front": "What is the skip-gram model?",
      "back": "Word embedding training that <b>reverses CBOW</b>:<br>Input: Center word<br>Output: Predict surrounding context words<br>Often works better for rare words than CBOW.<br>Both are forms of self-supervised learning.",
      "tags": [
        "ch12",
        "skip-gram",
        "word-embedding"
      ]
    },
    {
      "uid": "12-025",
      "front": "What is tokenization?",
      "back": "Converting text into a sequence of <b>tokens</b> (discrete units).<br>Options:<br><ul><li>Word-level: Each word is a token</li><li>Character-level: Each character is a token</li><li>Subword: Compromise (e.g., byte pair encoding)</li></ul>Tradeoff between vocabulary size and sequence length.",
      "tags": [
        "ch12",
        "tokenization",
        "preprocessing"
      ]
    },
    {
      "uid": "12-026",
      "front": "What is byte pair encoding (BPE)?",
      "back": "A subword tokenization algorithm:<br><ol><li>Start with individual characters</li><li>Find most frequent adjacent pair</li><li>Merge into new token</li><li>Repeat until vocabulary size reached</li></ol>Balances vocabulary size and sequence length. Handles rare/unknown words.",
      "tags": [
        "ch12",
        "bpe",
        "tokenization"
      ]
    },
    {
      "uid": "12-027",
      "front": "What is a bag-of-words model?",
      "back": "A model that <b>ignores word order</b>, treating text as unordered collection.<br>Limitation: 'Dog bites man' = 'Man bites dog'<br>Maximum likelihood just counts word frequencies.<br>Simple baseline but loses sequential information.",
      "tags": [
        "ch12",
        "bag-of-words",
        "language-model"
      ]
    },
    {
      "uid": "12-028",
      "front": "What are n-gram language models?",
      "back": "Models that predict next word based on previous <b>n-1 words</b>:<br><ul><li>Unigram (n=1): Word frequencies only</li><li>Bigram (n=2): P(word | previous word)</li><li>Trigram (n=3): P(word | previous 2 words)</li></ul>Poor scaling with sequence length; can't capture long-range dependencies.",
      "tags": [
        "ch12",
        "n-gram",
        "language-model"
      ]
    },
    {
      "uid": "12-029",
      "front": "What is a recurrent neural network (RNN)?",
      "back": "A network that processes sequences by maintaining a <b>hidden state</b>:<br>\\( \\vec{h}_n = f(\\vec{h}_{n-1}, \\vec{x}_n) \\)<br>State carries information from previous steps.<br>Limitations: Sequential (no parallelism), vanishing gradients, bottleneck for long sequences.",
      "tags": [
        "ch12",
        "rnn",
        "sequence-models"
      ]
    },
    {
      "uid": "12-030",
      "front": "What is backpropagation through time (BPTT)?",
      "back": "Training RNNs by <b>unrolling</b> the network over time steps and applying standard backpropagation.<br>Gradients flow backward through all time steps.<br>Problems: Vanishing/exploding gradients for long sequences.",
      "tags": [
        "ch12",
        "bptt",
        "rnn"
      ]
    },
    {
      "uid": "12-031",
      "front": "What is the bottleneck problem in sequence-to-sequence models?",
      "back": "A sequence of <b>arbitrary length</b> must be compressed into a <b>single hidden state</b> before decoding.<br>This fixed-size vector limits information capacity.<br>Solution: Attention mechanisms that can access all encoder states.",
      "tags": [
        "ch12",
        "bottleneck",
        "seq2seq"
      ]
    },
    {
      "uid": "12-032",
      "front": "What are LSTM and GRU?",
      "back": "<b>Long Short-Term Memory</b> and <b>Gated Recurrent Unit</b>: RNN variants with gating mechanisms.<br>Gates control information flow, allowing longer-range dependencies.<br>Better than vanilla RNN but still:<br><ul><li>Sequential (no parallelism)</li><li>Superseded by transformers for most tasks</li></ul>",
      "tags": [
        "ch12",
        "lstm",
        "gru"
      ]
    },
    {
      "uid": "12-033",
      "front": "What are the three categories of large language models?",
      "back": "<ol><li><b>Encoder-only</b> (e.g., BERT): Bidirectional, good for understanding</li><li><b>Decoder-only</b> (e.g., GPT): Autoregressive, good for generation</li><li><b>Encoder-decoder</b> (e.g., T5): Sequence-to-sequence tasks</li></ol>",
      "tags": [
        "ch12",
        "llm",
        "categories"
      ]
    },
    {
      "uid": "12-034",
      "front": "What is GPT?",
      "back": "<b>Generative Pre-trained Transformer</b>: A decoder-only transformer.<br>Trained to predict next token given previous tokens.<br>Autoregressive generation: Output becomes input for next step.<br>Pre-trained on large corpus, can be fine-tuned.",
      "tags": [
        "ch12",
        "gpt",
        "decoder"
      ]
    },
    {
      "uid": "12-035",
      "front": "What is masked attention (causal attention)?",
      "back": "Attention where tokens can only attend to <b>previous positions</b> (not future).<br>Implemented by setting attention weights to -∞ (before softmax) for future positions.<br>Required for autoregressive generation to prevent 'cheating'.",
      "tags": [
        "ch12",
        "masked-attention",
        "causal"
      ]
    },
    {
      "uid": "12-036",
      "front": "What is the padding token in transformers?",
      "back": "A special token \\( \\langle pad \\rangle \\) used to <b>fill sequences to equal length</b> for batching.<br>Padding positions are masked in attention to avoid influencing computation.<br>Allows parallel processing of variable-length sequences.",
      "tags": [
        "ch12",
        "padding",
        "batching"
      ]
    },
    {
      "uid": "12-037",
      "front": "Why can transformer decoding reuse computation?",
      "back": "Due to masked attention, the embedding for position n <b>doesn't change</b> when new tokens are added.<br>Key-value pairs for previous positions can be <b>cached</b> (KV cache).<br>Only need to compute attention for new token, not entire sequence.",
      "tags": [
        "ch12",
        "kv-cache",
        "efficiency"
      ]
    },
    {
      "uid": "12-038",
      "front": "What is greedy search in text generation?",
      "back": "Selecting the <b>highest probability token</b> at each step.<br>Cost: O(KN) for K vocabulary, N steps.<br>Problem: Locally optimal choices may not give globally optimal sequence.<br>Simple but often produces repetitive or suboptimal text.",
      "tags": [
        "ch12",
        "greedy-search",
        "decoding"
      ]
    },
    {
      "uid": "12-039",
      "front": "What is beam search?",
      "back": "Maintaining <b>B best hypotheses</b> (beam width) at each step.<br>For each hypothesis, expand all K possible next tokens.<br>Prune to keep top B by probability.<br>Cost: O(BKN). Better than greedy but still not optimal.<br>Normalize by length to avoid favoring short sequences.",
      "tags": [
        "ch12",
        "beam-search",
        "decoding"
      ]
    },
    {
      "uid": "12-040",
      "front": "Why do deterministic decoding methods produce unnatural text?",
      "back": "Human text has a <b>long tail</b> of many low-probability tokens.<br>Deterministic methods always pick high-probability tokens.<br>Result: Repetitive, predictable, 'boring' text.<br>Sampling introduces needed diversity.",
      "tags": [
        "ch12",
        "decoding",
        "diversity"
      ]
    },
    {
      "uid": "12-041",
      "front": "What is top-K sampling?",
      "back": "Sample from only the <b>K most probable</b> tokens.<br>Re-normalize probabilities over the top K.<br>Reduces probability of nonsensical tokens while maintaining diversity.<br>K is a hyperparameter (e.g., K=40).",
      "tags": [
        "ch12",
        "top-k",
        "sampling"
      ]
    },
    {
      "uid": "12-042",
      "front": "What is nucleus sampling (top-p)?",
      "back": "Sample from tokens whose <b>cumulative probability</b> exceeds threshold p.<br>Dynamically adjusts number of candidates based on distribution.<br>If confident: Few candidates. If uncertain: Many candidates.<br>Often works better than fixed top-K.",
      "tags": [
        "ch12",
        "nucleus-sampling",
        "top-p"
      ]
    },
    {
      "uid": "12-043",
      "front": "What is temperature in softmax sampling?",
      "back": "A parameter T that controls distribution sharpness:<br>\\( p_k = \\frac{\\exp(a_k/T)}{\\sum_j \\exp(a_j/T)} \\)<br><ul><li>T=0: Deterministic (argmax)</li><li>T=1: Original distribution</li><li>T>1: Flatter (more random)</li><li>T<1: Sharper (more deterministic)</li></ul>",
      "tags": [
        "ch12",
        "temperature",
        "sampling"
      ]
    },
    {
      "uid": "12-044",
      "front": "What is BERT?",
      "back": "<b>Bidirectional Encoder Representations from Transformers</b>.<br>Encoder-only model that sees words <b>both before and after</b> target.<br>Pre-trained with masked language modeling.<br>Good for understanding tasks (classification, NER) but not generation.",
      "tags": [
        "ch12",
        "bert",
        "encoder"
      ]
    },
    {
      "uid": "12-045",
      "front": "How does BERT training differ from GPT?",
      "back": "<b>BERT</b>: Masked language modeling - predict randomly masked words (15%)<br><b>GPT</b>: Next token prediction - predict each next word<br>BERT is bidirectional (sees full context), GPT is unidirectional.<br>BERT less efficient: Only masked tokens provide training signal.",
      "tags": [
        "ch12",
        "bert-vs-gpt",
        "training"
      ]
    },
    {
      "uid": "12-046",
      "front": "What is cross-attention in encoder-decoder transformers?",
      "back": "Attention where <b>queries come from decoder</b> and <b>keys/values come from encoder</b>.<br>Allows decoder to attend to relevant parts of encoder output.<br>Same as self-attention except Q and K/V sources differ.",
      "tags": [
        "ch12",
        "cross-attention",
        "encoder-decoder"
      ]
    },
    {
      "uid": "12-047",
      "front": "What is a vision transformer (ViT)?",
      "back": "Applying transformers to images by treating <b>image patches as tokens</b>.<br>Split image into fixed-size patches (e.g., 16x16).<br>Flatten and linearly embed each patch.<br>Add positional encoding.<br>Process with standard transformer.",
      "tags": [
        "ch12",
        "vit",
        "vision"
      ]
    },
    {
      "uid": "12-048",
      "front": "What is perplexity in language modeling?",
      "back": "<b>Perplexity</b> measures how 'surprised' a language model is by test data.<br><br><b>Formula</b>:<br>\\( \\text{PPL} = \\exp\\left(-\\frac{1}{N}\\sum_{i=1}^{N} \\ln p(w_i | w_{<i})\\right) \\)<br><br><b>Intuition</b>: The average number of equally likely next words the model considers. Lower = better.<ul><li>PPL = 1: Perfect prediction (only one choice)</li><li>PPL = 100: Model is as uncertain as choosing uniformly from 100 words</li></ul><b>Connection to cross-entropy</b>: \\( \\text{PPL} = \\exp(H) \\) where H is the cross-entropy loss.",
      "tags": [
        "ch12",
        "perplexity",
        "evaluation"
      ]
    },
    {
      "uid": "12-049",
      "front": "How do you compare language models using perplexity?",
      "back": "<b>Lower perplexity = better model</b> (less surprised by data).<br><br><b>Caveats</b>:<ul><li><b>Same vocabulary</b>: Can only compare models with the same tokenizer/vocabulary</li><li><b>Same test set</b>: Must evaluate on identical data</li><li><b>Domain matters</b>: A model trained on news will have high perplexity on code</li></ul><b>Typical values</b>:<ul><li>Good models on standard benchmarks: PPL ~20-50</li><li>State-of-the-art LLMs: PPL < 10 on some benchmarks</li></ul><b>Limitation</b>: Low perplexity doesn't guarantee good generations or task performance.",
      "tags": [
        "ch12",
        "perplexity",
        "evaluation"
      ]
    },
    {
      "uid": "12-050",
      "front": "What is Rotary Position Embedding (RoPE)?",
      "back": "<b>RoPE</b> encodes position by rotating the query and key vectors:<br><br>\\( \\text{RoPE}(\\vec{x}, m) = \\vec{x} \\cdot R_m \\)<br><br>where \\( R_m \\) is a rotation matrix that depends on position \\( m \\).<br><br><b>Key insight</b>: The dot product \\( q_m^T k_n \\) depends only on relative position \\( m - n \\), not absolute positions.<br><br><b>Advantages over sinusoidal</b>:<ul><li>Naturally encodes relative position in attention</li><li>Better extrapolation to longer sequences</li><li>No need for separate positional embedding</li></ul><b>Used in</b>: LLaMA, GPT-NeoX, PaLM, and most modern LLMs.",
      "tags": [
        "ch12",
        "rope",
        "positional-encoding"
      ]
    },
    {
      "uid": "12-051",
      "front": "What is the key difference between absolute and relative positional encodings?",
      "back": "<b>Absolute positional encoding</b>:<ul><li>Each position gets a fixed embedding</li><li>Added to input: \\( x' = x + PE_{pos} \\)</li><li>Examples: Sinusoidal (original Transformer), learned embeddings</li></ul><b>Relative positional encoding</b>:<ul><li>Encodes the <i>distance</i> between positions</li><li>Modifies attention directly, not input</li><li>Examples: RoPE, ALiBi, T5 bias</li></ul><b>Why relative is often better</b>:<ul><li>Translation invariant: Same relationship regardless of absolute position</li><li>Better length generalization</li><li>More natural for many tasks (relative order matters more than absolute)</li></ul>",
      "tags": [
        "ch12",
        "positional-encoding",
        "comparison"
      ]
    },
    {
      "uid": "12-052",
      "front": "What is ALiBi (Attention with Linear Biases)?",
      "back": "<b>ALiBi</b> adds a linear penalty to attention scores based on distance:<br><br>\\( \\text{attention}(q, k) = \\text{softmax}\\left(\\frac{qk^T}{\\sqrt{d}} - m \\cdot |i - j|\\right) \\)<br><br>where \\( m \\) is a head-specific slope.<br><br><b>Key idea</b>: No positional embeddings at all - just bias attention to prefer nearby tokens.<br><br><b>Advantages</b>:<ul><li>Excellent length extrapolation</li><li>Simpler than RoPE</li><li>No additional parameters</li></ul><b>Trade-off</b>: May perform slightly worse than RoPE on some tasks but generalizes better to longer sequences.",
      "tags": [
        "ch12",
        "alibi",
        "positional-encoding"
      ]
    },
    {
      "uid": "12-053",
      "front": "What is FlashAttention and why is it important?",
      "back": "<b>FlashAttention</b> is an IO-aware exact attention algorithm that avoids materializing the full \\( N \\times N \\) attention matrix.<br><br><b>Key insight</b>: Standard attention is memory-bound, not compute-bound. Moving data between GPU memory levels is the bottleneck.<br><br><b>How it works</b>:<ul><li>Compute attention in tiles/blocks</li><li>Use online softmax (compute softmax incrementally)</li><li>Fuse operations to minimize memory reads/writes</li></ul><b>Benefits</b>:<ul><li>2-4x faster than standard attention</li><li>Memory: O(N) instead of O(N^2)</li><li>Enables much longer context lengths</li><li>Exact (not approximate)</li></ul><b>Used in</b>: Most modern LLM training and inference.",
      "tags": [
        "ch12",
        "flash-attention",
        "efficiency"
      ]
    }
  ]
}
