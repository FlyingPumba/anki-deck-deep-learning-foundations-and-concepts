{
  "id": "03",
  "title": "Lesson 03: Standard Distributions",
  "lesson_title": "Standard Distributions",
  "objectives": [
    "Understand parametric vs nonparametric density estimation",
    "Master the Bernoulli, binomial, and multinomial distributions",
    "Learn the multivariate Gaussian distribution and its properties",
    "Understand Gaussian mixture models",
    "Learn nonparametric methods: histograms, kernel density estimation, K-nearest neighbors"
  ],
  "cards": [
    {
      "uid": "03-001",
      "front": "What is density estimation and why is it fundamentally ill-posed?",
      "back": "<b>Density estimation</b> is the problem of modeling the probability distribution \\( p(x) \\) given a finite set of observations.<br>It is <b>ill-posed</b> because infinitely many distributions could have generated the finite data. Any distribution \\( p(x) \\) that is non-zero at each data point is a potential candidate.",
      "tags": [
        "ch03",
        "density-estimation",
        "definition"
      ]
    },
    {
      "uid": "03-002",
      "front": "What are parametric distributions?",
      "back": "<b>Parametric distributions</b> are governed by a relatively small number of adjustable parameters (e.g., mean and variance of a Gaussian).<br>To apply them for density estimation, we determine parameter values from data, typically by maximizing the likelihood function.",
      "tags": [
        "ch03",
        "parametric",
        "definition"
      ]
    },
    {
      "uid": "03-003",
      "front": "What are nonparametric density estimation methods?",
      "back": "<b>Nonparametric methods</b> are density estimation approaches where the form of the distribution typically depends on the size of the data set.<br>They still contain parameters, but these control model complexity rather than the distribution form. Examples: histograms, kernel density estimation, K-nearest neighbors.",
      "tags": [
        "ch03",
        "nonparametric",
        "definition"
      ]
    },
    {
      "uid": "03-004",
      "front": "How does deep learning relate to parametric and nonparametric methods?",
      "back": "Deep learning combines the <b>efficiency of parametric models</b> with the <b>generality of nonparametric methods</b> by considering flexible distributions based on neural networks having a large, but fixed, number of parameters.",
      "tags": [
        "ch03",
        "deep-learning",
        "density-estimation"
      ]
    },
    {
      "uid": "03-005",
      "front": "What is the Bernoulli distribution?",
      "back": "The <b>Bernoulli distribution</b> models a binary random variable \\( x \\in \\{0, 1\\} \\):<br>\\( \\text{Bern}(x|\\mu) = \\mu^x (1-\\mu)^{1-x} \\)<br>where \\( \\mu = p(x=1) \\).<br>Mean: \\( E[x] = \\mu \\)<br>Variance: \\( \\text{var}[x] = \\mu(1-\\mu) \\)",
      "tags": [
        "ch03",
        "bernoulli",
        "distribution"
      ]
    },
    {
      "uid": "03-006",
      "front": "What is the binomial distribution?",
      "back": "The <b>binomial distribution</b> gives the probability of \\( m \\) successes in \\( N \\) independent Bernoulli trials:<br>\\( \\text{Bin}(m|N,\\mu) = \\binom{N}{m} \\mu^m (1-\\mu)^{N-m} \\)<br>Mean: \\( E[m] = N\\mu \\)<br>Variance: \\( \\text{var}[m] = N\\mu(1-\\mu) \\)",
      "tags": [
        "ch03",
        "binomial",
        "distribution"
      ]
    },
    {
      "uid": "03-007",
      "front": "What is the binomial coefficient \\( \\binom{N}{m} \\)?",
      "back": "The <b>binomial coefficient</b> is the number of ways to choose \\( m \\) objects from \\( N \\) identical objects without replacement:<br>\\( \\binom{N}{m} = \\frac{N!}{(N-m)!m!} \\)<br>Also written as \\( C(N,m) \\) or \"N choose m\".",
      "tags": [
        "ch03",
        "binomial-coefficient",
        "combinatorics"
      ]
    },
    {
      "uid": "03-008",
      "front": "What is one-hot encoding (1-of-K scheme)?",
      "back": "<b>One-hot encoding</b> represents a discrete variable with K states as a K-dimensional vector where exactly one element equals 1 and all others equal 0.<br>Example: For K=6 states, if state 3 is active:<br>\\( \\mathbf{x} = (0, 0, 1, 0, 0, 0)^T \\)<br>Property: \\( \\sum_{k=1}^{K} x_k = 1 \\)",
      "tags": [
        "ch03",
        "one-hot",
        "encoding"
      ]
    },
    {
      "uid": "03-009",
      "front": "What is the multinomial distribution?",
      "back": "The <b>multinomial distribution</b> is a generalization of the Bernoulli to more than two outcomes (K states).<br>For one observation: \\( p(\\mathbf{x}|\\boldsymbol{\\mu}) = \\prod_{k=1}^{K} \\mu_k^{x_k} \\)<br>where \\( \\sum_k \\mu_k = 1 \\) and \\( \\mu_k = p(x_k = 1) \\).",
      "tags": [
        "ch03",
        "multinomial",
        "distribution"
      ]
    },
    {
      "uid": "03-010",
      "front": "What is the multivariate Gaussian distribution?",
      "back": "For a D-dimensional vector \\( \\mathbf{x} \\):<br>\\( \\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{(2\\pi)^{D/2}|\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu})\\right) \\)<br>where \\( \\boldsymbol{\\mu} \\) is the mean vector and \\( \\boldsymbol{\\Sigma} \\) is the covariance matrix.",
      "tags": [
        "ch03",
        "multivariate-gaussian",
        "distribution"
      ]
    },
    {
      "uid": "03-011",
      "front": "What is the Mahalanobis distance?",
      "back": "The <b>Mahalanobis distance</b> from \\( \\mathbf{x} \\) to \\( \\boldsymbol{\\mu} \\) is:<br>\\( \\Delta^2 = (\\mathbf{x}-\\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x}-\\boldsymbol{\\mu}) \\)<br>It appears in the exponent of the multivariate Gaussian. When \\( \\boldsymbol{\\Sigma} = \\mathbf{I} \\), it reduces to Euclidean distance.",
      "tags": [
        "ch03",
        "mahalanobis",
        "distance"
      ]
    },
    {
      "uid": "03-012",
      "front": "What is the covariance matrix of a random vector?",
      "back": "The <b>covariance matrix</b> of a random vector \\( \\mathbf{x} \\) is:<br>\\( \\text{cov}[\\mathbf{x}] = E[(\\mathbf{x} - E[\\mathbf{x}])(\\mathbf{x} - E[\\mathbf{x}])^T] \\)<br>For the multivariate Gaussian: \\( \\text{cov}[\\mathbf{x}] = \\boldsymbol{\\Sigma} \\)",
      "tags": [
        "ch03",
        "covariance-matrix",
        "definition"
      ]
    },
    {
      "uid": "03-013",
      "front": "What geometric shape do the contours of constant density form for a multivariate Gaussian?",
      "back": "Contours of constant density form <b>ellipsoids</b> centered at \\( \\boldsymbol{\\mu} \\).<br>The axes are oriented along the <b>eigenvectors</b> \\( \\mathbf{u}_i \\) of the covariance matrix, with scaling factors \\( \\lambda_i^{1/2} \\) where \\( \\lambda_i \\) are the eigenvalues.",
      "tags": [
        "ch03",
        "gaussian-geometry",
        "eigenvectors"
      ]
    },
    {
      "uid": "03-014",
      "front": "When is a covariance matrix positive definite vs positive semidefinite?",
      "back": "<ul><li><b>Positive definite</b>: All eigenvalues are strictly positive (\\( \\lambda_i > 0 \\)). Required for a well-defined, normalizable Gaussian.</li></ul><ul><li><b>Positive semidefinite</b>: All eigenvalues are non-negative (\\( \\lambda_i \\geq 0 \\)). If some eigenvalues are zero, the distribution is singular and confined to a lower-dimensional subspace.</li></ul>",
      "tags": [
        "ch03",
        "positive-definite",
        "linear-algebra"
      ]
    },
    {
      "uid": "03-015",
      "front": "How many free parameters does a general multivariate Gaussian have in D dimensions?",
      "back": "A general multivariate Gaussian has:<br><ul><li>Mean \\( \\boldsymbol{\\mu} \\): D parameters</li><li>Covariance \\( \\boldsymbol{\\Sigma} \\) (symmetric): \\( D(D+1)/2 \\) parameters</li></ul><b>Total</b>: \\( D(D+3)/2 \\) parameters<br>This grows quadratically with D, which can be computationally prohibitive.",
      "tags": [
        "ch03",
        "gaussian-parameters",
        "complexity"
      ]
    },
    {
      "uid": "03-016",
      "front": "What are the three common restricted forms of covariance matrices?",
      "back": "<ol><li><b>General</b>: Full \\( D(D+1)/2 \\) parameters, ellipsoidal contours</li></ol><ol><li><b>Diagonal</b>: \\( \\boldsymbol{\\Sigma} = \\text{diag}(\\sigma_i^2) \\), 2D parameters, axis-aligned ellipsoids</li></ol><ol><li><b>Isotropic</b>: \\( \\boldsymbol{\\Sigma} = \\sigma^2 \\mathbf{I} \\), D+1 parameters, spherical contours</li></ol>",
      "tags": [
        "ch03",
        "covariance-types",
        "gaussian"
      ]
    },
    {
      "uid": "03-017",
      "front": "What are the maximum likelihood estimates for the multivariate Gaussian?",
      "back": "<b>Mean</b>: \\( \\boldsymbol{\\mu}_{ML} = \\frac{1}{N}\\sum_{n=1}^{N} \\mathbf{x}_n \\) (sample mean)<br><b>Covariance</b>: \\( \\boldsymbol{\\Sigma}_{ML} = \\frac{1}{N}\\sum_{n=1}^{N} (\\mathbf{x}_n - \\boldsymbol{\\mu}_{ML})(\\mathbf{x}_n - \\boldsymbol{\\mu}_{ML})^T \\)<br>Note: The ML covariance estimate is biased (underestimates true covariance).",
      "tags": [
        "ch03",
        "maximum-likelihood",
        "multivariate-gaussian"
      ]
    },
    {
      "uid": "03-018",
      "front": "What is a mixture distribution?",
      "back": "A <b>mixture distribution</b> is formed by taking linear combinations of more basic distributions.<br>General form: \\( p(\\mathbf{x}) = \\sum_{k=1}^{K} \\pi_k p_k(\\mathbf{x}) \\)<br>where \\( \\pi_k \\) are mixing coefficients satisfying \\( \\sum_k \\pi_k = 1 \\) and \\( 0 \\leq \\pi_k \\leq 1 \\).",
      "tags": [
        "ch03",
        "mixture-models",
        "definition"
      ]
    },
    {
      "uid": "03-019",
      "front": "What is a Gaussian mixture model (GMM)?",
      "back": "A <b>Gaussian mixture model</b> is a superposition of K Gaussian densities:<br>\\( p(\\mathbf{x}) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) \\)<br>Each component has its own mean \\( \\boldsymbol{\\mu}_k \\) and covariance \\( \\boldsymbol{\\Sigma}_k \\). By using enough components, almost any continuous distribution can be approximated.",
      "tags": [
        "ch03",
        "gmm",
        "mixture-models"
      ]
    },
    {
      "uid": "03-020",
      "front": "What are the mixing coefficients in a mixture model?",
      "back": "<b>Mixing coefficients</b> \\( \\pi_k \\) can be interpreted as the prior probability of selecting component k:<br>\\( \\pi_k = p(k) \\)<br>Constraints:<ul><li>\\( \\sum_{k=1}^{K} \\pi_k = 1 \\)</li><li>\\( 0 \\leq \\pi_k \\leq 1 \\)</li></ul>",
      "tags": [
        "ch03",
        "mixing-coefficients",
        "mixture-models"
      ]
    },
    {
      "uid": "03-021",
      "front": "What are responsibilities in a Gaussian mixture model?",
      "back": "<b>Responsibilities</b> \\( \\gamma_k(\\mathbf{x}) \\) are the posterior probabilities that component k generated observation \\( \\mathbf{x} \\):<br>\\( \\gamma_k(\\mathbf{x}) = p(k|\\mathbf{x}) = \\frac{\\pi_k \\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k)}{\\sum_l \\pi_l \\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}_l, \\boldsymbol{\\Sigma}_l)} \\)<br>Computed using Bayes' theorem.",
      "tags": [
        "ch03",
        "responsibilities",
        "gmm"
      ]
    },
    {
      "uid": "03-022",
      "front": "Why is maximum likelihood for Gaussian mixtures more complex than for a single Gaussian?",
      "back": "The log likelihood for a GMM contains a <b>sum inside the logarithm</b>:<br>\\( \\ln p(X|\\pi,\\mu,\\Sigma) = \\sum_{n=1}^{N} \\ln \\left( \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(\\mathbf{x}_n|\\boldsymbol{\\mu}_k, \\boldsymbol{\\Sigma}_k) \\right) \\)<br>This means no closed-form solution exists. Instead, use iterative optimization or the <b>Expectation-Maximization (EM)</b> algorithm.",
      "tags": [
        "ch03",
        "gmm",
        "maximum-likelihood"
      ]
    },
    {
      "uid": "03-023",
      "front": "What is sequential estimation and why is it useful?",
      "back": "<b>Sequential estimation</b> processes data points one at a time rather than in a batch.<br>For the mean: \\( \\mu_{ML}^{(N)} = \\mu_{ML}^{(N-1)} + \\frac{1}{N}(x_N - \\mu_{ML}^{(N-1)}) \\)<br>Useful for:<ul><li>Online applications</li><li>Large datasets where batch processing is infeasible</li><li>Memory-constrained settings</li></ul>",
      "tags": [
        "ch03",
        "sequential-estimation",
        "online-learning"
      ]
    },
    {
      "uid": "03-024",
      "front": "What is a limitation of using a single Gaussian for multimodal data?",
      "back": "A single Gaussian has only one mode (peak). If the data-generating process is <b>multimodal</b> (has multiple clusters), a single Gaussian will:<br><ul><li>Fail to capture the cluster structure</li><li>Place probability mass in regions with sparse data (between clusters)</li></ul>Solution: Use a mixture of Gaussians.",
      "tags": [
        "ch03",
        "gaussian-limitations",
        "multimodal"
      ]
    },
    {
      "uid": "03-025",
      "front": "How do histograms work for density estimation?",
      "back": "<b>Histograms</b> partition the input space into distinct bins of width \\( \\Delta_i \\) and count the number \\( n_i \\) of observations in each bin.<br>Density estimate: \\( p_i = \\frac{n_i}{N \\Delta_i} \\)<br>Advantage: Once computed, original data can be discarded.<br>Disadvantage: Estimated density has discontinuities at bin edges.",
      "tags": [
        "ch03",
        "histogram",
        "nonparametric"
      ]
    },
    {
      "uid": "03-026",
      "front": "What are the two fundamental approaches to nonparametric density estimation from a region R?",
      "back": "Given probability \\( P \\) of falling in region R with volume V and K points inside:<br><ol><li><b>Fix K, determine V from data</b>: K-nearest neighbors</li></ol><ol><li><b>Fix V, determine K from data</b>: Kernel density estimation</li></ol>Both use: \\( p(\\mathbf{x}) \\approx \\frac{K}{NV} \\)",
      "tags": [
        "ch03",
        "nonparametric",
        "density-estimation"
      ]
    },
    {
      "uid": "03-027",
      "front": "What is kernel density estimation (Parzen window)?",
      "back": "<b>Kernel density estimation</b> fixes the volume V and counts points K:<br>\\( p(\\mathbf{x}) = \\frac{1}{N} \\sum_{n=1}^{N} \\frac{1}{h^D} k\\left(\\frac{\\mathbf{x} - \\mathbf{x}_n}{h}\\right) \\)<br>where \\( k(\\cdot) \\) is a kernel function (e.g., Gaussian) and h is the bandwidth controlling smoothness.",
      "tags": [
        "ch03",
        "kernel-density",
        "nonparametric"
      ]
    },
    {
      "uid": "03-028",
      "front": "What is K-nearest neighbors (KNN) density estimation?",
      "back": "<b>K-nearest neighbors</b> fixes K and finds the volume V containing exactly K neighbors:<br>\\( p(\\mathbf{x}) = \\frac{K}{NV} \\)<br>where V is the volume of a sphere centered at \\( \\mathbf{x} \\) containing exactly K data points.<br>The density estimate adapts to local data density.",
      "tags": [
        "ch03",
        "knn",
        "nonparametric"
      ]
    },
    {
      "uid": "03-029",
      "front": "How does K-nearest neighbors work for classification?",
      "back": "To classify a new point:<br><ol><li>Identify the K nearest points from training data</li><li>Assign the new point to the class having the <b>largest number of representatives</b> among those K neighbors</li></ol>This is a simple, non-parametric classifier that requires storing all training data.",
      "tags": [
        "ch03",
        "knn-classification",
        "nonparametric"
      ]
    },
    {
      "uid": "03-030",
      "front": "What is a major limitation of nonparametric methods like KNN and kernel density estimation?",
      "back": "Both methods require storing the <b>entire training data set</b>.<br>The number of parameters effectively grows with dataset size, making them very inefficient for large datasets.<br>Deep learning overcomes this by using flexible neural networks with a large but <b>fixed</b> number of parameters.",
      "tags": [
        "ch03",
        "nonparametric-limitations",
        "scalability"
      ]
    },
    {
      "uid": "03-031",
      "front": "What is the unbiased estimator for the covariance matrix of a multivariate Gaussian?",
      "back": "The unbiased covariance estimator is:<br>\\( \\tilde{\\boldsymbol{\\Sigma}} = \\frac{1}{N-1}\\sum_{n=1}^{N} (\\mathbf{x}_n - \\boldsymbol{\\mu}_{ML})(\\mathbf{x}_n - \\boldsymbol{\\mu}_{ML})^T \\)<br>Note the \\( N-1 \\) in the denominator instead of \\( N \\).",
      "tags": [
        "ch03",
        "bias-correction",
        "covariance"
      ]
    },
    {
      "uid": "03-032",
      "front": "What is the conditional distribution of a subset of variables in a multivariate Gaussian?",
      "back": "If \\( \\mathbf{x} = (\\mathbf{x}_a, \\mathbf{x}_b) \\) follows a multivariate Gaussian, then the conditional \\( p(\\mathbf{x}_a | \\mathbf{x}_b) \\) is also Gaussian.<br>This is a key property making Gaussians analytically tractable for probabilistic inference.",
      "tags": [
        "ch03",
        "conditional-gaussian",
        "properties"
      ]
    },
    {
      "uid": "03-033",
      "front": "What is the marginal distribution of a subset of variables in a multivariate Gaussian?",
      "back": "If \\( \\mathbf{x} = (\\mathbf{x}_a, \\mathbf{x}_b) \\) follows a multivariate Gaussian with mean \\( (\\boldsymbol{\\mu}_a, \\boldsymbol{\\mu}_b) \\), then:<br>\\( p(\\mathbf{x}_a) = \\mathcal{N}(\\mathbf{x}_a | \\boldsymbol{\\mu}_a, \\boldsymbol{\\Sigma}_{aa}) \\)<br>where \\( \\boldsymbol{\\Sigma}_{aa} \\) is the corresponding block of the covariance matrix.",
      "tags": [
        "ch03",
        "marginal-gaussian",
        "properties"
      ]
    },
    {
      "uid": "03-034",
      "front": "What is the log likelihood function for the multivariate Gaussian?",
      "back": "\\( \\ln p(X|\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = -\\frac{ND}{2}\\ln(2\\pi) - \\frac{N}{2}\\ln|\\boldsymbol{\\Sigma}| - \\frac{1}{2}\\sum_{n=1}^{N}(\\mathbf{x}_n - \\boldsymbol{\\mu})^T \\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}_n - \\boldsymbol{\\mu}) \\)<br>Maximizing this gives the ML estimates for mean and covariance.",
      "tags": [
        "ch03",
        "log-likelihood",
        "multivariate-gaussian"
      ]
    },
    {
      "uid": "03-035",
      "front": "What is the precision matrix?",
      "back": "The <b>precision matrix</b> is the inverse of the covariance matrix:<br>\\( \\boldsymbol{\\Lambda} = \\boldsymbol{\\Sigma}^{-1} \\)<br>It appears naturally in the exponent of the multivariate Gaussian. Working with precision is sometimes more convenient for inference.",
      "tags": [
        "ch03",
        "precision-matrix",
        "definition"
      ]
    }
  ]
}
