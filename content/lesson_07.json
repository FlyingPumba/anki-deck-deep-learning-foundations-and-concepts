{
  "id": "07",
  "title": "Lesson 07: Gradient Descent",
  "lesson_title": "Gradient Descent",
  "objectives": [
    "Understand automatic differentiation and backpropagation",
    "Learn forward vs reverse mode autodiff",
    "Master regularization techniques: L1, L2, early stopping",
    "Understand inductive bias and symmetry in deep learning",
    "Learn about double descent and model complexity"
  ],
  "cards": [
    {
      "uid": "07-001",
      "front": "What is automatic differentiation (autodiff)?",
      "back": "A technique to compute exact derivatives by:<br><ol><li>Taking code that evaluates a function</li><li>Augmenting it to also compute derivatives</li></ol>Unlike numerical differentiation (finite differences) or symbolic differentiation, autodiff:<br><ul><li>Has no approximation error</li><li>Handles control flow (loops, conditionals)</li><li>Scales efficiently</li></ul>",
      "tags": [
        "ch07",
        "autodiff",
        "differentiation"
      ]
    },
    {
      "uid": "07-002",
      "front": "What is the evaluation trace in automatic differentiation?",
      "back": "A directed acyclic graph (DAG) representing the sequence of elementary operations in a computation.<br>Each node is an intermediate variable. Edges connect operations.<br><ul><li>pa(i): parents of node i (inputs to the operation)</li><li>ch(i): children of node i (operations that use this value)</li></ul>",
      "tags": [
        "ch07",
        "autodiff",
        "evaluation-trace"
      ]
    },
    {
      "uid": "07-003",
      "front": "What are primal and tangent variables in forward-mode autodiff?",
      "back": "<b>Primal variable</b> \\( z_i \\): The actual computed value at node i<br><b>Tangent variable</b> \\( \\dot{z}_i \\): The derivative \\( \\frac{\\partial z_i}{\\partial x_j} \\) with respect to one input \\( x_j \\)<br>Both are computed together in a single forward pass.",
      "tags": [
        "ch07",
        "forward-mode",
        "autodiff"
      ]
    },
    {
      "uid": "07-004",
      "front": "What is forward propagation in neural networks?",
      "back": "Computing the network output by passing inputs through layers sequentially:<br>\\( \\vec{x} \\to \\vec{z}^{(1)} \\to \\vec{z}^{(2)} \\to \\cdots \\to \\vec{y} \\)<br>Each layer applies: linear transformation + activation function.<br>Must be done before backpropagation.",
      "tags": [
        "ch07",
        "forward-propagation",
        "neural-networks"
      ]
    },
    {
      "uid": "07-005",
      "front": "What is the backpropagation formula?",
      "back": "For hidden unit j:<br>\\( \\delta_j = h'(a_j) \\sum_{k} w_{kj} \\delta_k \\)<br>Where:<br><ul><li>\\( \\delta_j = \\frac{\\partial E}{\\partial a_j} \\) is the error signal</li><li>Sum is over units k that receive input from j</li><li>\\( h' \\) is the activation derivative</li></ul>Errors propagate backwards from output to input.",
      "tags": [
        "ch07",
        "backpropagation",
        "formula"
      ]
    },
    {
      "uid": "07-006",
      "front": "How is the gradient with respect to a weight computed in backpropagation?",
      "back": "\\( \\frac{\\partial E}{\\partial w_{ji}} = \\delta_j \\cdot z_i \\)<br>The derivative is the product of:<br><ul><li>The error signal \\( \\delta_j \\) at the receiving unit</li><li>The activation \\( z_i \\) of the sending unit</li></ul>Simply multiply 'error signal' by 'input to the link'.",
      "tags": [
        "ch07",
        "backpropagation",
        "weight-gradient"
      ]
    },
    {
      "uid": "07-007",
      "front": "What is the computational complexity of backpropagation?",
      "back": "<b>O(W)</b> where W is the number of weights.<br>For networks with non-sparse connections, W dominates and the cost is essentially the same as a forward pass.<br>This is why gradient-based training is practical for large networks.",
      "tags": [
        "ch07",
        "backpropagation",
        "complexity"
      ]
    },
    {
      "uid": "07-008",
      "front": "What is the Jacobian matrix in neural networks?",
      "back": "Matrix of partial derivatives of outputs with respect to inputs:<br>\\( J_{ki} = \\frac{\\partial y_k}{\\partial x_i} \\)<br>Provides a measure of <b>local sensitivity</b> of outputs to input changes.<br>Useful for understanding network behavior and some training algorithms.",
      "tags": [
        "ch07",
        "jacobian",
        "sensitivity"
      ]
    },
    {
      "uid": "07-009",
      "front": "What are the drawbacks of numerical differentiation?",
      "back": "<b>Limited accuracy</b>: Finite difference approximation has truncation error.<br><b>Computational cost</b>: O(W) function evaluations needed for W parameters.<br>Symmetrical central differences improve accuracy but double the cost.",
      "tags": [
        "ch07",
        "numerical-differentiation",
        "limitations"
      ]
    },
    {
      "uid": "07-010",
      "front": "What are the drawbacks of symbolic differentiation?",
      "back": "<ol><li><b>Expression swell</b>: Resulting expressions can be exponentially larger than original</li></ol><ol><li><b>Requires closed-form expressions</b>: Cannot handle control flow (loops, conditionals)</li></ol><ol><li><b>Redundant computation</b>: May recompute shared subexpressions</li></ol>",
      "tags": [
        "ch07",
        "symbolic-differentiation",
        "limitations"
      ]
    },
    {
      "uid": "07-011",
      "front": "What is forward-mode automatic differentiation?",
      "back": "Computes derivatives by propagating tangent values <b>forward</b> through the computation graph.<br>For function with D inputs and K outputs:<br><ul><li>One forward pass computes derivatives w.r.t. <b>one input</b></li><li>Need D passes to get all gradients</li></ul>Efficient when K >> D (few inputs, many outputs).",
      "tags": [
        "ch07",
        "forward-mode",
        "autodiff"
      ]
    },
    {
      "uid": "07-012",
      "front": "What is reverse-mode automatic differentiation?",
      "back": "Computes derivatives by propagating <b>adjoint variables</b> backward through the computation graph.<br>For function with D inputs and K outputs:<br><ul><li>One backward pass computes derivatives w.r.t. <b>all inputs</b></li><li>Need K passes for all output gradients</li></ul>Efficient when D >> K (many inputs, few outputs). <b>Backpropagation is a special case.</b>",
      "tags": [
        "ch07",
        "reverse-mode",
        "autodiff"
      ]
    },
    {
      "uid": "07-013",
      "front": "What are adjoint variables in reverse-mode autodiff?",
      "back": "\\( \\bar{z}_i = \\frac{\\partial E}{\\partial z_i} \\)<br>The derivative of the final output (loss) with respect to intermediate variable \\( z_i \\).<br>Computed by backward propagation:<br>\\( \\bar{z}_i = \\sum_{j \\in \\text{ch}(i)} \\bar{z}_j \\frac{\\partial z_j}{\\partial z_i} \\)",
      "tags": [
        "ch07",
        "adjoint",
        "reverse-mode"
      ]
    },
    {
      "uid": "07-014",
      "front": "Why is reverse-mode autodiff more memory intensive than forward-mode?",
      "back": "Reverse mode requires storing <b>all intermediate values</b> from the forward pass to use during the backward pass.<br>Forward mode computes primal and tangent values together, discarding them as it goes.<br>Memory vs. computation tradeoff.",
      "tags": [
        "ch07",
        "memory",
        "autodiff"
      ]
    },
    {
      "uid": "07-015",
      "front": "What is a regularization penalty term?",
      "back": "An additional term in the loss function that penalizes model complexity:<br>\\( \\tilde{E}(\\vec{w}) = E(\\vec{w}) + \\lambda \\Omega(\\vec{w}) \\)<br>Where:<br><ul><li>\\( E \\) is the data error</li><li>\\( \\Omega \\) is the regularizer</li><li>\\( \\lambda \\) controls the strength</li></ul>Reduces overfitting by constraining parameter values.",
      "tags": [
        "ch07",
        "regularization",
        "penalty"
      ]
    },
    {
      "uid": "07-016",
      "front": "What is the bias-variance trade-off in the context of regularization?",
      "back": "Regularization:<br><ul><li><b>Increases bias</b>: Model is more constrained, may underfit</li><li><b>Decreases variance</b>: Less sensitive to training data variations</li></ul>Optimal regularization balances these to minimize total error on test data.",
      "tags": [
        "ch07",
        "bias-variance",
        "regularization"
      ]
    },
    {
      "uid": "07-017",
      "front": "What is weight decay (L2 regularization)?",
      "back": "Regularizer that penalizes large weight magnitudes:<br>\\( \\Omega(\\vec{w}) = \\frac{1}{2}\\|\\vec{w}\\|^2 = \\frac{1}{2}\\sum_i w_i^2 \\)<br>Called 'weight decay' because it encourages weights to <b>decay towards zero</b> unless supported by data.<br>Gradient contribution: \\( \\lambda w_i \\)",
      "tags": [
        "ch07",
        "weight-decay",
        "l2-regularization"
      ]
    },
    {
      "uid": "07-018",
      "front": "How does L2 regularization affect the effective number of parameters?",
      "back": "L2 regularization <b>suppresses parameters</b> that have little effect on the data fit.<br>The <b>effective number of parameters</b> is the number that remain active (non-negligible) after regularization.<br>This can be much smaller than the total parameter count.",
      "tags": [
        "ch07",
        "effective-parameters",
        "l2-regularization"
      ]
    },
    {
      "uid": "07-019",
      "front": "Why are biases typically excluded from weight decay regularization?",
      "back": "Regularizers should be <b>invariant to re-scaling of weights and shifts in biases</b>.<br>Including biases in regularization would:<br><ul><li>Penalize constant offsets in the data</li><li>Not be invariant to data preprocessing</li></ul>Only weights connecting units are regularized.",
      "tags": [
        "ch07",
        "bias",
        "regularization"
      ]
    },
    {
      "uid": "07-020",
      "front": "What is L1 regularization (lasso)?",
      "back": "Regularizer using absolute values:<br>\\( \\Omega(\\vec{w}) = \\sum_i |w_i| \\)<br>Key property: Produces <b>sparse solutions</b> where many weights are exactly zero.<br>As \\( \\lambda \\) increases, more parameters are driven to zero.<br>Useful for feature selection.",
      "tags": [
        "ch07",
        "l1-regularization",
        "lasso"
      ]
    },
    {
      "uid": "07-021",
      "front": "Why does L1 regularization produce sparse solutions while L2 does not?",
      "back": "L1 penalty has a <b>discontinuous gradient</b> at zero (corners in the constraint region).<br>L2 penalty has a <b>smooth gradient</b> that approaches zero as weights approach zero.<br>The corners of the L1 constraint region tend to intersect the error contours at sparse points.",
      "tags": [
        "ch07",
        "sparsity",
        "l1-vs-l2"
      ]
    },
    {
      "uid": "07-022",
      "front": "What is early stopping?",
      "back": "A regularization technique that <b>stops training before convergence</b> based on validation error.<br>As training progresses:<br><ul><li>Training error decreases</li><li>Validation error initially decreases, then increases (overfitting)</li></ul>Stop at minimum validation error.",
      "tags": [
        "ch07",
        "early-stopping",
        "regularization"
      ]
    },
    {
      "uid": "07-023",
      "front": "How does early stopping relate to weight decay?",
      "back": "Early stopping is approximately equivalent to L2 regularization.<br>The <b>effective number of parameters grows</b> during training as weights move from initialization.<br>Stopping early limits how far weights can move, similar to constraining their magnitude.",
      "tags": [
        "ch07",
        "early-stopping",
        "weight-decay"
      ]
    },
    {
      "uid": "07-024",
      "front": "What are learning curves?",
      "back": "Plots of performance measures (training/validation error) vs. training progress.<br>Used to diagnose:<br><ul><li>Overfitting (training error << validation error)</li><li>Underfitting (both errors high)</li><li>When to apply early stopping</li></ul>Can also plot vs. dataset size or model complexity.",
      "tags": [
        "ch07",
        "learning-curves",
        "diagnostics"
      ]
    },
    {
      "uid": "07-025",
      "front": "What is inductive bias in machine learning?",
      "back": "The preference for one hypothesis over others, beyond what the data supports.<br>Also called <b>learning bias</b>.<br>Examples:<br><ul><li>Preference for simpler models (Occam's razor)</li><li>Assumption of smoothness</li><li>Translation equivariance in CNNs</li></ul>Necessary for generalization beyond training data.",
      "tags": [
        "ch07",
        "inductive-bias",
        "learning"
      ]
    },
    {
      "uid": "07-026",
      "front": "What is the no free lunch theorem?",
      "back": "States that <b>every learning algorithm is as good as any other when averaged over all possible problems</b>.<br>Implication: If an algorithm is better than average on some problems, it must be <b>worse than average on others</b>.<br>No universal best algorithm exists.",
      "tags": [
        "ch07",
        "no-free-lunch",
        "theory"
      ]
    },
    {
      "uid": "07-027",
      "front": "What is translation invariance vs. translation equivariance?",
      "back": "<b>Invariance</b>: Output unchanged when input is translated.<br>\\( f(T(x)) = f(x) \\)<br><b>Equivariance</b>: Output transforms in the same way as input.<br>\\( f(T(x)) = T(f(x)) \\)<br>Invariance is a special case where the output transformation is identity.",
      "tags": [
        "ch07",
        "invariance",
        "equivariance"
      ]
    },
    {
      "uid": "07-028",
      "front": "What is geometric deep learning?",
      "back": "A framework for designing neural networks that respect <b>symmetries</b> in data.<br>Key idea: Build invariances/equivariances into network architecture.<br>Examples:<br><ul><li>CNNs: Translation equivariance</li><li>Graph NNs: Permutation equivariance</li><li>Spherical CNNs: Rotation equivariance</li></ul>",
      "tags": [
        "ch07",
        "geometric-deep-learning",
        "symmetry"
      ]
    },
    {
      "uid": "07-029",
      "front": "What are four ways to incorporate symmetries into neural networks?",
      "back": "<ol><li><b>Pre-processing</b>: Transform inputs to be invariant</li></ol><ol><li><b>Regularized error function</b>: Penalize changes under transformations</li></ol><ol><li><b>Data augmentation</b>: Expand training set with transformed examples</li></ol><ol><li><b>Network architecture</b>: Build equivariance into structure (e.g., CNNs)</li></ol>",
      "tags": [
        "ch07",
        "symmetry",
        "approaches"
      ]
    },
    {
      "uid": "07-030",
      "front": "What is the double descent phenomenon?",
      "back": "A non-monotonic relationship between model complexity and test error:<br><ol><li><b>Classic regime</b>: Test error decreases then increases (bias-variance tradeoff)</li><li><b>Interpolation threshold</b>: Error peaks when model can exactly fit training data</li><li><b>Modern regime</b>: Error decreases again with over-parameterization</li></ol>Challenges classical understanding of overfitting.",
      "tags": [
        "ch07",
        "double-descent",
        "generalization"
      ]
    },
    {
      "uid": "07-031",
      "front": "What is effective model complexity?",
      "back": "The <b>maximum training set size</b> that a model can interpolate (fit exactly).<br>Effective complexity increases with:<br><ul><li>More parameters</li><li>More training epochs</li><li>Smaller regularization</li></ul>Not just parameter count, but capacity to fit data.",
      "tags": [
        "ch07",
        "effective-complexity",
        "model-capacity"
      ]
    },
    {
      "uid": "07-032",
      "front": "Why can increasing model size sometimes improve generalization?",
      "back": "In the over-parameterized regime:<br><ul><li>Many solutions can interpolate training data</li><li>Optimization and regularization select <b>simpler</b> solutions</li><li>Larger models can find smoother interpolations</li></ul>This explains double descent: after the interpolation threshold, more capacity helps.",
      "tags": [
        "ch07",
        "over-parameterization",
        "generalization"
      ]
    },
    {
      "uid": "07-033",
      "front": "What is model-based machine learning?",
      "back": "An approach where:<br><ol><li><b>Domain knowledge</b> is encoded in model structure</li><li><b>Inductive biases</b> are explicitly designed</li><li>Learning is constrained by the model's assumptions</li></ol>Contrast with purely data-driven approaches. Combines expert knowledge with learning from data.",
      "tags": [
        "ch07",
        "model-based",
        "inductive-bias"
      ]
    }
  ]
}
