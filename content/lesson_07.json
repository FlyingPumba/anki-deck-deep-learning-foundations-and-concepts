{
  "id": "07",
  "title": "Lesson 07: Automatic Differentiation & Backprop Mechanics",
  "lesson_title": "Automatic Differentiation & Backprop Mechanics",
  "objectives": [
    "Understand automatic differentiation and backpropagation",
    "Learn forward vs reverse mode autodiff",
    "Master regularization techniques: L1, L2, early stopping",
    "Understand inductive bias and symmetry in deep learning",
    "Learn about double descent and model complexity"
  ],
  "cards": [
    {
      "uid": "deep-learning-foundations-and-concepts-07-001",
      "front": "What is automatic differentiation (autodiff)?",
      "back": "A technique to compute exact derivatives by:<br><ol><li>Taking code that evaluates a function</li><li>Augmenting it to also compute derivatives</li></ol>Unlike numerical differentiation (finite differences) or symbolic differentiation, autodiff:<br><ul><li>Has no approximation error</li><li>Handles control flow (loops, conditionals)</li><li>Scales efficiently</li></ul>",
      "tags": [
        "ch07",
        "autodiff",
        "differentiation"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-07-002",
      "front": "What is the evaluation trace in automatic differentiation?",
      "back": "A directed acyclic graph (DAG) representing the sequence of elementary operations in a computation.<br>Each node is an intermediate variable. Edges connect operations.<br><ul><li>pa(i): parents of node i (inputs to the operation)</li><li>ch(i): children of node i (operations that use this value)</li></ul>",
      "tags": [
        "ch07",
        "autodiff",
        "evaluation-trace"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-07-003",
      "front": "What are primal and tangent variables in forward-mode autodiff?",
      "back": "<b>Primal variable</b> \\( z_i \\): The actual computed value at node i<br><b>Tangent variable</b> \\( \\dot{z}_i \\): The derivative \\( \\frac{\\partial z_i}{\\partial x_j} \\) with respect to one input \\( x_j \\)<br>Both are computed together in a single forward pass.",
      "tags": [
        "ch07",
        "forward-mode",
        "autodiff"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-07-004",
      "front": "What is forward propagation in neural networks?",
      "back": "Computing the network output by passing inputs through layers sequentially:<br>\\( \\vec{x} \\to \\vec{z}^{(1)} \\to \\vec{z}^{(2)} \\to \\cdots \\to \\vec{y} \\)<br>Each layer applies: linear transformation + activation function.<br>Must be done before backpropagation.",
      "tags": [
        "ch07",
        "forward-propagation",
        "neural-networks"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-07-005",
      "front": "What is the backpropagation formula?",
      "back": "For hidden unit j:<br>\\( \\delta_j = h'(a_j) \\sum_{k} w_{kj} \\delta_k \\)<br>Where:<br><ul><li>\\( \\delta_j = \\frac{\\partial E}{\\partial a_j} \\) is the error signal</li><li>Sum is over units k that receive input from j</li><li>\\( h' \\) is the activation derivative</li></ul>Errors propagate backwards from output to input.",
      "tags": [
        "ch07",
        "backpropagation",
        "formula"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-07-006",
      "front": "How is the gradient with respect to a weight computed in backpropagation?",
      "back": "\\( \\frac{\\partial E}{\\partial w_{ji}} = \\delta_j \\cdot z_i \\)<br>The derivative is the product of:<br><ul><li>The error signal \\( \\delta_j \\) at the receiving unit</li><li>The activation \\( z_i \\) of the sending unit</li></ul>Simply multiply 'error signal' by 'input to the link'.",
      "tags": [
        "ch07",
        "backpropagation",
        "weight-gradient"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-07-007",
      "front": "What is the computational complexity of backpropagation?",
      "back": "<b>O(W)</b> where W is the number of weights.<br>For networks with non-sparse connections, W dominates and the cost is essentially the same as a forward pass.<br>This is why gradient-based training is practical for large networks.",
      "tags": [
        "ch07",
        "backpropagation",
        "complexity"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-07-008",
      "front": "What is the Jacobian matrix in neural networks?",
      "back": "Matrix of partial derivatives of outputs with respect to inputs:<br>\\( J_{ki} = \\frac{\\partial y_k}{\\partial x_i} \\)<br>Provides a measure of <b>local sensitivity</b> of outputs to input changes.<br>Useful for understanding network behavior and some training algorithms.",
      "tags": [
        "ch07",
        "jacobian",
        "sensitivity"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-07-009",
      "front": "What are the drawbacks of numerical differentiation?",
      "back": "<b>Limited accuracy</b>: Finite difference approximation has truncation error.<br><b>Computational cost</b>: O(W) function evaluations needed for W parameters.<br>Symmetrical central differences improve accuracy but double the cost.",
      "tags": [
        "ch07",
        "numerical-differentiation",
        "limitations"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-07-010",
      "front": "What are the drawbacks of symbolic differentiation?",
      "back": "<ol><li><b>Expression swell</b>: Resulting expressions can be exponentially larger than original</li><li><b>Requires closed-form expressions</b>: Cannot handle control flow (loops, conditionals)</li><li><b>Redundant computation</b>: May recompute shared subexpressions</li></ol>",
      "tags": [
        "ch07",
        "symbolic-differentiation",
        "limitations"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-07-011",
      "front": "What is forward-mode automatic differentiation?",
      "back": "Computes derivatives by propagating tangent values <b>forward</b> through the computation graph.<br>For function with D inputs and K outputs:<br><ul><li>One forward pass computes derivatives w.r.t. <b>one input</b></li><li>Need D passes to get all gradients</li></ul>Efficient when K >> D (few inputs, many outputs).",
      "tags": [
        "ch07",
        "forward-mode",
        "autodiff"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-07-012",
      "front": "What is reverse-mode automatic differentiation?",
      "back": "Computes derivatives by propagating <b>adjoint variables</b> backward through the computation graph.<br>For function with D inputs and K outputs:<br><ul><li>One backward pass computes derivatives w.r.t. <b>all inputs</b></li><li>Need K passes for all output gradients</li></ul>Efficient when D >> K (many inputs, few outputs). <b>Backpropagation is a special case.</b>",
      "tags": [
        "ch07",
        "reverse-mode",
        "autodiff"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-07-013",
      "front": "What are adjoint variables in reverse-mode autodiff?",
      "back": "\\( \\bar{z}_i = \\frac{\\partial E}{\\partial z_i} \\)<br>The derivative of the final output (loss) with respect to intermediate variable \\( z_i \\).<br>Computed by backward propagation:<br>\\( \\bar{z}_i = \\sum_{j \\in \\text{ch}(i)} \\bar{z}_j \\frac{\\partial z_j}{\\partial z_i} \\)",
      "tags": [
        "ch07",
        "adjoint",
        "reverse-mode"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-07-014",
      "front": "Why is reverse-mode autodiff more memory intensive than forward-mode?",
      "back": "Reverse mode requires storing <b>all intermediate values</b> from the forward pass to use during the backward pass.<br>Forward mode computes primal and tangent values together, discarding them as it goes.<br>Memory vs. computation tradeoff.",
      "tags": [
        "ch07",
        "memory",
        "autodiff"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-07-038",
      "front": "What is automatic differentiation and why is it important?",
      "back": "<b>Automatic differentiation</b> automatically generates code for computing error function gradients (backpropagation) from the forward propagation code.<br>This allows researchers to rapidly experiment with different architectures since only the forward pass needs to be coded explicitly.",
      "tags": [
        "ch07",
        "autodiff",
        "training"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-07-039",
      "front": "What is error backpropagation?",
      "back": "<b>Error backpropagation</b> (or just backprop) is an efficient algorithm for computing the derivatives of the error function with respect to all network parameters. Information flows backwards through the network from outputs towards inputs.",
      "tags": [
        "ch07",
        "backpropagation",
        "training"
      ]
    }
  ]
}
