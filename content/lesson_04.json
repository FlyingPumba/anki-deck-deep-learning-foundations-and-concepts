{
  "id": "04",
  "title": "Lesson 04: Single-layer Networks: Regression",
  "lesson_title": "Single-layer Networks: Regression",
  "objectives": [
    "Understand linear regression as a single-layer neural network",
    "Learn basis functions and feature extraction",
    "Master maximum likelihood for regression",
    "Understand the bias-variance trade-off",
    "Learn decision theory for regression",
    "Understand regularization and weight decay"
  ],
  "cards": [
    {
      "uid": "deep-learning-foundations-and-concepts-04-001",
      "front": "What is the goal of regression?",
      "back": "To predict the value of one or more <b>continuous target variables</b> \\( t \\) given the value of a D-dimensional vector \\( \\vec{x} \\) of input variables.",
      "tags": [
        "ch04",
        "regression",
        "basics"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-04-002",
      "front": "What is the bias parameter in a linear regression model?",
      "back": "The parameter \\( w_0 \\) that allows for any fixed offset in the data:<br>\\( y(\\vec{x}, \\vec{w}) = w_0 + w_1 x_1 + \\ldots + w_D x_D \\)<br>Also called the intercept. Not to be confused with statistical bias.",
      "tags": [
        "ch04",
        "regression",
        "parameters"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-04-004",
      "front": "How can a polynomial regression model be expressed using basis functions?",
      "back": "For a single input variable \\( x \\), use basis functions:<br>\\( \\phi_j(x) = x^j \\)<br>So:<br>\\( y(x, \\vec{w}) = w_0 + w_1 x + w_2 x^2 + \\ldots = \\sum_{j=0}^{M-1} w_j x^j \\)",
      "tags": [
        "ch04",
        "polynomial",
        "basis-functions"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-04-005",
      "front": "Why are models of the form \\( y(\\vec{x}, \\vec{w}) = \\sum_j w_j \\phi_j(\\vec{x}) \\) called linear models?",
      "back": "Because they are <b>linear in the parameters</b> \\( \\vec{w} \\), even though they can be nonlinear in the inputs \\( \\vec{x} \\) (via the basis functions \\( \\phi_j \\)).<br>This linearity in parameters greatly simplifies the analysis.",
      "tags": [
        "ch04",
        "linear-models",
        "basis-functions"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-04-006",
      "front": "What is least-squares regression?",
      "back": "A method to find parameters by minimizing the <b>sum-of-squares error function</b>:<br>\\( E_D(\\vec{w}) = \\frac{1}{2} \\sum_{n=1}^{N} \\{t_n - \\vec{w}^T \\phi(\\vec{x}_n)\\}^2 \\)<br>Equivalent to maximum likelihood under Gaussian noise assumption.",
      "tags": [
        "ch04",
        "least-squares",
        "optimization"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-04-007",
      "front": "Why is maximizing likelihood under Gaussian noise equivalent to minimizing sum-of-squares error?",
      "back": "The log likelihood under Gaussian noise is:<br>\\( \\ln p(\\mathbf{t}|X, \\vec{w}, \\sigma^2) = -\\frac{N}{2}\\ln\\sigma^2 - \\frac{N}{2}\\ln(2\\pi) - \\frac{1}{\\sigma^2}E_D(\\vec{w}) \\)<br>The first two terms are constants w.r.t. \\( \\vec{w} \\), so maximizing likelihood is equivalent to minimizing \\( E_D(\\vec{w}) \\).",
      "tags": [
        "ch04",
        "maximum-likelihood",
        "least-squares"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-04-008",
      "front": "What are the normal equations for least-squares regression?",
      "back": "The closed-form solution for \\( \\vec{w} \\):<br>\\( \\vec{w}_{ML} = (\\Phi^T \\Phi)^{-1} \\Phi^T \\mathbf{t} \\)<br>Where \\( \\Phi \\) is the design matrix with elements \\( \\Phi_{nj} = \\phi_j(\\vec{x}_n) \\).",
      "tags": [
        "ch04",
        "normal-equations",
        "least-squares"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-04-009",
      "front": "What is the Moore-Penrose pseudo-inverse?",
      "back": "A generalization of matrix inverse to non-square matrices:<br>\\( \\Phi^\\dagger = (\\Phi^T \\Phi)^{-1} \\Phi^T \\)<br>Used in the normal equations: \\( \\vec{w}_{ML} = \\Phi^\\dagger \\mathbf{t} \\)<br>If \\( \\Phi \\) is square and invertible, \\( \\Phi^\\dagger = \\Phi^{-1} \\).",
      "tags": [
        "ch04",
        "pseudo-inverse",
        "linear-algebra"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-04-010",
      "front": "What is a batch method in machine learning?",
      "back": "A method where the <b>entire training data set is processed at once</b> to update parameters.<br>Contrast with sequential/online methods that process data points one at a time.",
      "tags": [
        "ch04",
        "batch-learning",
        "training"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-04-011",
      "front": "What are sequential algorithms (online algorithms)?",
      "back": "Algorithms that process data points <b>one at a time</b> and then discard them.<br>Important for:<br><ul><li>Online applications</li><li>Large datasets where batch processing is infeasible</li></ul>Also called online algorithms.",
      "tags": [
        "ch04",
        "online-learning",
        "sequential"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-04-013",
      "front": "What is the LMS (least-mean-squares) algorithm?",
      "back": "The application of stochastic gradient descent to the sum-of-squares error function:<br>\\( \\vec{w}^{(\\tau+1)} = \\vec{w}^{(\\tau)} + \\eta(t_n - \\vec{w}^{(\\tau)T}\\phi_n)\\phi_n \\)<br>A classic sequential learning algorithm for linear regression.",
      "tags": [
        "ch04",
        "lms",
        "sgd"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-04-014",
      "front": "What is the simplest form of regularizer in regression?",
      "back": "The sum of squares of the weight vector elements (L2 regularization):<br>\\( E_W(\\vec{w}) = \\frac{1}{2}\\vec{w}^T\\vec{w} = \\frac{1}{2}\\|\\vec{w}\\|^2 \\)<br>Total error: \\( E(\\vec{w}) = E_D(\\vec{w}) + \\lambda E_W(\\vec{w}) \\)<br>Also called <b>parameter shrinkage</b> or <b>weight decay</b>.",
      "tags": [
        "ch04",
        "regularization",
        "weight-decay"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-04-015",
      "front": "What are the two stages in solving a regression problem using probabilistic methods?",
      "back": "<ol><li><b>Inference stage</b>: Learn a model for the predictive distribution \\( p(t|\\vec{x}) \\) from training data</li><li><b>Decision stage</b>: Use the predictive distribution to make predictions, minimizing a loss function</li></ol>This separation is a key principle of decision theory.",
      "tags": [
        "ch04",
        "decision-theory",
        "inference"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-04-016",
      "front": "What is a loss function in regression?",
      "back": "A function that quantifies the penalty for predicting \\( f(\\vec{x}) \\) when the true value is \\( t \\):<br>\\( L(t, f(\\vec{x})) \\)<br>The goal is to choose \\( f \\) to minimize the <b>expected loss</b> under the predictive distribution.",
      "tags": [
        "ch04",
        "loss-function",
        "decision-theory"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-04-018",
      "front": "What is the optimal prediction under squared loss?",
      "back": "The <b>conditional mean</b> (regression function):<br>\\( f^*(\\vec{x}) = E[t|\\vec{x}] = \\int t \\, p(t|\\vec{x}) \\, dt \\)<br>This minimizes the expected squared loss:<br>\\( E[L] = \\int\\int \\{f(\\vec{x}) - t\\}^2 p(\\vec{x}, t) \\, d\\vec{x} \\, dt \\)",
      "tags": [
        "ch04",
        "regression-function",
        "squared-loss"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-04-019",
      "front": "How can you determine a suitable value for the regularization coefficient \\( \\lambda \\)?",
      "back": "Methods include:<br><ul><li><b>Validation set</b>: Hold out data to evaluate different \\( \\lambda \\) values</li><li><b>Cross-validation</b>: K-fold validation to estimate generalization error</li><li><b>Bayesian methods</b>: Place a prior on \\( \\lambda \\) and marginalize</li></ul>The optimal \\( \\lambda \\) balances bias and variance.",
      "tags": [
        "ch04",
        "regularization",
        "model-selection"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-04-020",
      "front": "What is the bias-variance trade-off?",
      "back": "<b>Bias</b>: How far off the average prediction is from the truth (systematic error).<br><b>Variance</b>: How much predictions change across different training sets (sensitivity to data).<br><b>Trade-off</b>:<br>\\( \\text{Expected loss} = (\\text{bias})^2 + \\text{variance} + \\text{noise} \\)<br><ul><li>Flexible models: low bias, high variance (overfit)</li><li>Simple models: high bias, low variance (underfit)</li></ul>Optimal complexity balances these.",
      "tags": [
        "ch04",
        "bias-variance",
        "model-selection"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-04-021",
      "front": "What is the squared bias in the bias-variance decomposition?",
      "back": "The extent to which the <b>average prediction</b> over all possible datasets differs from the true value:<br>\\( (\\text{bias})^2 = \\{E_D[f(\\vec{x}; D)] - h(\\vec{x})\\}^2 \\)<br>Where \\( h(\\vec{x}) \\) is the true underlying function.",
      "tags": [
        "ch04",
        "bias-variance",
        "bias"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-04-022",
      "front": "What is the variance in the bias-variance decomposition?",
      "back": "The extent to which solutions for individual datasets vary around their average:<br>\\( \\text{variance} = E_D[\\{f(\\vec{x}; D) - E_D[f(\\vec{x}; D)]\\}^2] \\)<br>Measures sensitivity to the particular training set used.",
      "tags": [
        "ch04",
        "bias-variance",
        "variance"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-04-023",
      "front": "Why does the bias-variance decomposition have limited practical value?",
      "back": "Because it is based on <b>averages with respect to ensembles of datasets</b>.<br>In practice, we have only a single dataset, so we cannot directly measure these quantities.<br>Useful conceptually but hard to apply directly.",
      "tags": [
        "ch04",
        "bias-variance",
        "limitations"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-04-024",
      "front": "Does overfitting arise in Bayesian settings?",
      "back": "<b>No</b> - overfitting does not arise when we marginalize over parameters in a Bayesian setting.<br>The Bayesian approach integrates over the posterior distribution of parameters rather than using a single point estimate, naturally incorporating uncertainty.",
      "tags": [
        "ch04",
        "overfitting",
        "bayesian"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-04-025",
      "front": "How can averaging help with the bias-variance trade-off?",
      "back": "Averaging predictions from multiple models can reduce variance without increasing bias.<br>This is the basis for <b>ensemble methods</b> like:<br><ul><li>Bagging</li><li>Random forests</li><li>Model averaging</li></ul>Combines multiple high-variance models to get lower variance.",
      "tags": [
        "ch04",
        "ensemble",
        "averaging"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-04-036",
      "front": "Why is least-squares sensitive to outliers?",
      "back": "Because the squared error heavily penalizes large deviations.<br>Outliers produce large residuals that disproportionately influence the solution.<br>Techniques sensitive to a few data points are said to <b>lack robustness</b>.",
      "tags": [
        "ch04",
        "least-squares",
        "robustness"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-04-059",
      "front": "What is Huber loss (smooth L1 loss)?",
      "back": "<b>Huber loss</b> combines MSE and MAE to be robust to outliers:<br><br>\\( L_\\delta(y, f(x)) = \\begin{cases} \\frac{1}{2}(y - f(x))^2 & \\text{if } |y - f(x)| \\leq \\delta \\\\ \\delta |y - f(x)| - \\frac{1}{2}\\delta^2 & \\text{otherwise} \\end{cases} \\)<br><br><b>Intuition</b>: Acts like MSE for small errors (smooth gradients), but like MAE for large errors (less sensitive to outliers).<br><br><b>Properties</b>:<ul><li>Differentiable everywhere (unlike MAE)</li><li>Linear growth for large errors (unlike MSE's quadratic)</li><li>\\( \\delta \\) controls transition point</li></ul><b>Use cases</b>: Regression with outliers, object detection (bounding box regression), reinforcement learning.",
      "tags": [
        "ch04",
        "huber-loss",
        "robust-regression"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-04-060",
      "front": "What is a linear model in machine learning?",
      "back": "A <b>linear model</b> is a function that is linear in its unknown parameters (weights), even if it is nonlinear in the input variables.<br>Example: \\( y(x, w) = w_0 + w_1 x + w_2 x^2 + \\ldots + w_M x^M \\) is linear in \\( w \\) but nonlinear in \\( x \\).",
      "tags": [
        "ch04",
        "linear-models",
        "definition"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-04-061",
      "front": "What is the sum-of-squares error function?",
      "back": "The sum-of-squares error function measures the misfit between predictions and training data:<br>\\( E(w) = \\frac{1}{2} \\sum_{n=1}^{N} \\{y(x_n, w) - t_n\\}^2 \\)<br>where \\( y(x_n, w) \\) is the prediction and \\( t_n \\) is the target value.",
      "tags": [
        "ch04",
        "error-function",
        "loss"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-04-062",
      "front": "What is the root-mean-square (RMS) error and why is it useful?",
      "back": "\\( E_{RMS} = \\sqrt{\\frac{1}{N} \\sum_{n=1}^{N} \\{y(x_n, w) - t_n\\}^2} \\)<br>It is useful because:<br><ul><li>Division by \\( N \\) allows comparison across different dataset sizes</li><li>The square root ensures the error is measured in the same units as the target variable</li></ul>",
      "tags": [
        "ch04",
        "error-function",
        "evaluation"
      ]
    }
  ]
}
