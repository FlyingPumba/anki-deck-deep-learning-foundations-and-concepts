{
  "id": "08",
  "title": "Lesson 08: Backpropagation",
  "lesson_title": "Backpropagation",
  "objectives": [
    "Understand weight sharing and parameter tying",
    "Learn about residual connections and ResNets",
    "Master ensemble methods: committees, bagging, boosting",
    "Understand dropout and its interpretation as model averaging"
  ],
  "cards": [
    {
      "uid": "08-001",
      "front": "What is weight sharing (parameter tying)?",
      "back": "Using the <b>same weight value</b> for multiple connections in a network.<br>The shared value is learned from data.<br>Benefits:<br><ul><li>Reduces number of parameters</li><li>Encodes prior knowledge about structure</li><li>Key mechanism in CNNs (same filter applied everywhere)</li></ul>",
      "tags": [
        "ch08",
        "weight-sharing",
        "architecture"
      ]
    },
    {
      "uid": "08-002",
      "front": "What is the shattered gradients problem?",
      "back": "In very deep networks, gradients become increasingly <b>uncorrelated</b> and noisy across layers.<br>With ReLU activations: exponential increase in the number of linear regions.<br>This makes optimization difficult without special techniques like residual connections.",
      "tags": [
        "ch08",
        "shattered-gradients",
        "deep-networks"
      ]
    },
    {
      "uid": "08-003",
      "front": "What are residual connections (skip connections)?",
      "back": "Connections that <b>add the input directly to the output</b> of a layer:<br>\\( \\vec{y} = F(\\vec{x}) + \\vec{x} \\)<br>A particular form of skip-layer connections.<br>Allow gradients to flow directly through the network, mitigating vanishing gradients.",
      "tags": [
        "ch08",
        "residual-connections",
        "architecture"
      ]
    },
    {
      "uid": "08-004",
      "front": "What is a residual block?",
      "back": "A network module where input is added to output:<br>\\( \\vec{y} = F(\\vec{x}; \\vec{w}) + \\vec{x} \\)<br>The function \\( F \\) learns the <b>residual</b> - what needs to be added to the input.<br>Easier to learn small corrections than full transformations.",
      "tags": [
        "ch08",
        "residual-block",
        "architecture"
      ]
    },
    {
      "uid": "08-005",
      "front": "What is a ResNet?",
      "back": "<b>Residual Network</b>: A deep network composed of multiple residual blocks.<br>Key properties:<br><ul><li>Can train very deep networks (100+ layers)</li><li>Gradients flow through skip connections</li><li>Each block learns residual mapping</li></ul>Enabled breakthrough in image recognition (He et al., 2016).",
      "tags": [
        "ch08",
        "resnet",
        "architecture"
      ]
    },
    {
      "uid": "08-006",
      "front": "Why is the term 'residual' used in ResNets?",
      "back": "Because each block learns \\( F(\\vec{x}) = H(\\vec{x}) - \\vec{x} \\), the <b>residual</b> between desired output \\( H(\\vec{x}) \\) and input \\( \\vec{x} \\).<br>If identity mapping is optimal, the residual is just zero.<br>Easier to push residual to zero than learn identity explicitly.",
      "tags": [
        "ch08",
        "residual",
        "intuition"
      ]
    },
    {
      "uid": "08-007",
      "front": "What are committees (ensembles) in machine learning?",
      "back": "Combining predictions from <b>multiple models</b> to improve performance.<br>Methods:<br><ul><li><b>Averaging</b>: Mean of predictions</li><li><b>Voting</b>: Majority vote for classification</li><li><b>Weighted combination</b>: Based on model quality</li></ul>Reduces variance and often improves accuracy.",
      "tags": [
        "ch08",
        "ensemble",
        "committees"
      ]
    },
    {
      "uid": "08-008",
      "front": "What is bootstrap aggregation (bagging)?",
      "back": "Creating multiple models trained on <b>bootstrap samples</b> of the data.<br>Bootstrap sample: Draw N points <b>with replacement</b> from original N points.<br>Each model sees different subset. Average predictions for final output.<br>Reduces variance without increasing bias.",
      "tags": [
        "ch08",
        "bagging",
        "ensemble"
      ]
    },
    {
      "uid": "08-009",
      "front": "What is a bootstrap dataset?",
      "back": "A dataset \\( X_B \\) created by drawing N points <b>at random with replacement</b> from original dataset X of N points.<br>Some points appear multiple times, others not at all.<br>On average, ~63.2% of original points appear in each bootstrap sample.",
      "tags": [
        "ch08",
        "bootstrap",
        "sampling"
      ]
    },
    {
      "uid": "08-010",
      "front": "Why doesn't ensemble averaging reduce error as much as theory suggests?",
      "back": "Theory: Averaging M models reduces error by factor of M.<br>Reality: <b>Errors are typically highly correlated</b> between models.<br>Correlated errors don't cancel out. Reduction is much smaller than 1/M.<br>Diversity among ensemble members is key to improvement.",
      "tags": [
        "ch08",
        "ensemble",
        "correlation"
      ]
    },
    {
      "uid": "08-011",
      "front": "What is boosting?",
      "back": "An ensemble method that combines multiple <b>weak classifiers</b> sequentially.<br>Each new classifier focuses on examples the previous ones got wrong.<br>Final prediction is weighted combination.<br>Examples: AdaBoost, Gradient Boosting, XGBoost.",
      "tags": [
        "ch08",
        "boosting",
        "ensemble"
      ]
    },
    {
      "uid": "08-012",
      "front": "What is dropout?",
      "back": "A regularization technique that <b>randomly deletes nodes</b> during training.<br>With probability p, each hidden/input node is 'dropped' (set to zero).<br>At test time: Use all nodes but scale activations by (1-p).<br>Can be viewed as implicit model averaging.",
      "tags": [
        "ch08",
        "dropout",
        "regularization"
      ]
    },
    {
      "uid": "08-013",
      "front": "Which nodes does dropout apply to?",
      "back": "Dropout is applied to:<br><ul><li><b>Hidden nodes</b>: Yes</li><li><b>Input nodes</b>: Yes (sometimes)</li><li><b>Output nodes</b>: No</li></ul>Output nodes are never dropped as they define the prediction.",
      "tags": [
        "ch08",
        "dropout",
        "nodes"
      ]
    },
    {
      "uid": "08-014",
      "front": "Why is dropout interpreted as model averaging?",
      "back": "Each dropout mask defines a <b>different sub-network</b>.<br>With N droppable nodes, there are \\( 2^N \\) possible sub-networks.<br>Training samples different networks; inference averages them.<br>Approximate ensemble without training multiple models explicitly.",
      "tags": [
        "ch08",
        "dropout",
        "ensemble"
      ]
    },
    {
      "uid": "08-015",
      "front": "How does dropout affect training time?",
      "back": "Training takes <b>longer</b> with dropout because:<br><ul><li>Each parameter update uses only a subset of nodes</li><li>Updates are noisier (higher variance)</li><li>More epochs needed for convergence</li></ul>But generalization often improves, justifying the extra training.",
      "tags": [
        "ch08",
        "dropout",
        "training"
      ]
    },
    {
      "uid": "08-016",
      "front": "What is Monte Carlo dropout?",
      "back": "Using dropout at <b>test time</b> (not just training) to estimate uncertainty.<br>Run multiple forward passes with different dropout masks.<br>Variance in predictions indicates model uncertainty.<br>Provides approximate Bayesian inference cheaply.",
      "tags": [
        "ch08",
        "mc-dropout",
        "uncertainty"
      ]
    }
  ]
}
