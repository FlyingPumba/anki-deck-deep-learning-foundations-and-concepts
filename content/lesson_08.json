{
  "id": "08",
  "title": "Lesson 08: Model Compression & Robustness",
  "lesson_title": "Model Compression & Robustness",
  "objectives": [
    "Understand weight sharing and parameter tying",
    "Learn about residual connections and ResNets",
    "Master ensemble methods: committees, bagging, boosting",
    "Understand dropout and its interpretation as model averaging"
  ],
  "cards": [
    {
      "uid": "deep-learning-foundations-and-concepts-08-017",
      "front": "What is knowledge distillation?",
      "back": "<b>Knowledge distillation</b> transfers knowledge from a large 'teacher' model to a smaller 'student' model.<br><br><b>Key idea</b>: Train the student to mimic the teacher's <b>soft outputs</b> (probability distributions), not just the hard labels.<br><br><b>Why soft labels help</b>: They encode 'dark knowledge' - relationships between classes. If teacher says 'probably cat (0.7), maybe dog (0.2), unlikely car (0.001)', that's more informative than just 'cat'.<br><br><b>Loss</b>: \\( L = \\alpha L_{hard} + (1-\\alpha) L_{soft} \\)<br>where \\( L_{soft} \\) matches student and teacher distributions.",
      "tags": [
        "ch08",
        "knowledge-distillation",
        "compression"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-08-018",
      "front": "What is temperature in knowledge distillation?",
      "back": "<b>Temperature</b> controls how soft the probability distributions are:<br>\\( p_i = \\frac{\\exp(z_i / T)}{\\sum_j \\exp(z_j / T)} \\)<br><br><b>Effect</b>:<ul><li><b>T = 1</b>: Standard softmax</li><li><b>T > 1</b>: Softer distribution (more uniform)</li><li><b>T -> infinity</b>: Completely uniform</li></ul><b>Why use high T?</b> With T=1, teacher might output 0.99 for correct class, hiding information about other classes. Higher T reveals more structure.<br><br><b>Important</b>: Use same temperature for both teacher and student during distillation; reset to T=1 at inference.",
      "tags": [
        "ch08",
        "knowledge-distillation",
        "temperature"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-08-019",
      "front": "What are the benefits of knowledge distillation?",
      "back": "<b>1. Model compression</b>: Deploy smaller, faster models with similar accuracy.<br><br><b>2. Ensemble to single model</b>: Distill an ensemble's knowledge into one model.<br><br><b>3. Label smoothing effect</b>: Soft targets act as implicit regularization.<br><br><b>4. Cross-architecture transfer</b>: Teacher and student can have different architectures.<br><br><b>Use cases</b>:<ul><li>Mobile deployment</li><li>Edge devices</li><li>Reducing latency</li><li>Training with noisy labels (teacher can denoise)</li></ul>",
      "tags": [
        "ch08",
        "knowledge-distillation",
        "applications"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-08-020",
      "front": "What is model quantization?",
      "back": "<b>Quantization</b> reduces model size by using lower-precision numbers.<br><br><b>Common approaches</b>:<ul><li><b>FP32 to FP16</b>: 2x smaller, minimal accuracy loss</li><li><b>FP32 to INT8</b>: 4x smaller, faster inference</li><li><b>FP32 to INT4</b>: 8x smaller, used for LLM inference</li></ul><b>Types</b>:<ul><li><b>Post-training quantization (PTQ)</b>: Quantize after training</li><li><b>Quantization-aware training (QAT)</b>: Simulate quantization during training</li></ul><b>Trade-off</b>: Lower precision = smaller/faster but potential accuracy loss.<br><br><b>Use cases</b>: Mobile deployment, edge devices, LLM inference (GPTQ, AWQ, GGUF).",
      "tags": [
        "ch08",
        "quantization",
        "compression"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-08-021",
      "front": "What is the difference between PTQ and QAT?",
      "back": "<b>Post-Training Quantization (PTQ)</b>:<ul><li>Quantize a trained model without retraining</li><li>Fast and easy to apply</li><li>May have accuracy loss, especially for aggressive quantization</li><li>Uses calibration data to determine quantization ranges</li></ul><b>Quantization-Aware Training (QAT)</b>:<ul><li>Simulate quantization during training</li><li>Model learns to be robust to quantization noise</li><li>Better accuracy than PTQ</li><li>More expensive (requires training)</li></ul><b>Rule of thumb</b>: Use PTQ first; if accuracy drops too much, try QAT.",
      "tags": [
        "ch08",
        "quantization",
        "training"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-08-022",
      "front": "What is neural network pruning?",
      "back": "<b>Pruning</b> removes unnecessary weights or neurons to create smaller, faster models.<br><br><b>Types</b>:<ul><li><b>Unstructured pruning</b>: Remove individual weights (creates sparse matrices)</li><li><b>Structured pruning</b>: Remove entire neurons, channels, or layers (maintains dense operations)</li></ul><b>Process</b>:<ol><li>Train the full model</li><li>Identify unimportant weights (e.g., small magnitude)</li><li>Remove/zero them out</li><li>Fine-tune to recover accuracy</li></ol><b>Lottery Ticket Hypothesis</b>: Dense networks contain sparse subnetworks that can train to same accuracy from scratch.",
      "tags": [
        "ch08",
        "pruning",
        "compression"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-08-023",
      "front": "What is the Lottery Ticket Hypothesis?",
      "back": "<b>Lottery Ticket Hypothesis</b> (Frankle & Carlin, 2019): Dense neural networks contain sparse subnetworks (<b>winning tickets</b>) that, when trained in isolation from their original initialization, can match the full network's accuracy.<br><br><b>Key insight</b>: It's not just about the sparse architecture, but also the <b>initial weights</b>. Random reinitialization doesn't work.<br><br><b>Implications</b>:<ul><li>Overparameterization may help find good subnetworks</li><li>Pruning + rewinding can find efficient models</li><li>Not all initializations are equal</li></ul><b>Limitation</b>: Finding winning tickets requires training the full network first.",
      "tags": [
        "ch08",
        "lottery-ticket",
        "pruning"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-08-024",
      "front": "What are the different notions of robustness in machine learning?",
      "back": "<b>Adversarial robustness</b>: Resistance to small, intentionally crafted perturbations.<ul><li>Perturbations are often imperceptible to humans</li><li>Evaluated via PGD attacks, AutoAttack</li><li>Defense: Adversarial training, certified defenses</li></ul><b>Distributional robustness</b>: Performance on data from shifted distributions.<ul><li>Domain shift, covariate shift, dataset bias</li><li>Evaluated on OOD benchmarks (ImageNet-C, -R, -A)</li><li>Defense: Data augmentation, domain adaptation</li></ul><b>Label noise robustness</b>: Learning despite mislabeled training examples.<ul><li>Common in web-scraped or crowdsourced data</li><li>Defense: Noise-robust losses, co-teaching, confident learning</li></ul><b>Key differences</b>:<ul><li>Adversarial: Worst-case perturbations, small \\( \\ell_p \\) balls</li><li>Distributional: Natural shifts, may be large but structured</li><li>Label noise: Corruption in training labels, not inputs</li></ul><b>Trade-offs</b>: Adversarial training often hurts clean accuracy; models robust to one shift may fail on others.",
      "tags": [
        "ch08",
        "robustness",
        "adversarial"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-08-025",
      "front": "What is dataset shift (distribution shift) and what are the main types?",
      "back": "<b>Dataset shift</b> means the data distribution changes between training and deployment, so training performance no longer predicts real-world performance.<br><br><b>Common types</b>:<ul><li><b>Covariate shift</b>: \\( p(\\vec{x}) \\) changes but \\( p(y|\\vec{x}) \\) stays the same (inputs look different)</li><li><b>Label shift</b>: \\( p(y) \\) changes but \\( p(\\vec{x}|y) \\) stays the same (class proportions change)</li><li><b>Concept drift</b>: \\( p(y|\\vec{x}) \\) changes (the underlying relationship changes)</li></ul>",
      "tags": [
        "ch08",
        "dataset-shift",
        "generalization"
      ]
    }
  ]
}
