{
  "id": "18",
  "title": "Lesson 18: Normalizing Flows",
  "lesson_title": "Normalizing Flows",
  "objectives": [
    "Understand normalizing flows and invertible transformations",
    "Learn the change of variables formula for densities",
    "Master coupling flows and their architecture",
    "Understand autoregressive flows",
    "Learn about continuous normalizing flows"
  ],
  "cards": [
    {
      "uid": "18-001",
      "front": "What is a normalizing flow?",
      "back": "A generative model using an <b>invertible neural network</b> to transform a simple distribution (e.g., Gaussian) into a complex data distribution.\n\nKey requirement: Both forward x = f(z) and inverse z = g(x) must be computable.\n\nName origin: Probability 'flows' through sequence of transformations.",
      "tags": [
        "ch18",
        "normalizing-flow",
        "definition"
      ]
    },
    {
      "uid": "18-002",
      "front": "What is the change of variables formula for normalizing flows?",
      "back": "\\( p_x(x) = p_z(z(x)) |\\det J(x)| \\)\n\nwhere J is the Jacobian matrix:\n\\( J_{ij} = \\frac{\\partial z_i}{\\partial x_j} \\)\n\nThe determinant accounts for how the transformation stretches/compresses volume.",
      "tags": [
        "ch18",
        "change-of-variables",
        "jacobian"
      ]
    },
    {
      "uid": "18-003",
      "front": "Why must latent and data spaces have the same dimensionality in normalizing flows?",
      "back": "For the transformation to be <b>bijective</b> (invertible):\n\n- Each x corresponds to unique z and vice versa\n- Different dimensionalities would make the mapping non-invertible\n\nThis is a significant limitation compared to VAEs or GANs.",
      "tags": [
        "ch18",
        "normalizing-flow",
        "constraint"
      ]
    },
    {
      "uid": "18-004",
      "front": "Why is the Jacobian determinant important in normalizing flows?",
      "back": "The determinant measures <b>volume change</b> under transformation.\n\nFor invertible functions:\n\n- If det J > 1: Volume expands\n- If det J < 1: Volume contracts\n- If det J = 0: Not invertible\n\nMust be efficiently computable for practical training.",
      "tags": [
        "ch18",
        "jacobian",
        "determinant"
      ]
    },
    {
      "uid": "18-005",
      "front": "How are multiple normalizing flow layers composed?",
      "back": "Chain multiple invertible transformations:\n\n\\( x = f_L \\circ f_{L-1} \\circ \\cdots \\circ f_1(z) \\)\n\nKey property: <b>Determinant of composition is product of determinants</b>.\n\n\\( \\det J = \\prod_{l=1}^{L} \\det J_l \\)\n\nEach layer must be individually invertible.",
      "tags": [
        "ch18",
        "composition",
        "architecture"
      ]
    },
    {
      "uid": "18-006",
      "front": "Why can't standard neural network layers be used for normalizing flows?",
      "back": "Standard layers are typically <b>not invertible</b>:\n\n- ReLU zeros out negative values (information lost)\n- Many-to-one mappings common\n- Dimensionality changes between layers\n\nNeed specially designed invertible architectures.",
      "tags": [
        "ch18",
        "invertibility",
        "architecture"
      ]
    },
    {
      "uid": "18-007",
      "front": "What is a coupling flow?",
      "back": "An invertible layer that splits input into two parts:\n\n\\( x_A = z_A \\)\n\\( x_B = z_B \\odot s(z_A) + t(z_A) \\)\n\nwhere s, t are arbitrary neural networks (scale and translate).\n\n<b>Invertible</b> because \\( z_B = (x_B - t(x_A)) / s(x_A) \\)\n\nJacobian is triangular â†’ determinant is product of diagonal.",
      "tags": [
        "ch18",
        "coupling-flow",
        "architecture"
      ]
    },
    {
      "uid": "18-008",
      "front": "What is the Hadamard product in normalizing flows?",
      "back": "<b>Element-wise multiplication</b> of two vectors:\n\n\\( (a \\odot b)_i = a_i \\cdot b_i \\)\n\nUsed in coupling flows: \\( x_B = z_B \\odot s(z_A) + t(z_A) \\)\n\nAllows different scaling factors for each dimension.",
      "tags": [
        "ch18",
        "hadamard",
        "operation"
      ]
    },
    {
      "uid": "18-009",
      "front": "Why do coupling flows have triangular Jacobians?",
      "back": "Because one part (A) passes through unchanged:\n\n\\( \\frac{\\partial x_A}{\\partial z_B} = 0 \\)\n\nThe Jacobian is block triangular.\n\nDeterminant of triangular matrix = <b>product of diagonal elements</b>.\n\nThis makes training computationally efficient.",
      "tags": [
        "ch18",
        "coupling-flow",
        "jacobian"
      ]
    },
    {
      "uid": "18-010",
      "front": "How do coupling flows achieve expressiveness despite limited per-layer transformations?",
      "back": "By <b>composing two coupling layers</b> with different partitions:\n\nLayer 1: Transform B conditioned on A\nLayer 2: Transform A conditioned on B\n\nAlternating which part is transformed allows all dimensions to interact.",
      "tags": [
        "ch18",
        "coupling-flow",
        "expressiveness"
      ]
    },
    {
      "uid": "18-011",
      "front": "What is RealNVP?",
      "back": "<b>Real-valued Non-Volume Preserving</b> flow.\n\nA type of coupling flow where:\n\n\\( x_B = z_B \\odot \\exp(s(z_A)) + t(z_A) \\)\n\nThe exponential ensures positive scaling.\n\nLog-determinant = sum of s(z_A) outputs.",
      "tags": [
        "ch18",
        "realnvp",
        "architecture"
      ]
    },
    {
      "uid": "18-012",
      "front": "What is an autoregressive flow?",
      "back": "A flow where each output depends on <b>all previous inputs</b>:\n\n\\( x_i = z_i \\cdot s_i(z_{<i}) + t_i(z_{<i}) \\)\n\nJacobian is triangular by construction.\n\n<b>Trade-off</b>: Sampling is sequential (slow), but density evaluation can be parallelized.",
      "tags": [
        "ch18",
        "autoregressive-flow",
        "architecture"
      ]
    },
    {
      "uid": "18-013",
      "front": "What is MAF (Masked Autoregressive Flow)?",
      "back": "An autoregressive flow using <b>masking</b> to enforce autoregressive structure.\n\nForward pass (density evaluation): Parallelizable\n\nInverse pass (sampling): Sequential\n\nOpposite trade-off to IAF (Inverse Autoregressive Flow).",
      "tags": [
        "ch18",
        "maf",
        "autoregressive"
      ]
    },
    {
      "uid": "18-014",
      "front": "What is a continuous normalizing flow?",
      "back": "A flow defined by a <b>differential equation</b>:\n\n\\( \\frac{dz}{dt} = f(z(t), t) \\)\n\nContinuous transformation from z(0) to z(T).\n\nAdvantage: More flexible than discrete layer stacking.\n\nRelated to neural ODEs.",
      "tags": [
        "ch18",
        "continuous-flow",
        "ode"
      ]
    },
    {
      "uid": "18-015",
      "front": "What are the advantages of normalizing flows over GANs?",
      "back": "1. <b>Exact likelihood</b>: Can compute p(x) directly\n\n2. <b>Stable training</b>: Standard maximum likelihood\n\n3. <b>No mode collapse</b>: Bijective mapping covers full distribution\n\n4. <b>Latent inference</b>: Can map data x back to latent z",
      "tags": [
        "ch18",
        "normalizing-flow",
        "advantages"
      ]
    },
    {
      "uid": "18-016",
      "front": "What are the limitations of normalizing flows?",
      "back": "1. <b>Same dimensionality</b>: Latent space = data space\n\n2. <b>Architectural constraints</b>: Must maintain invertibility\n\n3. <b>Computational cost</b>: Jacobian determinant calculation\n\n4. <b>Expressiveness</b>: May need many layers for complex distributions",
      "tags": [
        "ch18",
        "normalizing-flow",
        "limitations"
      ]
    },
    {
      "uid": "18-017",
      "front": "Why are linear transformations insufficient for normalizing flows?",
      "back": "Linear transformations are <b>closed under composition</b>.\n\nA sequence of linear layers = single linear transformation.\n\nNo increase in expressiveness from stacking.\n\nNeed <b>nonlinear</b> invertible transformations.",
      "tags": [
        "ch18",
        "linear",
        "limitation"
      ]
    }
  ]
}
