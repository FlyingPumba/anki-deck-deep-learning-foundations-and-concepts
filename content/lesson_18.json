{
  "id": "18",
  "title": "Lesson 18: Normalizing Flows",
  "lesson_title": "Normalizing Flows",
  "objectives": [
    "Understand normalizing flows and invertible transformations",
    "Learn the change of variables formula for densities",
    "Master coupling flows and their architecture",
    "Understand autoregressive flows",
    "Learn about continuous normalizing flows"
  ],
  "cards": [
    {
      "uid": "18-001",
      "front": "What is a normalizing flow?",
      "back": "A generative model using an <b>invertible neural network</b> to transform a simple distribution (e.g., Gaussian) into a complex data distribution.<br>Key requirement: Both forward x = f(z) and inverse z = g(x) must be computable.<br>Name origin: Probability 'flows' through sequence of transformations.",
      "tags": [
        "ch18",
        "normalizing-flow",
        "definition"
      ]
    },
    {
      "uid": "18-002",
      "front": "What is the change of variables formula for normalizing flows?",
      "back": "\\( p_x(x) = p_z(z(x)) |\\det J(x)| \\)<br>where J is the Jacobian matrix:<br>\\( J_{ij} = \\frac{\\partial z_i}{\\partial x_j} \\)<br>The determinant accounts for how the transformation stretches/compresses volume.",
      "tags": [
        "ch18",
        "change-of-variables",
        "jacobian"
      ]
    },
    {
      "uid": "18-003",
      "front": "Why must latent and data spaces have the same dimensionality in normalizing flows?",
      "back": "For the transformation to be <b>bijective</b> (invertible):<br><ul><li>Each x corresponds to unique z and vice versa</li><li>Different dimensionalities would make the mapping non-invertible</li></ul>This is a significant limitation compared to VAEs or GANs.",
      "tags": [
        "ch18",
        "normalizing-flow",
        "constraint"
      ]
    },
    {
      "uid": "18-004",
      "front": "Why is the Jacobian determinant important in normalizing flows?",
      "back": "The determinant measures <b>volume change</b> under transformation.<br>For invertible functions:<br><ul><li>If det J > 1: Volume expands</li><li>If det J < 1: Volume contracts</li><li>If det J = 0: Not invertible</li></ul>Must be efficiently computable for practical training.",
      "tags": [
        "ch18",
        "jacobian",
        "determinant"
      ]
    },
    {
      "uid": "18-005",
      "front": "How are multiple normalizing flow layers composed?",
      "back": "Chain multiple invertible transformations:<br>\\( x = f_L \\circ f_{L-1} \\circ \\cdots \\circ f_1(z) \\)<br>Key property: <b>Determinant of composition is product of determinants</b>.<br>\\( \\det J = \\prod_{l=1}^{L} \\det J_l \\)<br>Each layer must be individually invertible.",
      "tags": [
        "ch18",
        "composition",
        "architecture"
      ]
    },
    {
      "uid": "18-006",
      "front": "Why can't standard neural network layers be used for normalizing flows?",
      "back": "Standard layers are typically <b>not invertible</b>:<br><ul><li>ReLU zeros out negative values (information lost)</li><li>Many-to-one mappings common</li><li>Dimensionality changes between layers</li></ul>Need specially designed invertible architectures.",
      "tags": [
        "ch18",
        "invertibility",
        "architecture"
      ]
    },
    {
      "uid": "18-007",
      "front": "What is a coupling flow?",
      "back": "An invertible layer that splits input into two parts:<br>\\( x_A = z_A \\)<br>\\( x_B = z_B \\odot s(z_A) + t(z_A) \\)<br>where s, t are arbitrary neural networks (scale and translate).<br><b>Invertible</b> because \\( z_B = (x_B - t(x_A)) / s(x_A) \\)<br>Jacobian is triangular â†’ determinant is product of diagonal.",
      "tags": [
        "ch18",
        "coupling-flow",
        "architecture"
      ]
    },
    {
      "uid": "18-008",
      "front": "What is the Hadamard product in normalizing flows?",
      "back": "<b>Element-wise multiplication</b> of two vectors:<br>\\( (a \\odot b)_i = a_i \\cdot b_i \\)<br>Used in coupling flows: \\( x_B = z_B \\odot s(z_A) + t(z_A) \\)<br>Allows different scaling factors for each dimension.",
      "tags": [
        "ch18",
        "hadamard",
        "operation"
      ]
    },
    {
      "uid": "18-009",
      "front": "Why do coupling flows have triangular Jacobians?",
      "back": "Because one part (A) passes through unchanged:<br>\\( \\frac{\\partial x_A}{\\partial z_B} = 0 \\)<br>The Jacobian is block triangular.<br>Determinant of triangular matrix = <b>product of diagonal elements</b>.<br>This makes training computationally efficient.",
      "tags": [
        "ch18",
        "coupling-flow",
        "jacobian"
      ]
    },
    {
      "uid": "18-010",
      "front": "How do coupling flows achieve expressiveness despite limited per-layer transformations?",
      "back": "By <b>composing two coupling layers</b> with different partitions:<br>Layer 1: Transform B conditioned on A<br>Layer 2: Transform A conditioned on B<br>Alternating which part is transformed allows all dimensions to interact.",
      "tags": [
        "ch18",
        "coupling-flow",
        "expressiveness"
      ]
    },
    {
      "uid": "18-011",
      "front": "What is RealNVP?",
      "back": "<b>Real-valued Non-Volume Preserving</b> flow.<br>A type of coupling flow where:<br>\\( x_B = z_B \\odot \\exp(s(z_A)) + t(z_A) \\)<br>The exponential ensures positive scaling.<br>Log-determinant = sum of s(z_A) outputs.",
      "tags": [
        "ch18",
        "realnvp",
        "architecture"
      ]
    },
    {
      "uid": "18-012",
      "front": "What is an autoregressive flow?",
      "back": "A flow where each output depends on <b>all previous inputs</b>:<br>\\( x_i = z_i \\cdot s_i(z_{<i}) + t_i(z_{<i}) \\)<br>Jacobian is triangular by construction.<br><b>Trade-off</b>: Sampling is sequential (slow), but density evaluation can be parallelized.",
      "tags": [
        "ch18",
        "autoregressive-flow",
        "architecture"
      ]
    },
    {
      "uid": "18-013",
      "front": "What is MAF (Masked Autoregressive Flow)?",
      "back": "An autoregressive flow using <b>masking</b> to enforce autoregressive structure.<br>Forward pass (density evaluation): Parallelizable<br>Inverse pass (sampling): Sequential<br>Opposite trade-off to IAF (Inverse Autoregressive Flow).",
      "tags": [
        "ch18",
        "maf",
        "autoregressive"
      ]
    },
    {
      "uid": "18-014",
      "front": "What is a continuous normalizing flow?",
      "back": "A flow defined by a <b>differential equation</b>:<br>\\( \\frac{dz}{dt} = f(z(t), t) \\)<br>Continuous transformation from z(0) to z(T).<br>Advantage: More flexible than discrete layer stacking.<br>Related to neural ODEs.",
      "tags": [
        "ch18",
        "continuous-flow",
        "ode"
      ]
    },
    {
      "uid": "18-015",
      "front": "What are the advantages of normalizing flows over GANs?",
      "back": "<ol><li><b>Exact likelihood</b>: Can compute p(x) directly</li></ol><ol><li><b>Stable training</b>: Standard maximum likelihood</li></ol><ol><li><b>No mode collapse</b>: Bijective mapping covers full distribution</li></ol><ol><li><b>Latent inference</b>: Can map data x back to latent z</li></ol>",
      "tags": [
        "ch18",
        "normalizing-flow",
        "advantages"
      ]
    },
    {
      "uid": "18-016",
      "front": "What are the limitations of normalizing flows?",
      "back": "<ol><li><b>Same dimensionality</b>: Latent space = data space</li></ol><ol><li><b>Architectural constraints</b>: Must maintain invertibility</li></ol><ol><li><b>Computational cost</b>: Jacobian determinant calculation</li></ol><ol><li><b>Expressiveness</b>: May need many layers for complex distributions</li></ol>",
      "tags": [
        "ch18",
        "normalizing-flow",
        "limitations"
      ]
    },
    {
      "uid": "18-017",
      "front": "Why are linear transformations insufficient for normalizing flows?",
      "back": "Linear transformations are <b>closed under composition</b>.<br>A sequence of linear layers = single linear transformation.<br>No increase in expressiveness from stacking.<br>Need <b>nonlinear</b> invertible transformations.",
      "tags": [
        "ch18",
        "linear",
        "limitation"
      ]
    }
  ]
}
