{
  "id": "09",
  "title": "Lesson 09: Regularization",
  "lesson_title": "Regularization",
  "objectives": [
    "Understand inductive bias and its role in machine learning",
    "Learn weight decay and L2 regularization",
    "Master early stopping and learning curves",
    "Understand parameter sharing and residual connections",
    "Learn model averaging and dropout"
  ],
  "cards": [
    {
      "uid": "09-001",
      "front": "What is inductive bias in machine learning?",
      "back": "The <b>preference for one hypothesis over others</b> beyond what the data supports.<br>Also called prior knowledge or learning bias.<br>Examples:<br><ul><li>Preference for smoother functions</li><li>Translation invariance in images</li><li>Assumption of linearity</li></ul>Necessary because inverse problems are ill-posed with infinitely many solutions.",
      "tags": [
        "ch09",
        "inductive-bias",
        "theory"
      ]
    },
    {
      "uid": "09-002",
      "front": "Why are machine learning tasks considered inverse problems?",
      "back": "We need to infer an <b>entire distribution</b> from only a finite number of samples.<br>This is intrinsically ill-posed: infinitely many distributions could have generated the observed data.<br>Any distribution with non-zero density at observed points is a candidate.<br>Inductive bias is needed to choose among them.",
      "tags": [
        "ch09",
        "inverse-problems",
        "theory"
      ]
    },
    {
      "uid": "09-003",
      "front": "What is the no free lunch theorem?",
      "back": "States that <b>every learning algorithm is as good as any other when averaged over all possible problems</b>.<br>If an algorithm is better than average on some problems, it must be worse on others.<br>Highlights that learning cannot happen purely from data without bias.",
      "tags": [
        "ch09",
        "no-free-lunch",
        "theory"
      ]
    },
    {
      "uid": "09-004",
      "front": "What are the four approaches to incorporate symmetries into neural networks?",
      "back": "<ol><li><b>Pre-processing</b>: Compute invariant features</li></ol><ol><li><b>Regularized error function</b>: Penalize changes under transformations</li></ol><ol><li><b>Data augmentation</b>: Expand training set with transformed examples</li></ol><ol><li><b>Network architecture</b>: Build equivariance into structure (e.g., CNNs)</li></ol>",
      "tags": [
        "ch09",
        "symmetry",
        "approaches"
      ]
    },
    {
      "uid": "09-005",
      "front": "What is translation invariance vs translation equivariance?",
      "back": "<b>Invariance</b>: Output unchanged when input is translated<br>\\( f(T(\\mathbf{x})) = f(\\mathbf{x}) \\)<br>Example: Object classification - cat remains a cat regardless of position<br><b>Equivariance</b>: Output transforms in the same way as input<br>\\( f(T(\\mathbf{x})) = T(f(\\mathbf{x})) \\)<br>Example: Image segmentation - translated input gives translated segmentation",
      "tags": [
        "ch09",
        "invariance",
        "equivariance"
      ]
    },
    {
      "uid": "09-006",
      "front": "What is data augmentation?",
      "back": "Expanding the training set with <b>transformed versions</b> of training examples.<br>Common transformations for images:<br><ul><li>Translation, rotation, scaling</li><li>Horizontal flip</li><li>Brightness/contrast changes</li><li>Additive noise</li><li>Colour shifts</li></ul>Each transformed example keeps the same target label.",
      "tags": [
        "ch09",
        "data-augmentation",
        "regularization"
      ]
    },
    {
      "uid": "09-007",
      "front": "What is weight decay (L2 regularization)?",
      "back": "A regularizer that penalizes large weight magnitudes:<br>\\( \\tilde{E}(\\mathbf{w}) = E(\\mathbf{w}) + \\frac{\\lambda}{2} \\mathbf{w}^T \\mathbf{w} \\)<br>Gradient contribution: \\( \\lambda \\mathbf{w} \\)<br>Called 'weight decay' because weights decay towards zero unless supported by data.<br>Can be interpreted as Gaussian prior over weights.",
      "tags": [
        "ch09",
        "weight-decay",
        "l2"
      ]
    },
    {
      "uid": "09-008",
      "front": "How does L2 regularization affect the effective number of parameters?",
      "back": "L2 regularization <b>suppresses parameters</b> that have little effect on data fit.<br>Parameters with small effect are pushed towards zero.<br>Effective number of parameters = number that remain 'active' after regularization.<br><ul><li>\\( \\lambda \\to \\infty \\): Effective parameters \\( \\to 0 \\)</li><li>\\( \\lambda = 0 \\): Effective parameters = total parameters</li></ul>",
      "tags": [
        "ch09",
        "effective-parameters",
        "regularization"
      ]
    },
    {
      "uid": "09-009",
      "front": "Why should biases typically be excluded from weight decay?",
      "back": "Regularizers should be <b>invariant to re-scaling of weights and shifts of biases</b>.<br>Including biases would:<br><ul><li>Penalize constant offsets in data</li><li>Not be invariant to data preprocessing</li></ul>Consistent regularizer: \\( \\frac{\\lambda_1}{2} \\sum_{w \\in W_1} w^2 + \\frac{\\lambda_2}{2} \\sum_{w \\in W_2} w^2 \\)<br>where biases are excluded from both sums.",
      "tags": [
        "ch09",
        "weight-decay",
        "biases"
      ]
    },
    {
      "uid": "09-010",
      "front": "What is L1 regularization (Lasso)?",
      "back": "Regularizer using sum of absolute values:<br>\\( \\Omega(\\mathbf{w}) = \\sum_i |w_i| \\)<br>Key property: Produces <b>sparse solutions</b> where many weights are exactly zero.<br>As \\( \\lambda \\) increases, more parameters driven to zero.<br>Useful for feature selection.",
      "tags": [
        "ch09",
        "l1",
        "sparsity"
      ]
    },
    {
      "uid": "09-011",
      "front": "Why does L1 produce sparse solutions while L2 does not?",
      "back": "<b>L1</b>: Constraint region has <b>corners</b> (diamond shape). Error contours tend to intersect at corners where some parameters are exactly zero.<br><b>L2</b>: Constraint region is <b>smooth</b> (circular). Gradient approaches zero as weights approach zero.<br>L1's discontinuous gradient at zero drives parameters exactly to zero.",
      "tags": [
        "ch09",
        "l1-vs-l2",
        "sparsity"
      ]
    },
    {
      "uid": "09-012",
      "front": "What are learning curves?",
      "back": "Plots of performance (training/validation error) vs training progress.<br>Used to diagnose:<br><ul><li><b>Overfitting</b>: Training error << validation error</li><li><b>Underfitting</b>: Both errors high</li><li>When to apply <b>early stopping</b></li></ul>Can also plot vs dataset size or model complexity.",
      "tags": [
        "ch09",
        "learning-curves",
        "diagnostics"
      ]
    },
    {
      "uid": "09-013",
      "front": "What is early stopping?",
      "back": "A regularization technique that <b>stops training before convergence</b> based on validation error.<br>As training progresses:<br><ul><li>Training error decreases</li><li>Validation error initially decreases, then increases (overfitting)</li></ul>Stop at minimum validation error.<br>Approximately equivalent to L2 regularization.",
      "tags": [
        "ch09",
        "early-stopping",
        "regularization"
      ]
    },
    {
      "uid": "09-014",
      "front": "How does early stopping relate to L2 regularization?",
      "back": "Early stopping is approximately <b>equivalent to L2 regularization</b>.<br>The effective number of parameters <b>grows during training</b> as weights move from initialization.<br>Stopping early limits how far weights can move, similar to constraining their magnitude with L2.",
      "tags": [
        "ch09",
        "early-stopping",
        "l2-relation"
      ]
    },
    {
      "uid": "09-015",
      "front": "What is the double descent phenomenon?",
      "back": "A non-monotonic relationship between model complexity and test error:<br><ol><li><b>Classic regime</b>: Error decreases then increases (bias-variance tradeoff)</li></ol><ol><li><b>Interpolation threshold</b>: Error peaks when model can exactly fit training data</li></ol><ol><li><b>Modern regime</b>: Error decreases again with over-parameterization</li></ol>Challenges classical understanding of overfitting.",
      "tags": [
        "ch09",
        "double-descent",
        "generalization"
      ]
    },
    {
      "uid": "09-016",
      "front": "What is parameter sharing (weight tying)?",
      "back": "Using the <b>same weight value</b> for multiple connections in a network.<br>Benefits:<br><ul><li>Reduces number of parameters</li><li>Encodes prior knowledge about structure</li><li>Key mechanism in CNNs (same filter everywhere)</li></ul>The shared value is learned from data.",
      "tags": [
        "ch09",
        "weight-sharing",
        "architecture"
      ]
    },
    {
      "uid": "09-017",
      "front": "What are residual connections (skip connections)?",
      "back": "Connections that <b>add the input directly to the output</b> of a layer:<br>\\( \\mathbf{y} = F(\\mathbf{x}) + \\mathbf{x} \\)<br>Benefits:<br><ul><li>Allow gradients to flow directly through network</li><li>Enable training of very deep networks (100+ layers)</li><li>Each block learns the 'residual' - what needs to be added</li></ul>Foundation of ResNets.",
      "tags": [
        "ch09",
        "residual",
        "architecture"
      ]
    },
    {
      "uid": "09-018",
      "front": "What is dropout?",
      "back": "A regularization technique that <b>randomly drops nodes</b> during training.<br>With probability p, each hidden/input node is set to zero.<br>At test time: Use all nodes but scale activations by (1-p).<br>Interpretation: Implicit ensemble of \\( 2^N \\) sub-networks.",
      "tags": [
        "ch09",
        "dropout",
        "regularization"
      ]
    },
    {
      "uid": "09-019",
      "front": "Why is dropout interpreted as model averaging?",
      "back": "Each dropout mask defines a <b>different sub-network</b>.<br>With N droppable nodes, there are \\( 2^N \\) possible sub-networks.<br><ul><li>Training: Sample different networks</li><li>Inference: Approximate average of all networks</li></ul>Achieves ensemble benefits without training multiple models explicitly.",
      "tags": [
        "ch09",
        "dropout",
        "ensemble"
      ]
    },
    {
      "uid": "09-020",
      "front": "How does dropout prevent co-adaptation of features?",
      "back": "With dropout, each node <b>cannot rely on specific other nodes</b> being present.<br>Nodes must learn robust features useful independently.<br>Prevents complex co-adaptations that might overfit.<br>Forces redundancy and distributed representations.",
      "tags": [
        "ch09",
        "dropout",
        "co-adaptation"
      ]
    },
    {
      "uid": "09-021",
      "front": "What is Monte Carlo dropout?",
      "back": "Using dropout at <b>test time</b> (not just training) to estimate uncertainty.<br>Method:<br><ol><li>Run multiple forward passes with different dropout masks</li><li>Variance in predictions indicates model uncertainty</li></ol>Provides approximate Bayesian inference cheaply.",
      "tags": [
        "ch09",
        "mc-dropout",
        "uncertainty"
      ]
    },
    {
      "uid": "09-022",
      "front": "How does dropout affect training time?",
      "back": "Training takes <b>longer</b> with dropout because:<br><ul><li>Each update uses only a subset of nodes</li><li>Updates are noisier (higher variance)</li><li>More epochs needed for convergence</li></ul>But generalization often improves, justifying extra training.",
      "tags": [
        "ch09",
        "dropout",
        "training"
      ]
    },
    {
      "uid": "09-023",
      "front": "What is the relationship between model size and generalization in modern deep learning?",
      "back": "In the over-parameterized regime:<br><ul><li>Many solutions can interpolate training data</li><li>Optimization and regularization select <b>simpler</b> solutions</li><li>Larger models can find smoother interpolations</li></ul>Best results: Large network + strong regularization<br>Not: Small network with just enough capacity.",
      "tags": [
        "ch09",
        "over-parameterization",
        "generalization"
      ]
    },
    {
      "uid": "09-024",
      "front": "What is a group in the context of symmetries?",
      "back": "A mathematical structure describing transformations that form a symmetry.<br>Four axioms:<br><ol><li><b>Closed</b>: Composition of two elements is in the set</li><li><b>Associative</b>: (A \\( \\circ \\) B) \\( \\circ \\) C = A \\( \\circ \\) (B \\( \\circ \\) C)</li><li><b>Identity</b>: Element I such that A \\( \\circ \\) I = A</li><li><b>Inverse</b>: For each A, exists \\( A^{-1} \\) such that A \\( \\circ A^{-1} \\) = I</li></ol>",
      "tags": [
        "ch09",
        "groups",
        "symmetry"
      ]
    },
    {
      "uid": "09-025",
      "front": "What is geometric deep learning?",
      "back": "A framework for designing neural networks that respect <b>geometric symmetries</b>.<br>Key idea: Build invariances/equivariances into network architecture.<br>Examples:<br><ul><li>CNNs: Translation equivariance</li><li>Graph NNs: Permutation equivariance</li><li>Spherical CNNs: Rotation equivariance</li></ul>Unifying principle for many network architectures.",
      "tags": [
        "ch09",
        "geometric-deep-learning",
        "symmetry"
      ]
    }
  ]
}
