{
  "id": "09",
  "title": "Lesson 09: Regularization & Generalization",
  "lesson_title": "Regularization & Generalization",
  "objectives": [
    "Understand inductive bias and its role in machine learning",
    "Learn weight decay and L2 regularization",
    "Master early stopping and learning curves",
    "Understand parameter sharing and residual connections",
    "Learn model averaging and dropout"
  ],
  "cards": [
    {
      "uid": "deep-learning-foundations-and-concepts-09-001",
      "front": "What is inductive bias in machine learning?",
      "back": "The <b>preference for one hypothesis over others</b> beyond what the data supports.<br>Also called prior knowledge or learning bias.<br>Examples:<br><ul><li>Preference for smoother functions</li><li>Translation invariance in images</li><li>Assumption of linearity</li></ul>Necessary because inverse problems are ill-posed with infinitely many solutions.",
      "tags": [
        "ch09",
        "inductive-bias",
        "theory"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-002",
      "front": "Why are machine learning tasks considered inverse problems?",
      "back": "We need to infer an <b>entire distribution</b> from only a finite number of samples.<br>This is intrinsically ill-posed: infinitely many distributions could have generated the observed data.<br>Any distribution with non-zero density at observed points is a candidate.<br>Inductive bias is needed to choose among them.",
      "tags": [
        "ch09",
        "inverse-problems",
        "theory"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-003",
      "front": "What is the no free lunch theorem?",
      "back": "States that <b>every learning algorithm is as good as any other when averaged over all possible problems</b>.<br>If an algorithm is better than average on some problems, it must be worse on others.<br>Highlights that learning cannot happen purely from data without bias.",
      "tags": [
        "ch09",
        "no-free-lunch",
        "theory"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-004",
      "front": "What are the four approaches to incorporate symmetries into neural networks?",
      "back": "<ol><li><b>Pre-processing</b>: Compute invariant features</li><li><b>Regularized error function</b>: Penalize changes under transformations</li><li><b>Data augmentation</b>: Expand training set with transformed examples</li><li><b>Network architecture</b>: Build equivariance into structure (e.g., CNNs)</li></ol>",
      "tags": [
        "ch09",
        "symmetry",
        "approaches"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-006",
      "front": "What is data augmentation?",
      "back": "Expanding the training set with <b>transformed versions</b> of training examples.<br>Common transformations for images:<br><ul><li>Translation, rotation, scaling</li><li>Horizontal flip</li><li>Brightness/contrast changes</li><li>Additive noise</li><li>Colour shifts</li></ul>Each transformed example keeps the same target label.",
      "tags": [
        "ch09",
        "data-augmentation",
        "regularization"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-009",
      "front": "Why should biases typically be excluded from weight decay?",
      "back": "<b>Intuition</b>: Biases are:<ul><li>Fewer parameters (one per unit vs many weights)</li><li>Mostly controlling baseline activation levels, not complex input-dependent behavior</li></ul>So regularizing them gives little benefit and can hurt - e.g., a ReLU unit may need a positive bias to be 'on' at all.<br><br><b>Technical reason</b>: Regularizers should be invariant to shifts in biases. Including biases would penalize constant offsets in data and not be invariant to data preprocessing.<br><br><b>Nuance</b>: It's not <i>wrong</i> to decay biases - just often unnecessary and makes training more sensitive to arbitrary shifts/centering of data.",
      "tags": [
        "ch09",
        "weight-decay",
        "biases"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-014",
      "front": "How does early stopping relate to L2 regularization?",
      "back": "Early stopping is approximately <b>equivalent to L2 regularization</b>.<br>The effective number of parameters <b>grows during training</b> as weights move from initialization.<br>Stopping early limits how far weights can move, similar to constraining their magnitude with L2.",
      "tags": [
        "ch09",
        "early-stopping",
        "l2-relation"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-020",
      "front": "How does dropout prevent co-adaptation of features?",
      "back": "<b>Co-adaptation</b>: When neurons learn to depend on specific other neurons, creating fragile feature detectors that only work together.<br><br><b>How dropout helps</b>: Since any neuron might be randomly dropped, each neuron must learn features that are useful <i>on their own</i>, not just in combination with specific partners.<br><br>This forces the network to learn redundant, distributed representations rather than brittle specialized ones.",
      "tags": [
        "ch09",
        "dropout",
        "co-adaptation"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-021",
      "front": "What are the trade-offs between invariance and equivariance in representation learning?",
      "back": "<b>Invariance</b>: \\( f(T(x)) = f(x) \\) - output unchanged under transformation T.<br><b>Equivariance</b>: \\( f(T(x)) = T'(f(x)) \\) - output transforms predictably.<br><br><b>When invariance helps</b>:<ul><li>Classification: Category shouldn't change with pose/position</li><li>Retrieval: Same object should match regardless of view</li></ul><b>When invariance hurts</b>:<ul><li><b>Loses information</b>: Can't distinguish 'upside-down cat' from 'right-side-up cat' if rotation-invariant</li><li><b>Downstream tasks may need the information</b>: Pose estimation requires knowing orientation</li><li><b>Collapse risk</b>: Overly invariant representations can collapse (everything maps to same vector)</li></ul><b>Equivariance advantages</b>:<ul><li>Preserves structure while sharing computation</li><li>Intermediate representations carry transformation info</li><li>Better for dense prediction (segmentation, detection)</li></ul><b>Modern view</b>: Build equivariant early layers, pool for invariance only at the end when needed for the task.",
      "tags": [
        "ch09",
        "invariance",
        "equivariance"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-023",
      "front": "What is dropout?",
      "back": "A regularization technique that <b>randomly deletes nodes</b> during training.<br>With probability p, each hidden/input node is 'dropped' (set to zero).<br>At test time: Use all nodes but scale activations by (1-p).<br>Can be viewed as implicit model averaging.",
      "tags": [
        "ch09",
        "dropout",
        "regularization"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-024",
      "front": "Which nodes does dropout apply to?",
      "back": "Dropout is applied to:<br><ul><li><b>Hidden nodes</b>: Yes</li><li><b>Input nodes</b>: Yes (sometimes)</li><li><b>Output nodes</b>: No</li></ul>Output nodes are never dropped as they define the prediction.",
      "tags": [
        "ch09",
        "dropout",
        "nodes"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-025",
      "front": "Why is dropout interpreted as model averaging?",
      "back": "Each dropout mask defines a <b>different sub-network</b>.<br>With N droppable nodes, there are \\( 2^N \\) possible sub-networks.<br>Training samples different networks; inference averages them.<br>Approximate ensemble without training multiple models explicitly.",
      "tags": [
        "ch09",
        "dropout",
        "ensemble"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-026",
      "front": "How does dropout affect training time?",
      "back": "Training takes <b>longer</b> with dropout because:<br><ul><li>Each parameter update uses only a subset of nodes</li><li>Updates are noisier (higher variance)</li><li>More epochs needed for convergence</li></ul>But generalization often improves, justifying the extra training.",
      "tags": [
        "ch09",
        "dropout",
        "training"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-027",
      "front": "What is Monte Carlo dropout?",
      "back": "Using dropout at <b>test time</b> (not just training) to estimate uncertainty.<br>Run multiple forward passes with different dropout masks.<br>Variance in predictions indicates model uncertainty.<br>Provides approximate Bayesian inference cheaply.",
      "tags": [
        "ch09",
        "mc-dropout",
        "uncertainty"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-028",
      "front": "What are shrinkage methods? What is another name for this in neural networks?",
      "back": "<b>Shrinkage methods</b> are techniques that reduce the value of the coefficients through regularization.<br>In the context of neural networks, this is called <b>weight decay</b> because the regularizer encourages the weights to decay towards zero.",
      "tags": [
        "ch09",
        "regularization",
        "weight-decay"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-029",
      "front": "What is the Vapnik-Chervonenkis (VC) dimension?",
      "back": "<b>VC dimension</b> measures the <b>capacity</b> (expressiveness) of a hypothesis class.<br><br><b>Definition</b>: The largest number of points that can be <b>shattered</b> (classified in all possible ways) by the hypothesis class.<br><br><b>Examples</b>:<ul><li>Linear classifiers in 2D: VC dim = 3 (can shatter 3 points, not 4)</li><li>Linear classifiers in d dimensions: VC dim = d + 1</li><li>Neural networks: Roughly proportional to number of parameters</li></ul><b>Why it matters</b>:<ul><li>Bounds generalization error: higher VC dim needs more data</li><li>Helps understand model complexity beyond parameter count</li><li>Fundamental concept in statistical learning theory</li></ul>",
      "tags": [
        "ch09",
        "vc-dimension",
        "learning-theory"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-030",
      "front": "What is the PAC (Probably Approximately Correct) learning framework?",
      "back": "<b>PAC learning</b> formalizes what it means for a hypothesis class to be learnable.<br><br><b>Definition</b>: A class is PAC-learnable if there exists an algorithm that, for any target function in the class and any distribution over inputs, can find a hypothesis with:<ul><li>Error at most \\( \\epsilon \\) (approximately correct)</li><li>Probability at least \\( 1 - \\delta \\) (probably)</li><li>Using polynomial samples in \\( 1/\\epsilon \\), \\( 1/\\delta \\), and problem size</li></ul><b>Key results</b>:<ul><li>Finite hypothesis classes are PAC-learnable</li><li>Sample complexity depends on VC dimension</li><li>Fundamental theorem: A class is PAC-learnable iff it has finite VC dimension</li></ul><b>Intuition</b>: PAC gives guarantees on <i>how much data</i> you need for <i>how good</i> a solution.",
      "tags": [
        "ch09",
        "pac-learning",
        "learning-theory"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-031",
      "front": "What is a regularization penalty term?",
      "back": "An additional term in the loss function that penalizes model complexity:<br>\\( \\tilde{E}(\\vec{w}) = E(\\vec{w}) + \\lambda \\Omega(\\vec{w}) \\)<br>Where:<br><ul><li>\\( E \\) is the data error</li><li>\\( \\Omega \\) is the regularizer</li><li>\\( \\lambda \\) controls the strength</li></ul>Reduces overfitting by constraining parameter values.",
      "tags": [
        "ch09",
        "regularization",
        "penalty"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-032",
      "front": "What is the bias-variance trade-off in the context of regularization?",
      "back": "Regularization:<br><ul><li><b>Increases bias</b>: Model is more constrained, may underfit</li><li><b>Decreases variance</b>: Less sensitive to training data variations</li></ul>Optimal regularization balances these to minimize total error on test data.",
      "tags": [
        "ch09",
        "bias-variance",
        "regularization"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-033",
      "front": "What is weight decay (L2 regularization)?",
      "back": "Regularizer that penalizes large weight magnitudes:<br>\\( \\Omega(\\vec{w}) = \\frac{1}{2}\\|\\vec{w}\\|^2 = \\frac{1}{2}\\sum_i w_i^2 \\)<br>Called 'weight decay' because it encourages weights to <b>decay towards zero</b> unless supported by data.<br>Gradient contribution: \\( \\lambda w_i \\)",
      "tags": [
        "ch09",
        "weight-decay",
        "l2-regularization"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-034",
      "front": "How does L2 regularization affect the effective number of parameters?",
      "back": "L2 regularization <b>suppresses parameters</b> that have little effect on the data fit.<br>The <b>effective number of parameters</b> is the number that remain active (non-negligible) after regularization.<br>This can be much smaller than the total parameter count.",
      "tags": [
        "ch09",
        "effective-parameters",
        "l2-regularization"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-035",
      "front": "What is L1 regularization (lasso)?",
      "back": "Regularizer using absolute values:<br>\\( \\Omega(\\vec{w}) = \\sum_i |w_i| \\)<br>Key property: Produces <b>sparse solutions</b> where many weights are exactly zero.<br>As \\( \\lambda \\) increases, more parameters are driven to zero.<br>Useful for feature selection.",
      "tags": [
        "ch09",
        "l1-regularization",
        "lasso"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-036",
      "front": "Why does L1 regularization produce sparse solutions while L2 does not?",
      "back": "<b>Intuition</b>: Think of minimizing error subject to a budget constraint on weights.<ul><li><b>L2 constraint</b>: A circle (2D) or sphere. The optimal point where error contours touch the circle is usually <i>not</i> on an axis - weights are small but non-zero.</li><li><b>L1 constraint</b>: A diamond shape with <i>corners on the axes</i>. Error contours are much more likely to first touch the diamond at a corner, where some weights are exactly zero.</li></ul><b>Gradient view</b>:<ul><li>L1 gradient is constant (\\( \\pm 1 \\)) regardless of weight magnitude - it pushes weights all the way to zero.</li><li>L2 gradient is proportional to weight (\\( 2w \\)) - it weakens as weights shrink, so they approach but rarely reach zero.</li></ul>",
      "tags": [
        "ch09",
        "sparsity",
        "l1-vs-l2"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-037",
      "front": "What is early stopping?",
      "back": "A regularization technique that <b>stops training before convergence</b> based on validation error.<br>As training progresses:<br><ul><li>Training error decreases</li><li>Validation error initially decreases, then increases (overfitting)</li></ul>Stop at minimum validation error.",
      "tags": [
        "ch09",
        "early-stopping",
        "regularization"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-038",
      "front": "What are learning curves?",
      "back": "Plots of performance measures (training/validation error) vs. training progress.<br>Used to diagnose:<br><ul><li>Overfitting (training error << validation error)</li><li>Underfitting (both errors high)</li><li>When to apply early stopping</li></ul>Can also plot vs. dataset size or model complexity.",
      "tags": [
        "ch09",
        "learning-curves",
        "diagnostics"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-039",
      "front": "What is the double descent phenomenon?",
      "back": "A model's test error initially <b>decreases</b> with the number of parameters, then <b>peaks</b>, then <b>decreases again</b>.<br><br>The peak usually occurs near the <b>interpolation threshold</b>, where the number of parameters equals the number of training data points.<br><br><b>Why does error decrease again?</b> With many more parameters than needed, the model can choose among many perfect fits and picks a smoother one (via implicit regularization from optimization).<br><br>This is surprising because it contradicts classical assumptions about overfitting - traditionally we expected error to only increase once models become too complex.",
      "tags": [
        "ch09",
        "double-descent",
        "generalization"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-040",
      "front": "What is effective model complexity?",
      "back": "The <b>maximum training set size</b> that a model can interpolate (fit exactly).<br>Effective complexity increases with:<br><ul><li>More parameters</li><li>More training epochs</li><li>Smaller regularization</li></ul>Not just parameter count, but capacity to fit data.",
      "tags": [
        "ch09",
        "effective-complexity",
        "model-capacity"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-041",
      "front": "Why can increasing model size sometimes improve generalization?",
      "back": "In the over-parameterized regime:<br><ul><li>Many solutions can interpolate training data</li><li>Optimization and regularization select <b>simpler</b> solutions</li><li>Larger models can find smoother interpolations</li></ul>This explains double descent: after the interpolation threshold, more capacity helps.",
      "tags": [
        "ch09",
        "over-parameterization",
        "generalization"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-042",
      "front": "What is Elastic Net regularization?",
      "back": "<b>Elastic Net</b> combines L1 and L2 regularization:<br><br>\\( \\Omega(\\vec{w}) = \\lambda_1 \\|\\vec{w}\\|_1 + \\lambda_2 \\|\\vec{w}\\|_2^2 \\)<br><br>Or with mixing parameter \\( \\alpha \\in [0,1] \\):<br>\\( \\Omega(\\vec{w}) = \\alpha \\|\\vec{w}\\|_1 + (1-\\alpha) \\|\\vec{w}\\|_2^2 \\)<br><br><b>Why combine them?</b><ul><li><b>L1 alone</b>: Sparse but unstable when features are correlated (picks one arbitrarily)</li><li><b>L2 alone</b>: Stable but not sparse</li><li><b>Elastic Net</b>: Sparse AND groups correlated features together</li></ul><b>When to use</b>: Many features, some correlated, want sparse selection.",
      "tags": [
        "ch09",
        "elastic-net",
        "regularization"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-043",
      "front": "Why does early stopping act like regularization? When does this analogy break?",
      "back": "<b>Rigorous explanation</b>:<br>For linear regression with gradient descent starting from \\( w=0 \\):<br>\\( w_t = (I - (I - \\eta H)^t) H^{-1} X^T y \\)<br><br>As \\( t \\to \\infty \\): \\( w_t \\to w_{OLS} \\) (ordinary least squares)<br><br><b>Key insight</b>: The eigenvalues of \\( (I - \\eta H)^t \\) decay at different rates. Directions with small eigenvalues (high curvature) are learned slowly. Stopping early limits how much the model can move in these directions.<br><br><b>Equivalence to L2</b>: For small \\( \\eta \\) and appropriate stopping time, early stopping approximates L2 regularization with \\( \\lambda \\propto 1/(\\eta t) \\).<br><br><b>When the analogy breaks</b>:<ul><li><b>Nonlinear models</b>: Trajectory depends on initialization, not just distance from origin</li><li><b>Adaptive optimizers</b>: Adam, RMSprop break the simple correspondence</li><li><b>Large learning rates</b>: Trajectory can be chaotic, not monotonic</li><li><b>Multiple passes through data</b>: SGD noise complicates the picture</li></ul><b>In practice</b>: Early stopping still regularizes deep networks, but the mechanism is more complex than L2 equivalence.",
      "tags": [
        "ch09",
        "early-stopping",
        "regularization"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-044",
      "front": "How does the size of the training data affect overfitting?",
      "back": "For a given model complexity, the overfitting problem becomes less severe as the size of the data set increases. With a larger data set, we can afford to fit a more complex (more flexible) model to the data.",
      "tags": [
        "ch09",
        "overfitting",
        "data-size"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-045",
      "front": "What is regularization in machine learning?",
      "back": "<b>Regularization</b> involves adding a penalty term to the error function to discourage the coefficients from having large magnitudes. It controls overfitting as an alternative to limiting the number of parameters.",
      "tags": [
        "ch09",
        "regularization",
        "overfitting"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-046",
      "front": "What is the regularized error function with L2 penalty?",
      "back": "\\( \\tilde{E}(w) = \\frac{1}{2} \\sum_{n=1}^{N} \\{y(x_n, w) - t_n\\}^2 + \\frac{\\lambda}{2} \\|w\\|^2 \\)<br>where \\( \\|w\\|^2 = w^T w = w_0^2 + w_1^2 + \\ldots + w_M^2 \\)<br>The coefficient \\( \\lambda \\) governs the relative importance of the regularization term.",
      "tags": [
        "ch09",
        "regularization",
        "l2-regularization"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-047",
      "front": "What is the bias-variance trade-off?",
      "back": "<b>Bias</b>: How far off the average prediction is from the truth (systematic error).<br><b>Variance</b>: How much predictions change across different training sets (sensitivity to data).<br><b>Trade-off</b>:<br>\\( \\text{Expected loss} = (\\text{bias})^2 + \\text{variance} + \\text{noise} \\)<br><ul><li>Flexible models: low bias, high variance (overfit)</li><li>Simple models: high bias, low variance (underfit)</li></ul>Optimal complexity balances these.",
      "tags": [
        "ch09",
        "bias-variance",
        "model-selection"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-048",
      "front": "What is the squared bias in the bias-variance decomposition?",
      "back": "The extent to which the <b>average prediction</b> over all possible datasets differs from the true value:<br>\\( (\\text{bias})^2 = \\{E_D[f(\\vec{x}; D)] - h(\\vec{x})\\}^2 \\)<br>Where \\( h(\\vec{x}) \\) is the true underlying function.",
      "tags": [
        "ch09",
        "bias-variance",
        "bias"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-049",
      "front": "What is the variance in the bias-variance decomposition?",
      "back": "The extent to which solutions for individual datasets vary around their average:<br>\\( \\text{variance} = E_D[\\{f(\\vec{x}; D) - E_D[f(\\vec{x}; D)]\\}^2] \\)<br>Measures sensitivity to the particular training set used.",
      "tags": [
        "ch09",
        "bias-variance",
        "variance"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-050",
      "front": "Why does the bias-variance decomposition have limited practical value?",
      "back": "Because it is based on <b>averages with respect to ensembles of datasets</b>.<br>In practice, we have only a single dataset, so we cannot directly measure these quantities.<br>Useful conceptually but hard to apply directly.",
      "tags": [
        "ch09",
        "bias-variance",
        "limitations"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-051",
      "front": "Does overfitting arise in Bayesian settings?",
      "back": "<b>No</b> - overfitting does not arise when we marginalize over parameters in a Bayesian setting.<br>The Bayesian approach integrates over the posterior distribution of parameters rather than using a single point estimate, naturally incorporating uncertainty.",
      "tags": [
        "ch09",
        "overfitting",
        "bayesian"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-052",
      "front": "How do L2 and L1 regularization correspond to priors in a MAP view?",
      "back": "<b>L2 regularization</b> corresponds to a <b>Gaussian prior</b> on weights:<br>\\( p(\\vec{w}) \\propto \\exp\\left(-\\frac{\\lambda}{2}\\|\\vec{w}\\|^2\\right) \\) so \\( -\\ln p(\\vec{w}) \\propto \\frac{\\lambda}{2}\\|\\vec{w}\\|^2 \\).<br><br><b>L1 regularization</b> corresponds to a <b>Laplace prior</b>:<br>\\( p(\\vec{w}) \\propto \\exp\\left(-\\lambda \\|\\vec{w}\\|_1\\right) \\) so \\( -\\ln p(\\vec{w}) \\propto \\lambda \\|\\vec{w}\\|_1 \\).<br><br><b>Intuition</b>: The Laplace prior has a sharp peak at 0, which encourages sparsity.",
      "tags": [
        "ch09",
        "l1-vs-l2",
        "bayesian"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-053",
      "front": "What is generalization in machine learning?",
      "back": "<b>Generalization</b> is the ability to make accurate predictions on previously unseen inputs. It is a key goal in machine learning.",
      "tags": [
        "ch09",
        "generalization",
        "definition"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-054",
      "front": "What is overfitting?",
      "back": "<b>Overfitting</b> occurs when a model fits the training data very well (possibly exactly) but gives poor predictions on new, unseen data. The model has essentially memorized the noise in the training data rather than learning the underlying pattern.",
      "tags": [
        "ch09",
        "overfitting",
        "generalization"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-055",
      "front": "What is a test set used for?",
      "back": "A <b>test set</b> is a separate set of data used to evaluate how well a model generalizes to new data. It measures the model's predictive performance on data not used during training.",
      "tags": [
        "ch09",
        "test-set",
        "evaluation"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-056",
      "front": "What is a hyperparameter?",
      "back": "A <b>hyperparameter</b> is a parameter whose value is fixed during training (not learned from data). Examples include:<br><ul><li>The regularization coefficient \\( \\lambda \\)</li><li>The order \\( M \\) of a polynomial</li><li>Learning rate</li></ul>",
      "tags": [
        "ch09",
        "hyperparameters",
        "definition"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-057",
      "front": "What is the difference between a training set and a validation set?",
      "back": "<ul><li><b>Training set</b>: Used to determine the model parameters (weights)</li><li><b>Validation set</b> (also called hold-out or development set): Used to select hyperparameters and model architecture</li></ul>A third <b>test set</b> may be kept aside for final evaluation.",
      "tags": [
        "ch09",
        "data-splits",
        "model-selection"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-058",
      "front": "What is S-fold cross-validation?",
      "back": "<b>S-fold cross-validation</b> partitions data into S groups. For each run:<br><ul><li>Train on S-1 groups</li><li>Evaluate on the remaining group</li></ul>Repeat for all S choices of held-out group and average the scores. This uses (S-1)/S of data for training while evaluating on all data.",
      "tags": [
        "ch09",
        "cross-validation",
        "model-selection"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-059",
      "front": "What is data leakage in machine learning?",
      "back": "<b>Data leakage</b> is when information that would not be available at prediction time (often from the validation/test set, or from the future) influences training.<br><br><b>Why it matters</b>: It makes evaluation metrics look much better than real-world performance.<br><br><b>Common leak patterns</b>:<ul><li><b>Preprocessing leakage</b>: Fitting normalization/feature selection on all data (train+val+test) instead of training data only</li><li><b>Target leakage</b>: A feature directly/indirectly encodes the label (e.g., using \"refund issued\" when predicting fraud)</li><li><b>Temporal leakage</b>: Using future information when predicting the past</li></ul><b>Fix</b>: Fit every transform strictly on the training split, then apply to validation/test.",
      "tags": [
        "ch09",
        "data-leakage",
        "evaluation"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-060",
      "front": "When is a random train/validation/test split a bad idea?",
      "back": "Random splitting is risky when it breaks the assumptions of independence between splits.<br><br><b>Common cases</b>:<ul><li><b>Time series</b>: Random split causes temporal leakage; prefer chronological splits</li><li><b>Grouped data</b> (e.g., multiple samples per user/patient): Random split leaks identity; use group-based splits</li><li><b>Near-duplicates</b> (e.g., similar frames/images): Duplicates across splits inflate metrics; deduplicate or split by source</li></ul>",
      "tags": [
        "ch09",
        "data-splits",
        "evaluation"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-061",
      "front": "What is the correct role of the test set in model development?",
      "back": "The <b>test set</b> should be used <b>only once</b>, at the end, to estimate generalization after all modeling decisions are fixed.<br><br><b>Rule</b>: If you look at test performance and then change anything (features, preprocessing, architecture, hyperparameters), the test set has become part of model selection and your evaluation is biased.<br><br>Use <b>validation</b> (or cross-validation) for all choices; reserve the test set for the final report.",
      "tags": [
        "ch09",
        "test-set",
        "model-selection"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-062",
      "front": "What is a stratified split and when should you use it?",
      "back": "A <b>stratified split</b> preserves the class proportions across train/validation/test splits.<br><br><b>When to use</b>: In classification, especially with <b>imbalanced classes</b>, stratification reduces the chance that one split has too few positives (which makes metrics noisy and misleading).",
      "tags": [
        "ch09",
        "data-splits",
        "imbalanced-classes"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-063",
      "front": "Why can't we determine hyperparameters by minimizing the training error jointly with model parameters?",
      "back": "Minimizing training error jointly would lead to extreme values:<br><ul><li>Regularization \\( \\lambda \\to 0 \\) (no regularization)</li><li>Polynomial order \\( M \\to \\) large values</li></ul>Both result in overfitting with small/zero training error but poor generalization.",
      "tags": [
        "ch09",
        "hyperparameters",
        "overfitting"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-064",
      "front": "What is the information bottleneck principle in representation learning?",
      "back": "<b>Information bottleneck</b>: A framework for learning representations that compress input while preserving task-relevant information.<br><br><b>Objective</b>: Find representation Z that:<ul><li>Minimizes \\( I(X; Z) \\) (compression - forget irrelevant details)</li><li>Maximizes \\( I(Z; Y) \\) (relevance - keep what predicts target)</li></ul><b>Trade-off</b>: \\( \\max_Z [I(Z; Y) - \\beta I(X; Z)] \\)<br><br><b>Intuition</b>: Like summarizing a book - keep the plot (relevant), discard prose style (irrelevant).<br><br><b>Practical pitfalls</b>:<ul><li>Mutual information is hard to estimate in high dimensions</li><li>Deterministic networks have infinite \\( I(X; Z) \\)</li><li>Need stochastic layers or other approximations</li><li>\\( \\beta \\) is hard to tune</li></ul><b>Applications</b>: Deep learning theory, representation learning, variational inference.",
      "tags": [
        "ch09",
        "information-bottleneck",
        "representation-learning"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-09-065",
      "front": "What is the implicit bias (inductive bias) of SGD in deep learning?",
      "back": "<b>Implicit bias</b>: The tendency of an optimization algorithm to prefer certain solutions over others, even without explicit regularization.<br><br><b>What SGD prefers</b>:<ul><li><b>Flat minima</b>: Solutions with low curvature (wide valleys) generalize better. SGD noise helps find these.</li><li><b>Low-rank solutions</b>: In matrix factorization, gradient descent finds minimum nuclear norm solutions.</li><li><b>Max-margin classifiers</b>: For separable data, gradient descent on logistic loss converges to max-margin (SVM-like) solution.</li><li><b>Simpler functions</b>: Neural networks trained with SGD tend toward lower-complexity functions.</li></ul><b>Why it happens</b>:<ul><li>Gradient flow dynamics have inherent geometry</li><li>Mini-batch noise acts as implicit regularization</li><li>Learning rate affects sharpness of found minima</li></ul><b>Practical implications</b>: SGD often finds solutions that generalize well even without explicit regularization.",
      "tags": [
        "ch09",
        "implicit-bias",
        "sgd"
      ]
    }
  ]
}
