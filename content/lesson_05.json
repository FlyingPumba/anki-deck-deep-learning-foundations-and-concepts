{
  "id": "05",
  "title": "Lesson 05: Single-layer Networks: Classification",
  "lesson_title": "Single-layer Networks: Classification",
  "objectives": [
    "Understand logistic regression and softmax functions",
    "Master discriminant functions and decision boundaries",
    "Learn generative vs discriminative probabilistic models",
    "Understand the cross-entropy error function",
    "Learn about limitations of linear models and the curse of dimensionality",
    "Understand neural network fundamentals: hidden units, activation functions, deep architectures"
  ],
  "cards": [
    {
      "uid": "05-001",
      "front": "What is the logistic sigmoid function?",
      "back": "\\( \\sigma(a) = \\frac{1}{1 + e^{-a}} \\)<br>Properties:<br><ul><li>Maps the whole real axis to the interval (0, 1)</li><li>'Sigmoid' means S-shaped</li><li>Used to model probabilities in binary classification</li></ul>",
      "tags": [
        "ch05",
        "sigmoid",
        "activation-functions"
      ]
    },
    {
      "uid": "05-002",
      "front": "What is the logit function?",
      "back": "The <b>inverse of the logistic sigmoid</b>:<br>\\( \\text{logit}(p) = \\ln\\frac{p}{1-p} = \\sigma^{-1}(p) \\)<br>Also called the log-odds function. Maps (0,1) to the real line.",
      "tags": [
        "ch05",
        "logit",
        "activation-functions"
      ]
    },
    {
      "uid": "05-003",
      "front": "What is the softmax function (normalized exponential)?",
      "back": "For K classes:<br>\\( p(C_k|\\vec{x}) = \\frac{\\exp(a_k)}{\\sum_j \\exp(a_j)} \\)<br>where \\( a_k = \\vec{w}_k^T \\vec{x} \\)<br>Properties:<br><ul><li>Outputs sum to 1</li><li>All outputs in (0, 1)</li><li>Generalizes logistic sigmoid to K > 2 classes</li></ul>",
      "tags": [
        "ch05",
        "softmax",
        "activation-functions"
      ]
    },
    {
      "uid": "05-004",
      "front": "For what types of inputs do posterior probabilities take the form of logistic sigmoid or softmax?",
      "back": "For both:<br><ul><li><b>Gaussian distributed inputs</b> (with shared covariance)</li><li><b>Discrete inputs</b></li></ul>The posterior class probabilities are logistic sigmoid (2 classes) or softmax (K classes) functions of a linear combination of inputs.",
      "tags": [
        "ch05",
        "posterior",
        "generative-models"
      ]
    },
    {
      "uid": "05-005",
      "front": "What is the advantage of discriminative models over generative models?",
      "back": "In the discriminative approach:<br><ul><li>Model \\( p(C_k|\\vec{x}) \\) directly</li><li><b>Fewer learnable parameters</b> to determine</li><li>Don't need to model \\( p(\\vec{x}) \\) which may have complex structure irrelevant to classification</li></ul>Generative models must model the full joint distribution.",
      "tags": [
        "ch05",
        "discriminative",
        "generative"
      ]
    },
    {
      "uid": "05-006",
      "front": "What is an activation function in neural networks?",
      "back": "A nonlinear function applied to the weighted sum of inputs:<br>\\( z = f(\\vec{w}^T \\vec{x} + b) \\)<br>For classification with discrete labels, we need a nonlinear activation to:<br><ul><li>Squash outputs to valid probability ranges</li><li>Create nonlinear decision boundaries</li></ul>",
      "tags": [
        "ch05",
        "activation-functions",
        "neural-networks"
      ]
    },
    {
      "uid": "05-007",
      "front": "What are generalized linear models?",
      "back": "Models where the output is a nonlinear function of a linear combination of inputs:<br>\\( y = f(\\vec{w}^T \\phi(\\vec{x})) \\)<br>The function \\( f \\) is the <b>activation function</b> (or link function inverse).<br>Examples: Logistic regression, Poisson regression.",
      "tags": [
        "ch05",
        "generalized-linear-models",
        "glm"
      ]
    },
    {
      "uid": "05-008",
      "front": "Are classes that are linearly separable in feature space necessarily linearly separable in input space?",
      "back": "<b>No</b>. Classes that are linearly separable in feature space \\( \\phi(\\vec{x}) \\) need not be linearly separable in the original input space \\( \\vec{x} \\).<br>This is the power of using nonlinear basis functions/features.",
      "tags": [
        "ch05",
        "linear-separability",
        "feature-space"
      ]
    },
    {
      "uid": "05-009",
      "front": "What is logistic regression?",
      "back": "A discriminative model for binary classification:<br>\\( p(C_1|\\vec{x}) = \\sigma(\\vec{w}^T \\phi(\\vec{x})) \\)<br>Trained by maximizing likelihood (or minimizing cross-entropy error).<br>Stochastic gradient descent is the principal approach for training.",
      "tags": [
        "ch05",
        "logistic-regression",
        "classification"
      ]
    },
    {
      "uid": "05-010",
      "front": "How is the error function for logistic regression derived?",
      "back": "Take the <b>negative logarithm of the likelihood</b>:<br>\\( E(\\vec{w}) = -\\sum_{n=1}^{N} \\{t_n \\ln y_n + (1-t_n) \\ln(1-y_n)\\} \\)<br>where \\( y_n = \\sigma(\\vec{w}^T \\phi(\\vec{x}_n)) \\)<br>This is the <b>cross-entropy error function</b>.",
      "tags": [
        "ch05",
        "cross-entropy",
        "logistic-regression"
      ]
    },
    {
      "uid": "05-011",
      "front": "What is the cross-entropy error function for multi-class classification?",
      "back": "\\( E(\\vec{w}) = -\\sum_{n=1}^{N} \\sum_{k=1}^{K} t_{nk} \\ln y_{nk} \\)<br>Where:<br><ul><li>\\( t_{nk} \\) is the one-hot encoded target</li><li>\\( y_{nk} = p(C_k|\\vec{x}_n) \\) from softmax</li></ul>Also called categorical cross-entropy or log loss.",
      "tags": [
        "ch05",
        "cross-entropy",
        "multi-class"
      ]
    },
    {
      "uid": "05-012",
      "front": "What are shortcomings of linear models with fixed basis functions?",
      "back": "The assumption that basis functions are fixed independent of training data leads to:<br><ol><li><b>Poor scaling</b> with dimensionality (curse of dimensionality)</li><li><b>Inability to adapt</b> features to the problem</li><li><b>Exponential growth</b> in number of parameters for expressive models</li></ol>Solution: Learn the basis functions from data (neural networks).",
      "tags": [
        "ch05",
        "linear-models",
        "limitations"
      ]
    },
    {
      "uid": "05-013",
      "front": "How does the number of coefficients grow for a polynomial of order M in D dimensions?",
      "back": "The growth is \\( O(D^M) \\)<br>For high-dimensional spaces, polynomials become unwieldy very quickly.<br>Example: Degree 3 polynomial in 100 dimensions requires millions of terms.",
      "tags": [
        "ch05",
        "polynomial",
        "curse-of-dimensionality"
      ]
    },
    {
      "uid": "05-014",
      "front": "What is the curse of dimensionality?",
      "back": "The exponential increase in volume/complexity as dimensionality grows.<br>Consequences:<br><ul><li>Data becomes sparse</li><li>Distance metrics become less meaningful</li><li>Number of parameters explodes</li><li>More training data needed</li></ul>If we divide space into M bins per dimension, total bins = \\( M^D \\).",
      "tags": [
        "ch05",
        "curse-of-dimensionality",
        "high-dimensional"
      ]
    },
    {
      "uid": "05-015",
      "front": "Where is most of the volume of a high-dimensional hypersphere concentrated?",
      "back": "In spaces of high dimensionality, <b>most of the volume is concentrated in a thin shell near the surface</b>.<br>The volume scales as \\( r^D \\), so the ratio of inner volume to total volume vanishes as D increases.",
      "tags": [
        "ch05",
        "hypersphere",
        "high-dimensional"
      ]
    },
    {
      "uid": "05-016",
      "front": "Why can classification be easier in higher-dimensional spaces?",
      "back": "Because there's more room to separate classes with hyperplanes.<br>Classes that overlap in low dimensions may become linearly separable when projected to higher-dimensional feature spaces.<br>This is the intuition behind kernel methods and neural network embeddings.",
      "tags": [
        "ch05",
        "high-dimensional",
        "classification"
      ]
    },
    {
      "uid": "05-017",
      "front": "Why does real data often not suffer from the full curse of dimensionality?",
      "back": "Real data is typically <b>confined to a region having lower effective dimensionality</b> than the ambient space.<br>Data lies on or near a <b>low-dimensional manifold</b>.<br>Example: Adjacent pixels in images are highly correlated, not independent.",
      "tags": [
        "ch05",
        "manifold",
        "curse-of-dimensionality"
      ]
    },
    {
      "uid": "05-018",
      "front": "What are basis functions also called in machine learning?",
      "back": "<b>Features</b>.<br>The terms are used interchangeably. Feature engineering/extraction was the traditional approach before deep learning.<br>In neural networks, features are learned from data rather than hand-crafted.",
      "tags": [
        "ch05",
        "features",
        "basis-functions"
      ]
    },
    {
      "uid": "05-019",
      "front": "What is the key idea behind neural networks?",
      "back": "Choose basis functions \\( \\phi_j(\\vec{x}) \\) that <b>themselves have learnable parameters</b>.<br>This allows the basis functions to be adapted to the data during training, rather than being fixed in advance.",
      "tags": [
        "ch05",
        "neural-networks",
        "learnable-features"
      ]
    },
    {
      "uid": "05-020",
      "front": "What are hidden units in a neural network?",
      "back": "The learnable basis functions \\( \\phi_j(\\vec{x}) \\) in a neural network.<br>Each hidden unit computes:<br>\\( z_j = h(\\vec{w}_j^T \\vec{x} + b_j) \\)<br>where \\( h \\) is a nonlinear activation function.<br>Called 'hidden' because their values are not directly observed.",
      "tags": [
        "ch05",
        "hidden-units",
        "neural-networks"
      ]
    },
    {
      "uid": "05-021",
      "front": "Why can neural networks be trained with gradient-based methods?",
      "back": "Because provided the activation function \\( h(\\cdot) \\) is differentiable, the overall network function is differentiable with respect to all parameters.<br>This enables backpropagation and gradient descent optimization.",
      "tags": [
        "ch05",
        "gradient-descent",
        "differentiable"
      ]
    },
    {
      "uid": "05-022",
      "front": "What are universal approximators?",
      "back": "Networks that can approximate any continuous function to arbitrary accuracy.<br>Two-layer networks with a wide range of activation functions are <b>universal approximators</b>.<br>However, this says nothing about:<br><ul><li>How many units are needed</li><li>Whether such a network can be found by learning</li></ul>",
      "tags": [
        "ch05",
        "universal-approximation",
        "neural-networks"
      ]
    },
    {
      "uid": "05-023",
      "front": "What is the no free lunch theorem?",
      "back": "Averaged over all possible problems, no learning algorithm is better than any other (including random guessing).<br><b>Implications</b>:<br><ul><li>Every successful algorithm must have <b>inductive biases</b> suited to the problems it solves</li><li>If an algorithm is better than average on some problems, it must be <b>worse than average on others</b></li></ul>No universal algorithm works best on everything.",
      "tags": [
        "ch05",
        "no-free-lunch",
        "inductive-bias"
      ]
    },
    {
      "uid": "05-024",
      "front": "Why are nonlinear activation functions necessary in deep networks?",
      "back": "Because <b>composition of linear transformations is itself linear</b>.<br>Without nonlinearities, a deep network would collapse to a single linear transformation:<br>\\( W_L \\cdots W_2 W_1 \\vec{x} = W_{\\text{effective}} \\vec{x} \\)<br>Nonlinearities enable learning complex functions.",
      "tags": [
        "ch05",
        "activation-functions",
        "nonlinearity"
      ]
    },
    {
      "uid": "05-025",
      "front": "What is a bottleneck network of linear units equivalent to?",
      "back": "<b>Principal Component Analysis (PCA)</b>.<br>If the number of hidden units is smaller than both input and output dimensions, a linear network learns a low-rank approximation.<br>This corresponds to projecting onto the principal components.",
      "tags": [
        "ch05",
        "bottleneck",
        "pca"
      ]
    },
    {
      "uid": "05-026",
      "front": "What is the tanh activation function and how does it relate to sigmoid?",
      "back": "\\( \\tanh(a) = \\frac{e^a - e^{-a}}{e^a + e^{-a}} \\)<br>Relation to sigmoid:<br>\\( \\tanh(a) = 2\\sigma(2a) - 1 \\)<br>Outputs range: (-1, 1) vs (0, 1) for sigmoid.<br>A linear combination of tanh functions is equivalent to a linear combination of sigmoids.",
      "tags": [
        "ch05",
        "tanh",
        "activation-functions"
      ]
    },
    {
      "uid": "05-027",
      "front": "What is the ReLU activation function?",
      "back": "<b>Rectified Linear Unit</b>:<br>\\( \\text{ReLU}(a) = \\max(0, a) \\)<br>Advantages:<br><ul><li>One of the best-performing activation functions</li><li>Much less sensitive to random weight initialization</li><li>Computationally cheap to evaluate</li><li>Avoids vanishing gradients (for positive inputs)</li></ul>",
      "tags": [
        "ch05",
        "relu",
        "activation-functions"
      ]
    },
    {
      "uid": "05-028",
      "front": "What is the softplus activation function?",
      "back": "A smooth approximation to ReLU:<br>\\( \\text{softplus}(a) = \\ln(1 + e^a) \\)<br>Properties:<br><ul><li>Differentiable everywhere</li><li>Approaches ReLU as \\( a \\to \\infty \\)</li><li>Also called soft ReLU</li></ul>",
      "tags": [
        "ch05",
        "softplus",
        "activation-functions"
      ]
    },
    {
      "uid": "05-029",
      "front": "What is the leaky ReLU activation function?",
      "back": "\\( \\text{LeakyReLU}(a) = \\begin{cases} a & \\text{if } a > 0 \\\\ \\alpha a & \\text{if } a \\leq 0 \\end{cases} \\)<br>where \\( \\alpha \\) is a small positive constant (e.g., 0.01).<br>Advantage: Non-zero gradient for negative inputs (avoids 'dying ReLU' problem).",
      "tags": [
        "ch05",
        "leaky-relu",
        "activation-functions"
      ]
    },
    {
      "uid": "05-030",
      "front": "What is the weight-space symmetry in neural networks?",
      "back": "A neural network with M hidden units has <b>\\( M! \\cdot 2^M \\)</b> equivalent weight configurations.<br>Sources:<br><ul><li>\\( M! \\) from permuting hidden units</li><li>\\( 2^M \\) from sign flips (for symmetric activations like tanh)</li></ul>All produce identical input-output mappings.",
      "tags": [
        "ch05",
        "weight-symmetry",
        "neural-networks"
      ]
    },
    {
      "uid": "05-031",
      "front": "What determines the expressive power of a neural network?",
      "back": "The <b>number of layers of learnable weights</b>, not just the total number of layers.<br>Deep networks can represent certain functions with <b>exponentially fewer parameters</b> than shallow networks.",
      "tags": [
        "ch05",
        "depth",
        "expressive-power"
      ]
    },
    {
      "uid": "05-032",
      "front": "How does depth help neural networks divide input space?",
      "back": "The network function divides input space into a number of regions that is <b>exponential in the depth</b>.<br>Deeper networks can create more complex decision boundaries with the same number of parameters.",
      "tags": [
        "ch05",
        "depth",
        "decision-boundaries"
      ]
    },
    {
      "uid": "05-033",
      "front": "What is the compositional inductive bias of neural networks?",
      "back": "The network architecture encodes that the target function is built from <b>compositions of simpler functions</b>.<br>Each layer transforms representations, building hierarchically complex features from simpler ones.<br>This matches how many real-world problems are structured.",
      "tags": [
        "ch05",
        "compositional",
        "inductive-bias"
      ]
    },
    {
      "uid": "05-034",
      "front": "What is a distributed representation?",
      "back": "A representation where each unit in a hidden layer represents a 'feature' at that level of abstraction.<br>Combinations of active units encode meaning, rather than individual units having fixed meanings.<br>Contrast with local/one-hot representations.",
      "tags": [
        "ch05",
        "distributed-representation",
        "neural-networks"
      ]
    },
    {
      "uid": "05-035",
      "front": "What is an embedding space?",
      "back": "The space of learned representations in a neural network's hidden layers.<br>Successive layers transform data into spaces where classification/regression becomes easier.<br>The network learns a <b>nonlinear transformation</b> that makes the problem simpler.",
      "tags": [
        "ch05",
        "embedding",
        "representation"
      ]
    },
    {
      "uid": "05-037",
      "front": "What is an autoencoder?",
      "back": "A neural network trained to reconstruct its input:<br>Input \\( \\to \\) Encoder \\( \\to \\) Bottleneck \\( \\to \\) Decoder \\( \\to \\) Output \\( \\approx \\) Input<br>The bottleneck forces learning a compressed representation.<br>Useful for unsupervised feature learning.",
      "tags": [
        "ch05",
        "autoencoder",
        "unsupervised"
      ]
    },
    {
      "uid": "05-039",
      "front": "What is pre-training in neural networks?",
      "back": "Learning parameters on one task (with abundant data) that are then applied to another task.<br>Typically followed by <b>fine-tuning</b>: adapting the whole network to the target task with a small learning rate for limited iterations.<br>Prevents over-fitting to small target datasets.",
      "tags": [
        "ch05",
        "pretraining",
        "transfer-learning"
      ]
    },
    {
      "uid": "05-040",
      "front": "What is fine-tuning in transfer learning?",
      "back": "Adapting a pre-trained network to a new task by:<br><ol><li>Starting from pre-trained weights</li><li>Training on new task data with a <b>very small learning rate</b></li><li>Running for <b>limited iterations</b></li></ol>This ensures the network doesn't over-fit to the (often smaller) target dataset.",
      "tags": [
        "ch05",
        "fine-tuning",
        "transfer-learning"
      ]
    },
    {
      "uid": "05-041",
      "front": "What is meta-learning (learning to learn)?",
      "back": "Learning algorithms that improve their learning ability across multiple tasks.<br>Goal: Learn to learn new tasks quickly from few examples.<br>Extends transfer learning to learning across many tasks, adapting the learning process itself.",
      "tags": [
        "ch05",
        "meta-learning",
        "neural-networks"
      ]
    },
    {
      "uid": "05-042",
      "front": "What notation is used for the layers in a deep neural network?",
      "back": "\\( \\vec{z}^{(l)} \\) = activations at layer \\( l \\)<br>Special cases:<br><ul><li>\\( \\vec{z}^{(0)} = \\vec{x} \\) (input vector)</li><li>\\( \\vec{z}^{(L)} = \\vec{y} \\) (output vector)</li></ul>Number of layers counted by learnable weight matrices.",
      "tags": [
        "ch05",
        "notation",
        "deep-learning"
      ]
    },
    {
      "uid": "05-043",
      "front": "What is a support vector machine (SVM)?",
      "back": "A classification model that:<br><ul><li>Finds the maximum-margin hyperplane separating classes</li><li>Uses the <b>kernel trick</b> to work in high-dimensional feature spaces</li><li>Only depends on a subset of training points (support vectors)</li></ul>Was popular before deep learning for working with learned features.",
      "tags": [
        "ch05",
        "svm",
        "classification"
      ]
    },
    {
      "uid": "05-044",
      "front": "Why can neural networks learn deep hierarchical representations?",
      "back": "Because:<br><ol><li>Each layer learns features from the previous layer's output</li><li>Early layers learn simple features (edges, textures)</li><li>Later layers combine these into complex features (objects, concepts)</li><li>The composition creates a hierarchy of abstractions</li></ol>This matches the compositional structure of many real problems.",
      "tags": [
        "ch05",
        "hierarchical",
        "representation"
      ]
    }
  ]
}
