{
  "id": "05",
  "title": "Lesson 05: Single-layer Networks: Classification",
  "lesson_title": "Single-layer Networks: Classification",
  "objectives": [
    "Understand logistic regression and softmax functions",
    "Master discriminant functions and decision boundaries",
    "Learn generative vs discriminative probabilistic models",
    "Understand the cross-entropy error function",
    "Learn about limitations of linear models and the curse of dimensionality",
    "Understand neural network fundamentals: hidden units, activation functions, deep architectures"
  ],
  "cards": [
    {
      "uid": "deep-learning-foundations-and-concepts-05-001",
      "front": "What is the logistic sigmoid function?",
      "back": "\\( \\sigma(a) = \\frac{1}{1 + e^{-a}} \\)<br>Properties:<br><ul><li>Maps the whole real axis to the interval (0, 1)</li><li>'Sigmoid' means S-shaped</li><li>Used to model probabilities in binary classification</li></ul>",
      "tags": [
        "ch05",
        "sigmoid",
        "activation-functions"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-002",
      "front": "What is the logit function?",
      "back": "The <b>inverse of the logistic sigmoid</b>:<br>\\( \\text{logit}(p) = \\ln\\frac{p}{1-p} = \\sigma^{-1}(p) \\)<br>Also called the log-odds function. Maps (0,1) to the real line.",
      "tags": [
        "ch05",
        "logit",
        "activation-functions"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-003",
      "front": "What is the softmax function (normalized exponential)?",
      "back": "For K classes:<br>\\( p(C_k|\\vec{x}) = \\frac{\\exp(a_k)}{\\sum_j \\exp(a_j)} \\)<br>where \\( a_k = \\vec{w}_k^T \\vec{x} \\)<br>Properties:<br><ul><li>Outputs sum to 1</li><li>All outputs in (0, 1)</li><li>Generalizes logistic sigmoid to K > 2 classes</li></ul>",
      "tags": [
        "ch05",
        "softmax",
        "activation-functions"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-004",
      "front": "For what types of inputs do posterior probabilities take the form of logistic sigmoid or softmax?",
      "back": "For both:<br><ul><li><b>Gaussian distributed inputs</b> (with shared covariance)</li><li><b>Discrete inputs</b></li></ul>The posterior class probabilities are logistic sigmoid (2 classes) or softmax (K classes) functions of a linear combination of inputs.",
      "tags": [
        "ch05",
        "posterior",
        "generative-models"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-005",
      "front": "What is the advantage of discriminative models over generative models?",
      "back": "<b>Intuition</b>: To tell cats from dogs, you don't need to know everything about what makes a cat - just what makes them <i>different</i>.<br><b>Advantages</b>:<ul><li><b>Fewer parameters</b>: Only model the decision boundary, not the full data distribution</li><li><b>Focused</b>: Ignores complex structure in \\( p(\\vec{x}) \\) that's irrelevant for classification</li><li><b>Often more accurate</b>: All capacity goes toward the classification task</li></ul>",
      "tags": [
        "ch05",
        "discriminative",
        "generative"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-006",
      "front": "What is an activation function in neural networks?",
      "back": "A nonlinear function applied to the weighted sum of inputs:<br>\\( z = f(\\vec{w}^T \\vec{x} + b) \\)<br>For classification with discrete labels, we need a nonlinear activation to:<br><ul><li>Squash outputs to valid probability ranges</li><li>Create nonlinear decision boundaries</li></ul>",
      "tags": [
        "ch05",
        "activation-functions",
        "neural-networks"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-007",
      "front": "What are generalized linear models?",
      "back": "Models where the output is a nonlinear function of a linear combination of inputs:<br>\\( y = f(\\vec{w}^T \\phi(\\vec{x})) \\)<br>The function \\( f \\) is the <b>activation function</b> (or link function inverse).<br>Examples: Logistic regression, Poisson regression.",
      "tags": [
        "ch05",
        "generalized-linear-models",
        "glm"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-008",
      "front": "Are classes that are linearly separable in feature space necessarily linearly separable in input space?",
      "back": "<b>No</b>. Classes that are linearly separable in feature space \\( \\phi(\\vec{x}) \\) need not be linearly separable in the original input space \\( \\vec{x} \\).<br>This is the power of using nonlinear basis functions/features.",
      "tags": [
        "ch05",
        "linear-separability",
        "feature-space"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-009",
      "front": "What is logistic regression?",
      "back": "A discriminative model for binary classification:<br>\\( p(C_1|\\vec{x}) = \\sigma(\\vec{w}^T \\phi(\\vec{x})) \\)<br>Trained by maximizing likelihood (or minimizing cross-entropy error).<br>Stochastic gradient descent is the principal approach for training.",
      "tags": [
        "ch05",
        "logistic-regression",
        "classification"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-010",
      "front": "How is the error function for logistic regression derived?",
      "back": "Take the <b>negative logarithm of the likelihood</b>:<br>\\( E(\\vec{w}) = -\\sum_{n=1}^{N} \\{t_n \\ln y_n + (1-t_n) \\ln(1-y_n)\\} \\)<br>where:<ul><li>\\( t_n \\in \\{0, 1\\} \\) is the target class label for example n</li><li>\\( y_n = \\sigma(\\vec{w}^T \\phi(\\vec{x}_n)) \\) is the predicted probability</li></ul><b>Why log?</b> Turns products into sums, making optimization easier and numerically stable.<br><b>Why negative?</b> We want to <i>minimize</i> error, but <i>maximize</i> likelihood. Negating converts max to min.<br>This is the <b>cross-entropy error function</b>.",
      "tags": [
        "ch05",
        "cross-entropy",
        "logistic-regression"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-011",
      "front": "What is the cross-entropy error function for multi-class classification?",
      "back": "\\( E(\\vec{w}) = -\\sum_{n=1}^{N} \\sum_{k=1}^{K} t_{nk} \\ln y_{nk} \\)<br>Where:<br><ul><li>\\( t_{nk} \\) is the one-hot encoded target</li><li>\\( y_{nk} = p(C_k|\\vec{x}_n) \\) from softmax</li></ul>Also called categorical cross-entropy or log loss.",
      "tags": [
        "ch05",
        "cross-entropy",
        "multi-class"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-012",
      "front": "What are shortcomings of linear models with fixed basis functions?",
      "back": "<b>Basis functions</b> \\( \\phi_j(\\vec{x}) \\) transform the input before the linear combination (e.g., polynomials, Gaussians). In fixed basis models, these are chosen beforehand.<br><b>Shortcomings</b>:<ol><li><b>Poor scaling</b> with dimensionality (curse of dimensionality)</li><li><b>Inability to adapt</b> features to the problem</li><li><b>Exponential growth</b> in parameters for expressive models</li></ol><b>Solution</b>: Learn the basis functions from data (neural networks).",
      "tags": [
        "ch05",
        "linear-models",
        "limitations"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-013",
      "front": "How does the number of coefficients grow for a polynomial of order M in D dimensions?",
      "back": "The growth is \\( O(D^M) \\)<br>For high-dimensional spaces, polynomials become unwieldy very quickly.<br>Example: Degree 3 polynomial in 100 dimensions requires millions of terms.",
      "tags": [
        "ch05",
        "polynomial",
        "curse-of-dimensionality"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-014",
      "front": "What is the curse of dimensionality?",
      "back": "The exponential increase in volume/complexity as dimensionality grows.<br>Consequences:<br><ul><li>Data becomes sparse</li><li>Distance metrics become less meaningful</li><li>Number of parameters explodes</li><li>More training data needed</li></ul>If we divide space into M bins per dimension, total bins = \\( M^D \\).",
      "tags": [
        "ch05",
        "curse-of-dimensionality",
        "high-dimensional"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-015",
      "front": "Where is most of the volume of a high-dimensional hypersphere concentrated?",
      "back": "In spaces of high dimensionality, <b>most of the volume is concentrated in a thin shell near the surface</b>.<br>The volume scales as \\( r^D \\), so the ratio of inner volume to total volume vanishes as D increases.",
      "tags": [
        "ch05",
        "hypersphere",
        "high-dimensional"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-016",
      "front": "Why can classification be easier in higher-dimensional spaces?",
      "back": "Because there's more room to separate classes with hyperplanes.<br>Classes that overlap in low dimensions may become linearly separable when projected to higher-dimensional feature spaces.<br>This is the intuition behind kernel methods and neural network embeddings.",
      "tags": [
        "ch05",
        "high-dimensional",
        "classification"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-017",
      "front": "Why does real data often not suffer from the full curse of dimensionality?",
      "back": "Real data is typically <b>confined to a region having lower effective dimensionality</b> than the ambient space.<br>Data lies on or near a <b>low-dimensional manifold</b>.<br>Example: Adjacent pixels in images are highly correlated, not independent.",
      "tags": [
        "ch05",
        "manifold",
        "curse-of-dimensionality"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-018",
      "front": "What are basis functions also called in machine learning?",
      "back": "<b>Features</b>.<br>The terms are used interchangeably. Feature engineering/extraction was the traditional approach before deep learning.<br>In neural networks, features are learned from data rather than hand-crafted.",
      "tags": [
        "ch05",
        "features",
        "basis-functions"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-045",
      "front": "Why do we subtract the maximum logit before applying softmax?",
      "back": "For <b>numerical stability</b>.<br><br>Softmax is invariant to adding the same constant to every logit:<br>\\( \\text{softmax}(\\vec{a}) = \\text{softmax}(\\vec{a} - c\\mathbf{1}) \\).<br><br>So we compute:<br>\\( y_k = \\frac{\\exp(a_k - \\max_j a_j)}{\\sum_i \\exp(a_i - \\max_j a_j)} \\)<br>which prevents overflow when some logits are large.",
      "tags": [
        "ch05",
        "softmax",
        "numerical-stability"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-046",
      "front": "What is label smoothing and why can it help in classification?",
      "back": "<b>Label smoothing</b> replaces a one-hot target distribution with a slightly \"soft\" target distribution.<br><br>For K classes and smoothing \\( \\epsilon \\):<br>\\( \\tilde{t}_k = (1-\\epsilon) t_k + \\epsilon/K \\).<br><br><b>Why it helps</b>: It acts as a regularizer that discourages overconfident predictions, often improving generalization and probability calibration.",
      "tags": [
        "ch05",
        "label-smoothing",
        "regularization"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-050",
      "front": "What is class imbalance and why is it a problem?",
      "back": "<b>Class imbalance</b>: When classes have very different numbers of samples (e.g., 99% negative, 1% positive).<br><br><b>Problems</b>:<ul><li>Model can achieve high accuracy by predicting majority class</li><li>Minority class is poorly learned</li><li>Standard metrics (accuracy) are misleading</li></ul><b>Examples</b>: Fraud detection, disease diagnosis, rare event prediction.<br><br><b>Metrics for imbalanced data</b>:<ul><li>Precision, recall, F1-score</li><li>AUROC (area under ROC curve)</li><li>AUPRC (area under precision-recall curve)</li><li>Matthews correlation coefficient</li></ul>",
      "tags": [
        "ch05",
        "class-imbalance",
        "classification"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-051",
      "front": "How do you handle class imbalance in training?",
      "back": "<b>Data-level methods</b>:<ul><li><b>Oversampling minority</b>: Duplicate minority samples (risk: overfitting)</li><li><b>Undersampling majority</b>: Remove majority samples (risk: lose information)</li><li><b>SMOTE</b>: Generate synthetic minority samples by interpolating</li></ul><b>Algorithm-level methods</b>:<ul><li><b>Class weights</b>: Weight loss inversely proportional to class frequency</li><li><b>Focal loss</b>: Down-weight easy examples, focus on hard ones</li><li><b>Threshold tuning</b>: Adjust decision threshold post-training</li></ul><b>Best practices</b>:<ul><li>Use appropriate metrics (F1, AUPRC)</li><li>Start with class weights, then try data-level methods</li><li>Cross-validate on stratified splits</li></ul>",
      "tags": [
        "ch05",
        "class-imbalance",
        "techniques"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-053",
      "front": "When should you use AUPRC vs AUROC for imbalanced data?",
      "back": "<b>AUROC</b> (Area Under ROC Curve):<ul><li>ROC plots TPR vs FPR</li><li>Insensitive to class imbalance in some sense</li><li>Can be overly optimistic when negatives vastly outnumber positives</li></ul><b>AUPRC</b> (Area Under Precision-Recall Curve):<ul><li>PR curve plots Precision vs Recall</li><li>Focuses on positive class performance</li><li>More informative when positives are rare</li></ul><b>Rule of thumb</b>:<ul><li>Balanced classes: AUROC is fine</li><li>Imbalanced (rare positive): AUPRC is more informative</li><li>When false positives are costly: focus on precision</li><li>When false negatives are costly: focus on recall</li></ul>",
      "tags": [
        "ch05",
        "auprc",
        "auroc"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-054",
      "front": "How do you derive the gradient of the logistic regression loss?",
      "back": "<b>Setup</b>: For binary logistic regression with \\( y = \\sigma(\\vec{w}^T \\vec{x}) \\) and cross-entropy loss \\( E = -[t \\ln y + (1-t) \\ln(1-y)] \\).<br><br><b>Key derivative</b>: \\( \\frac{d\\sigma(a)}{da} = \\sigma(a)(1-\\sigma(a)) = y(1-y) \\)<br><br><b>Derivation</b>:<br>\\( \\frac{\\partial E}{\\partial w_j} = -\\left[\\frac{t}{y} - \\frac{1-t}{1-y}\\right] \\cdot y(1-y) \\cdot x_j \\)<br>\\( = -[t(1-y) - (1-t)y] \\cdot x_j \\)<br>\\( = (y - t) \\cdot x_j \\)<br><br><b>Result</b>: \\( \\nabla_w E = (y - t) \\vec{x} \\)<br><br><b>Implicit assumptions</b>:<ul><li>Data is IID (independent, identically distributed)</li><li>Linear decision boundary in feature space</li><li>Well-specified model (true relationship is logistic)</li></ul>",
      "tags": [
        "ch05",
        "logistic-regression",
        "gradient"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-055",
      "front": "What is the difference between calibration and accuracy in classifiers?",
      "back": "<b>Accuracy</b>: How often the predicted class is correct.<br>\\( \\text{Accuracy} = \\frac{\\text{Correct predictions}}{\\text{Total predictions}} \\)<br><br><b>Calibration</b>: How well predicted probabilities match actual frequencies.<br><b>Intuition</b>: If model says \"80% chance of rain\" for 100 days, it should rain on ~80 of them.<br><br><b>Key insight</b>: A model can be <b>accurate but miscalibrated</b> (overconfident predictions), or <b>calibrated but less accurate</b>.<br><br><b>Measuring calibration</b>:<ul><li><b>Reliability diagram</b>: Plot predicted probability vs actual frequency</li><li><b>Expected Calibration Error (ECE)</b>: Weighted average of calibration gaps across bins</li><li><b>Brier score</b>: MSE of probability predictions</li></ul><b>Improving calibration</b>:<ul><li>Temperature scaling (divide logits by T)</li><li>Platt scaling (fit sigmoid to logits)</li><li>Isotonic regression</li></ul>",
      "tags": [
        "ch05",
        "calibration",
        "classification"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-056",
      "front": "What are proper scoring rules and why do they matter?",
      "back": "<b>Proper scoring rule</b>: A loss function that is minimized when the predicted probabilities match the true probabilities.<br><br><b>Why it matters</b>: Improper scoring rules can incentivize wrong probability predictions. A proper rule ensures honest probability reporting is optimal.<br><br><b>Examples of proper scoring rules</b>:<ul><li><b>Log loss (cross-entropy)</b>: \\( -\\sum_k p_k \\log q_k \\)</li><li><b>Brier score</b>: \\( \\frac{1}{K}\\sum_k (p_k - q_k)^2 \\)</li><li><b>Spherical score</b>: \\( \\frac{q_k}{\\|\\vec{q}\\|} \\)</li></ul><b>Strictly proper</b>: Unique minimum at true distribution.<br><br><b>Improper example</b>: 0-1 accuracy (only cares about argmax, not probability values).<br><br><b>Practical implications</b>:<ul><li>Use cross-entropy for training classifiers</li><li>Use Brier score for evaluating probability quality</li><li>Avoid metrics that ignore probability magnitudes</li></ul>",
      "tags": [
        "ch05",
        "proper-scoring-rules",
        "loss-functions"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-058",
      "front": "What is a decision region in classification?",
      "back": "A region of input space where all points are assigned to the same class.<br>The input space is divided into decision regions \\( R_k \\) for each class \\( C_k \\).",
      "tags": [
        "ch05",
        "classification",
        "decision-regions"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-059",
      "front": "What are decision boundaries (decision surfaces)?",
      "back": "The boundaries between decision regions.<br>For a linear discriminant: the hyperplane where \\( y(\\vec{x}) = 0 \\), i.e., \\( \\vec{w}^T\\vec{x} + w_0 = 0 \\).",
      "tags": [
        "ch05",
        "classification",
        "decision-boundaries"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-060",
      "front": "What does it mean for data to be linearly separable?",
      "back": "Data from different classes can be separated by a linear decision boundary (hyperplane).<br>If classes overlap, they are not linearly separable.",
      "tags": [
        "ch05",
        "classification",
        "linear-separability"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-061",
      "front": "What is a discriminant function?",
      "back": "A function that takes an input vector \\( \\vec{x} \\) and assigns it directly to one of K classes.<br>Simplest form for two classes:<br>\\( y(\\vec{x}) = \\vec{w}^T\\vec{x} + w_0 \\)<br>Classify as \\( C_1 \\) if \\( y(\\vec{x}) > 0 \\), else \\( C_2 \\).",
      "tags": [
        "ch05",
        "classification",
        "discriminant"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-062",
      "front": "What is the geometric interpretation of the weight vector in a linear discriminant?",
      "back": "The weight vector \\( \\vec{w} \\) is <b>orthogonal to the decision surface</b>.<br>The bias \\( w_0 \\) determines the <b>location</b> of the decision surface (its distance from the origin).",
      "tags": [
        "ch05",
        "classification",
        "geometry"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-063",
      "front": "What is a one-versus-the-rest classifier?",
      "back": "For K classes, train K binary classifiers, each separating one class from all others combined.<br>Problem: Can lead to ambiguous regions where multiple classifiers claim the point, or no classifier claims it.",
      "tags": [
        "ch05",
        "multi-class",
        "classification"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-064",
      "front": "What is a one-versus-one classifier?",
      "back": "For K classes, train \\( K(K-1)/2 \\) binary classifiers for every pair of classes.<br>Requires more classifiers than one-versus-the-rest but can still lead to ambiguous regions.",
      "tags": [
        "ch05",
        "multi-class",
        "classification"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-065",
      "front": "For K-class linear discriminants, what shape are the decision regions?",
      "back": "The decision regions are always <b>singly connected and convex</b>.<br>This follows from the linearity of the discriminant functions.",
      "tags": [
        "ch05",
        "classification",
        "decision-regions"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-066",
      "front": "What is a discriminative probabilistic model?",
      "back": "A model that directly models the posterior probability \\( p(C_k|\\vec{x}) \\) without modeling the class-conditional densities.<br>Examples: Logistic regression, neural network classifiers.<br>Contrast with generative models.",
      "tags": [
        "ch05",
        "discriminative",
        "probabilistic"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-067",
      "front": "What is a generative probabilistic model?",
      "back": "A model that computes posterior probabilities using Bayes' theorem by modeling:<br><ul><li>Class-conditional densities \\( p(\\vec{x}|C_k) \\)</li><li>Prior probabilities \\( p(C_k) \\)</li></ul>Then uses: \\( p(C_k|\\vec{x}) = \\frac{p(\\vec{x}|C_k)p(C_k)}{p(\\vec{x})} \\)<br>Also models the input distribution \\( p(\\vec{x}) \\).",
      "tags": [
        "ch05",
        "generative",
        "probabilistic"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-068",
      "front": "What is the confusion matrix in classification?",
      "back": "A table showing prediction outcomes:<br>|  | Predicted + | Predicted - |<br>|--|-------------|-------------|<br>| Actual + | True Positive (TP) | False Negative (FN) |<br>| Actual - | False Positive (FP) | True Negative (TN) |<br>False positives = Type 1 errors<br>False negatives = Type 2 errors",
      "tags": [
        "ch05",
        "confusion-matrix",
        "evaluation"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-069",
      "front": "Why can accuracy be misleading for imbalanced classes?",
      "back": "With strongly imbalanced classes, a naive classifier that always predicts the majority class can achieve high accuracy.<br>Example: 99% negative class - predicting all negative gives 99% accuracy but is useless.<br>Need metrics like precision, recall, F-score instead.",
      "tags": [
        "ch05",
        "accuracy",
        "imbalanced-classes"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-070",
      "front": "What is precision in classification?",
      "back": "The fraction of positive predictions that are correct:<br>\\( \\text{Precision} = \\frac{TP}{TP + FP} \\)<br>Represents the probability that a positive prediction is actually positive.<br>Also called Positive Predictive Value (PPV).",
      "tags": [
        "ch05",
        "precision",
        "evaluation"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-071",
      "front": "What is recall in classification?",
      "back": "The fraction of actual positives that are correctly identified:<br>\\( \\text{Recall} = \\frac{TP}{TP + FN} \\)<br>Also called sensitivity or True Positive Rate (TPR).",
      "tags": [
        "ch05",
        "recall",
        "evaluation"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-072",
      "front": "What is the false positive rate (FPR)?",
      "back": "The fraction of actual negatives that are incorrectly classified as positive:<br>\\( \\text{FPR} = \\frac{FP}{FP + TN} \\)<br>Also called the fall-out rate. Used in ROC curves.",
      "tags": [
        "ch05",
        "fpr",
        "evaluation"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-073",
      "front": "What is the false discovery rate (FDR)?",
      "back": "The fraction of positive predictions that are incorrect:<br>\\( \\text{FDR} = \\frac{FP}{FP + TP} = 1 - \\text{Precision} \\)<br>The complement of precision.",
      "tags": [
        "ch05",
        "fdr",
        "evaluation"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-074",
      "front": "What is an ROC curve?",
      "back": "<b>Receiver Operating Characteristic</b> curve: a plot of True Positive Rate vs False Positive Rate as the classification threshold varies.<br>Each point represents a different threshold, giving a different confusion matrix.<br>Useful for comparing classifiers and choosing operating points.",
      "tags": [
        "ch05",
        "roc",
        "evaluation"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-075",
      "front": "What does a classifier below the diagonal mean on an ROC curve?",
      "back": "It performs <b>worse than random guessing</b>.<br>The diagonal represents random classification.<br>Above diagonal = better than random.<br>Perfect classifier = top-left corner (TPR=1, FPR=0).",
      "tags": [
        "ch05",
        "roc",
        "evaluation"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-076",
      "front": "What is AUC (Area Under the ROC Curve)?",
      "back": "A summary measure of classifier performance:<br><ul><li>AUC = 0.5: Random guessing</li><li>AUC = 1.0: Perfect classifier</li></ul>Represents the probability that a randomly chosen positive example ranks higher than a randomly chosen negative example.",
      "tags": [
        "ch05",
        "auc",
        "evaluation"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-077",
      "front": "What is the F-score?",
      "back": "The harmonic mean of precision and recall:<br>\\( F_1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}} \\)<br>Balances precision and recall. The general \\( F_\\beta \\) score weights recall \\( \\beta \\) times as much as precision.",
      "tags": [
        "ch05",
        "f-score",
        "evaluation"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-078",
      "front": "What is the reject option in classification?",
      "back": "Refusing to classify inputs where the classifier is uncertain.<br>Implemented by introducing a threshold \\( \\theta \\) and rejecting inputs where the largest posterior probability \\( \\max_k p(C_k|\\vec{x}) < \\theta \\).<br>Reduces errors at the cost of leaving some inputs unclassified.",
      "tags": [
        "ch05",
        "reject-option",
        "classification"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-079",
      "front": "What are four reasons to compute posterior probabilities even if you just need class predictions?",
      "back": "<ol><li><b>Minimizing risk</b>: Incorporate different misclassification costs</li><li><b>Reject option</b>: Refuse uncertain predictions</li><li><b>Compensating for class priors</b>: Adjust for different train/test class distributions</li><li><b>Combining models</b>: Properly combine predictions from multiple models</li></ol>",
      "tags": [
        "ch05",
        "posterior",
        "decision-theory"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-080",
      "front": "What is a precision-recall (PR) curve?",
      "back": "A <b>precision-recall curve</b> plots <b>precision</b> vs <b>recall</b> as the classification threshold varies.<br><br>Each threshold gives a different confusion matrix, hence different precision/recall.<br><br>PR curves are especially informative when the <b>positive class is rare</b>.<br><br><b>Intuition</b>: With rare positives (e.g., 1% fraud), a bad classifier predicting 'always negative' gets 99% accuracy and low FPR (few false positives relative to many true negatives). ROC curves, which use FPR, can look deceptively good. But precision asks: 'Of predicted positives, how many are correct?' This exposes the failure - if you never predict positive, precision is undefined; if you predict a few, precision will be low because true positives are so rare. PR curves focus entirely on the positive class, ignoring the overwhelming true negatives that mask poor performance.",
      "tags": [
        "ch05",
        "precision-recall",
        "evaluation"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-081",
      "front": "What is PR-AUC (area under the precision-recall curve)?",
      "back": "<b>PR-AUC</b> summarizes a PR curve as a single number (area under the curve).<br><br><b>Baseline</b>: A random classifier has precision equal to the positive class prevalence \\( p(y=1) \\), so the baseline PR curve is a horizontal line at that value.<br><br>Higher PR-AUC indicates better performance on the positive class.",
      "tags": [
        "ch05",
        "pr-auc",
        "evaluation"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-082",
      "front": "What does it mean for a classifier to be well-calibrated?",
      "back": "A classifier is <b>calibrated</b> if its predicted probabilities match empirical frequencies.<br><br><b>Example</b>: Among examples where the model predicts 0.8, about 80% should truly be positive.<br><br><b>Why useful</b>: Calibration matters when probabilities drive decisions (thresholding, ranking with costs, risk estimates), not just class labels.<br><br><b>How to check</b>: reliability diagram (calibration curve).<br><b>How to fix</b>: post-hoc calibration such as <b>temperature scaling</b> (common for neural nets).",
      "tags": [
        "ch05",
        "calibration",
        "evaluation"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-083",
      "front": "What is focal loss and when is it used?",
      "back": "<b>Focal loss</b> down-weights easy examples to focus training on hard ones:<br><br>\\( FL(p_t) = -\\alpha_t (1 - p_t)^\\gamma \\log(p_t) \\)<br><br>where \\( p_t \\) is the predicted probability for the true class.<br><br><b>Key idea</b>: The \\( (1 - p_t)^\\gamma \\) factor reduces the loss for well-classified examples (high \\( p_t \\)).<br><br><b>Parameters</b>:<ul><li>\\( \\gamma \\): Focusing parameter (typically 2). Higher = more focus on hard examples.</li><li>\\( \\alpha_t \\): Class weight for imbalance</li></ul><b>When to use</b>: Object detection (many easy negatives, few hard positives), extreme class imbalance.<br><br><b>Origin</b>: RetinaNet (2017).",
      "tags": [
        "ch05",
        "focal-loss",
        "imbalanced-classes"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-084",
      "front": "What is plotted on the x-axis and y-axis of a ROC curve?",
      "back": "<b>X-axis</b>: False Positive Rate (FPR) = FP / (FP + TN)<br><b>Y-axis</b>: True Positive Rate (TPR) = TP / (TP + FN)<br><br><b>Intuition</b>:<ul><li>X-axis: Of all actual negatives, what fraction did we incorrectly call positive?</li><li>Y-axis: Of all actual positives, what fraction did we correctly identify?</li></ul><b>Ideal point</b>: Top-left corner (0, 1) - zero false positives, all true positives detected.",
      "tags": [
        "ch05",
        "roc",
        "evaluation"
      ]
    },
    {
      "uid": "deep-learning-foundations-and-concepts-05-085",
      "front": "How do you 'read' the ROC curve and what information does it tell you?",
      "back": "The ROC curve plots <b>false positive rate</b> (x-axis) vs <b>true positive rate</b> (y-axis) for a range of thresholds.<br><br><b>Why thresholds matter</b>: Classifiers usually output a continuous score rather than a binary decision. You must choose a threshold to binarize this score before computing classification metrics.<br><br><b>The trade-off</b>: Raising or lowering the threshold directly trades off between TPR and FPR:<ul><li>Lower threshold: More positives predicted → higher TPR but also higher FPR</li><li>Higher threshold: Fewer positives predicted → lower FPR but also lower TPR</li></ul><b>Reading the curve</b>: The ROC curve shows this entire trade-off. The <b>area under the curve (AUC)</b> measures how 'nice' this trade-off is - a larger area means the classifier can achieve high TPR without incurring high FPR.",
      "tags": [
        "ch05",
        "roc",
        "evaluation"
      ]
    }
  ]
}
