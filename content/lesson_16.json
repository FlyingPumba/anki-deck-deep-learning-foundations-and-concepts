{
  "id": "16",
  "title": "Lesson 16: Continuous Latent Variables",
  "lesson_title": "Continuous Latent Variables",
  "objectives": [
    "Understand Principal Component Analysis (PCA)",
    "Learn both maximum variance and minimum error formulations of PCA",
    "Master probabilistic PCA and its advantages",
    "Understand data whitening and standardization",
    "Learn about factor analysis and ICA"
  ],
  "cards": [
    {
      "uid": "16-001",
      "front": "What is the motivation for continuous latent variable models?",
      "back": "Many data sets lie close to a <b>manifold of lower dimensionality</b> than the data space.<br>Example: Handwritten digits with varying position and rotation have 3 degrees of freedom (translation + rotation) in a 10,000-pixel space.<br>The translation and rotation are <b>latent variables</b> not directly observed.",
      "tags": [
        "ch16",
        "latent-variables",
        "motivation"
      ]
    },
    {
      "uid": "16-002",
      "front": "What is Principal Component Analysis (PCA)?",
      "back": "A technique for <b>linear dimensionality reduction</b>.<br>Applications:<br><ul><li>Dimensionality reduction</li><li>Lossy data compression</li><li>Feature extraction</li><li>Data visualization</li></ul>Projects data onto a lower-dimensional <b>principal subspace</b>.",
      "tags": [
        "ch16",
        "pca",
        "definition"
      ]
    },
    {
      "uid": "16-003",
      "front": "What are the two equivalent definitions of PCA?",
      "back": "<ol><li><b>Maximum variance</b>: Linear projection that maximizes variance of projected data</li><li><b>Minimum error</b>: Linear projection that minimizes mean squared distance between points and their projections</li></ol>Both lead to same solution: eigenvectors of covariance matrix.",
      "tags": [
        "ch16",
        "pca",
        "formulations"
      ]
    },
    {
      "uid": "16-004",
      "front": "How is the first principal component found?",
      "back": "Maximize projected variance \\( \\mathbf{u}_1^T \\mathbf{S} \\mathbf{u}_1 \\) subject to \\( ||\\mathbf{u}_1|| = 1 \\).<br>Using Lagrange multipliers: \\( \\mathbf{S} \\mathbf{u}_1 = \\lambda_1 \\mathbf{u}_1 \\)<br>Solution: \\( \\mathbf{u}_1 \\) is eigenvector of covariance matrix S with <b>largest eigenvalue</b> \\( \\lambda_1 \\).<br>The variance equals \\( \\lambda_1 \\).",
      "tags": [
        "ch16",
        "pca",
        "first-component"
      ]
    },
    {
      "uid": "16-005",
      "front": "What is the data covariance matrix in PCA?",
      "back": "\\( \\mathbf{S} = \\frac{1}{N} \\sum_{n=1}^{N} (\\mathbf{x}_n - \\bar{\\mathbf{x}})(\\mathbf{x}_n - \\bar{\\mathbf{x}})^T \\)<br>where \\( \\bar{\\mathbf{x}} = \\frac{1}{N} \\sum_n \\mathbf{x}_n \\) is the sample mean.<br>The eigenvectors of S define the principal directions.",
      "tags": [
        "ch16",
        "pca",
        "covariance"
      ]
    },
    {
      "uid": "16-006",
      "front": "How is the PCA reconstruction error computed?",
      "back": "Using M principal components, the error is:<br>\\( J = \\sum_{i=M+1}^{D} \\lambda_i \\)<br>The sum of eigenvalues of <b>discarded</b> eigenvectors.<br>Choose the M eigenvectors with <b>largest</b> eigenvalues to minimize J.",
      "tags": [
        "ch16",
        "pca",
        "error"
      ]
    },
    {
      "uid": "16-007",
      "front": "What is the PCA approximation of a data point?",
      "back": "\\( \\tilde{\\mathbf{x}}_n = \\bar{\\mathbf{x}} + \\sum_{i=1}^{M} (\\mathbf{x}_n^T \\mathbf{u}_i - \\bar{\\mathbf{x}}^T \\mathbf{u}_i) \\mathbf{u}_i \\)<br>Compression: Replace D-dimensional vector with M coefficients \\( (\\mathbf{x}_n^T \\mathbf{u}_i - \\bar{\\mathbf{x}}^T \\mathbf{u}_i) \\).<br>Reconstruction: Linear combination of M eigenvectors plus mean.",
      "tags": [
        "ch16",
        "pca",
        "reconstruction"
      ]
    },
    {
      "uid": "16-008",
      "front": "What is data whitening (sphering)?",
      "back": "Transformation to give data <b>zero mean</b> and <b>identity covariance</b>:<br>\\( \\mathbf{y}_n = \\mathbf{L}^{-1/2} \\mathbf{U}^T (\\mathbf{x}_n - \\bar{\\mathbf{x}}) \\)<br>where U contains eigenvectors and L is diagonal matrix of eigenvalues.<br>Result: Variables become uncorrelated with unit variance.",
      "tags": [
        "ch16",
        "whitening",
        "preprocessing"
      ]
    },
    {
      "uid": "16-009",
      "front": "What is data standardization?",
      "back": "Transform each variable to have <b>zero mean</b> and <b>unit variance</b>:<br>\\( z_i = \\frac{x_i - \\mu_i}{\\sigma_i} \\)<br>Used when variables have different units or scales.<br>Correlation matrix of standardized data has \\( \\rho_{ij} = 1 \\) if perfectly correlated.",
      "tags": [
        "ch16",
        "standardization",
        "preprocessing"
      ]
    },
    {
      "uid": "16-010",
      "front": "How to handle PCA when N < D?",
      "back": "When data points N < dimensionality D:<br><ul><li>At most N-1 non-zero eigenvalues exist</li><li>Solve eigenproblem for \\( N \\times N \\) matrix \\( \\frac{1}{N} \\mathbf{X} \\mathbf{X}^T \\) instead of \\( D \\times D \\)</li><li>Cost: \\( O(N^3) \\) instead of \\( O(D^3) \\)</li><li>Recover eigenvectors via \\( \\mathbf{u}_i = \\frac{1}{\\sqrt{N\\lambda_i}} \\mathbf{X}^T \\mathbf{v}_i \\)</li></ul>",
      "tags": [
        "ch16",
        "pca",
        "high-dimensional"
      ]
    },
    {
      "uid": "16-011",
      "front": "What is probabilistic PCA?",
      "back": "PCA reformulated as a <b>linear-Gaussian latent variable model</b>:<br>\\( p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{z}|\\mathbf{0}, \\mathbf{I}) \\)<br>\\( p(\\mathbf{x}|\\mathbf{z}) = \\mathcal{N}(\\mathbf{x}|\\mathbf{W}\\mathbf{z} + \\boldsymbol{\\mu}, \\sigma^2 \\mathbf{I}) \\)<br>The D-dimensional x is a linear transformation of M-dimensional z plus Gaussian noise.",
      "tags": [
        "ch16",
        "probabilistic-pca",
        "model"
      ]
    },
    {
      "uid": "16-012",
      "front": "What are the advantages of probabilistic PCA over standard PCA?",
      "back": "<ol><li><b>EM algorithm</b> for efficient computation</li><li>Handle <b>missing values</b> in data</li><li>Build <b>mixtures</b> of PCA models</li><li><b>Compare</b> with other density models via likelihood</li><li><b>Bayesian</b> treatment to find dimensionality automatically</li><li><b>Generate samples</b> from the distribution</li></ol>",
      "tags": [
        "ch16",
        "probabilistic-pca",
        "advantages"
      ]
    },
    {
      "uid": "16-013",
      "front": "What is the marginal distribution in probabilistic PCA?",
      "back": "Marginalizing over z:<br>\\( p(\\mathbf{x}) = \\mathcal{N}(\\mathbf{x}|\\boldsymbol{\\mu}, \\mathbf{C}) \\)<br>where:<br>\\( \\mathbf{C} = \\mathbf{W}\\mathbf{W}^T + \\sigma^2 \\mathbf{I} \\)<br>This is a Gaussian with constrained covariance structure (low-rank plus diagonal).",
      "tags": [
        "ch16",
        "probabilistic-pca",
        "marginal"
      ]
    },
    {
      "uid": "16-014",
      "front": "What is the maximum likelihood solution for probabilistic PCA?",
      "back": "The MLE for W is:<br>\\( \\mathbf{W}_{ML} = \\mathbf{U}_M (\\mathbf{L}_M - \\sigma^2 \\mathbf{I})^{1/2} \\mathbf{R} \\)<br>where:<br><ul><li>\\( \\mathbf{U}_M \\): M leading eigenvectors of S</li><li>\\( \\mathbf{L}_M \\): corresponding eigenvalues</li><li>R: arbitrary rotation matrix</li></ul>Reduces to standard PCA as \\( \\sigma^2 \\to 0 \\).",
      "tags": [
        "ch16",
        "probabilistic-pca",
        "mle"
      ]
    },
    {
      "uid": "16-015",
      "front": "What is factor analysis?",
      "back": "Similar to probabilistic PCA but with <b>diagonal</b> noise covariance:<br>\\( p(\\mathbf{x}|\\mathbf{z}) = \\mathcal{N}(\\mathbf{x}|\\mathbf{W}\\mathbf{z} + \\boldsymbol{\\mu}, \\boldsymbol{\\Psi}) \\)<br>where \\( \\boldsymbol{\\Psi} = \\text{diag}(\\psi_1, \\ldots, \\psi_D) \\).<br>Allows different noise levels for different observed variables.<br>No closed-form MLE; use EM algorithm.",
      "tags": [
        "ch16",
        "factor-analysis",
        "model"
      ]
    },
    {
      "uid": "16-016",
      "front": "What is Independent Component Analysis (ICA)?",
      "back": "Latent variable model where sources are <b>non-Gaussian and independent</b>:<br>\\( \\mathbf{x} = \\mathbf{A}\\mathbf{s} \\)<br>Goal: Recover independent sources s from observed mixtures x.<br>Requires non-Gaussian distributions (Gaussian sources are unidentifiable).<br>Application: Blind source separation (cocktail party problem).",
      "tags": [
        "ch16",
        "ica",
        "model"
      ]
    },
    {
      "uid": "16-017",
      "front": "Why can't ICA recover Gaussian sources?",
      "back": "For Gaussian distributions, <b>uncorrelated implies independent</b>.<br>Any rotation of independent Gaussians gives independent Gaussians.<br>The mixing matrix A is only identifiable up to rotation.<br>Non-Gaussian distributions break this rotational symmetry.",
      "tags": [
        "ch16",
        "ica",
        "identifiability"
      ]
    },
    {
      "uid": "16-018",
      "front": "What is a Kalman filter?",
      "back": "A linear-Gaussian model for <b>sequential data</b> with continuous latent variables.<br>\\( p(\\mathbf{z}_t | \\mathbf{z}_{t-1}) = \\mathcal{N}(\\mathbf{z}_t | \\mathbf{A}\\mathbf{z}_{t-1}, \\mathbf{Q}) \\)<br>\\( p(\\mathbf{x}_t | \\mathbf{z}_t) = \\mathcal{N}(\\mathbf{x}_t | \\mathbf{C}\\mathbf{z}_t, \\mathbf{R}) \\)<br>Used for tracking, navigation, signal processing.",
      "tags": [
        "ch16",
        "kalman",
        "sequential"
      ]
    },
    {
      "uid": "16-019",
      "front": "How does the EM algorithm apply to probabilistic PCA?",
      "back": "E step: Compute posterior \\( p(\\mathbf{z}_n | \\mathbf{x}_n, \\mathbf{W}, \\sigma^2) \\)<br>M step: Update W and \\( \\sigma^2 \\) using expected sufficient statistics.<br>Advantages:<br><ul><li>Avoids explicit covariance matrix computation</li><li>Cost O(NM) per iteration instead of O(D^3)</li><li>Handles missing data naturally</li></ul>",
      "tags": [
        "ch16",
        "em-algorithm",
        "pca"
      ]
    },
    {
      "uid": "16-020",
      "front": "What is the relationship between PCA and rotation invariance?",
      "back": "If M = D (no dimensionality reduction), PCA is just a <b>rotation</b> of coordinate axes.<br>Axes align with principal components (eigenvectors of covariance).<br>The MLE solution for probabilistic PCA has <b>rotational ambiguity</b> - any rotation R of W gives the same likelihood.",
      "tags": [
        "ch16",
        "pca",
        "rotation"
      ]
    },
    {
      "uid": "16-021",
      "front": "Why is the principal subspace projection orthogonal?",
      "back": "The error vector \\( \\mathbf{x}_n - \\tilde{\\mathbf{x}}_n \\) lies in the space <b>orthogonal</b> to principal subspace.<br>This minimizes squared projection error because:<br><ul><li>Projected points can move freely within subspace</li><li>Minimum distance from a point to a subspace is perpendicular distance</li></ul>",
      "tags": [
        "ch16",
        "pca",
        "projection"
      ]
    },
    {
      "uid": "16-022",
      "front": "What determines how many principal components to keep?",
      "back": "Methods to choose M:<br><ol><li><b>Variance explained</b>: Keep components explaining X% of variance</li><li><b>Elbow method</b>: Look for \"elbow\" in eigenvalue spectrum</li><li><b>Cross-validation</b>: Choose M minimizing held-out reconstruction error</li><li><b>Bayesian PCA</b>: Automatically determines M from data</li><li><b>Application-specific</b>: Based on downstream task performance</li></ol>",
      "tags": [
        "ch16",
        "pca",
        "model-selection"
      ]
    },
    {
      "uid": "16-023",
      "front": "What is the computational complexity of standard PCA?",
      "back": "Full eigenvector decomposition: \\( O(D^3) \\)<br>For M principal components only: \\( O(MD^2) \\) using power method<br>For N < D: \\( O(N^3) \\) by using the smaller matrix \\( \\mathbf{X}\\mathbf{X}^T \\)<br>EM for probabilistic PCA: \\( O(NMD) \\) per iteration",
      "tags": [
        "ch16",
        "pca",
        "complexity"
      ]
    },
    {
      "uid": "16-024",
      "front": "What is the generative view of latent variable models?",
      "back": "To generate a data point:<br><ol><li>Sample latent variable z from prior p(z)</li><li>Sample observation x from conditional p(x|z)</li></ol>The departures from the manifold are interpreted as <b>noise</b>.<br>This view enables sampling new data points from the learned distribution.",
      "tags": [
        "ch16",
        "generative",
        "latent-variables"
      ]
    },
    {
      "uid": "16-025",
      "front": "How do nonlinear latent variable models extend PCA?",
      "back": "Replace linear mapping with <b>neural network</b>:<br>\\( p(\\mathbf{x}|\\mathbf{z}) = \\mathcal{N}(\\mathbf{x}|f(\\mathbf{z}; \\theta), \\sigma^2 \\mathbf{I}) \\)<br>where f is a deep neural network.<br>Examples:<br><ul><li>Variational Autoencoders (VAEs)</li><li>Normalizing Flows</li><li>Diffusion Models</li></ul>Can model complex nonlinear manifolds.",
      "tags": [
        "ch16",
        "nonlinear",
        "deep-learning"
      ]
    }
  ]
}
