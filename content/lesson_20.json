{
  "id": "20",
  "title": "Lesson 20: Diffusion Models",
  "lesson_title": "Diffusion Models",
  "objectives": [
    "Understand the forward noising process in diffusion models",
    "Learn the reverse denoising process",
    "Master the diffusion model ELBO and training objective",
    "Understand noise prediction and sampling",
    "Learn about score-based models and connections"
  ],
  "cards": [
    {
      "uid": "20-001",
      "front": "What is a diffusion model?",
      "back": "A generative model that learns to <b>reverse a gradual noising process</b>.<br>Two processes:<br><ol><li><b>Forward</b>: Gradually add noise until data becomes pure Gaussian noise</li><li><b>Reverse</b>: Learn neural network to denoise step by step</li></ol>Sampling: Start from noise, iteratively denoise to generate data.",
      "tags": [
        "ch20",
        "diffusion",
        "definition"
      ]
    },
    {
      "uid": "20-002",
      "front": "What is the forward noising process in diffusion models?",
      "back": "A Markov chain that gradually adds Gaussian noise:<br>\\( q(z_t | z_{t-1}) = \\mathcal{N}(z_t | \\sqrt{1-\\beta_t} z_{t-1}, \\beta_t I) \\)<br>Starting from x = \\( z_0 \\), after T steps, \\( z_T \\) is approximately standard Gaussian.<br>No learnable parameters - just fixed noise schedule \\( \\{\\beta_t\\} \\).",
      "tags": [
        "ch20",
        "forward-process",
        "noising"
      ]
    },
    {
      "uid": "20-003",
      "front": "What is the diffusion kernel?",
      "back": "A formula to sample <b>any step directly</b> without iterating:<br>\\( q(z_t | x) = \\mathcal{N}(z_t | \\sqrt{\\alpha_t} x, (1-\\alpha_t) I) \\)<br>where \\( \\alpha_t = \\prod_{\\tau=1}^{t} (1-\\beta_\\tau) \\)<br>Enables efficient training by jumping to any timestep.",
      "tags": [
        "ch20",
        "diffusion-kernel",
        "efficiency"
      ]
    },
    {
      "uid": "20-004",
      "front": "What is the reverse denoising process in diffusion models?",
      "back": "A learned Markov chain that removes noise:<br>\\( p(z_{t-1} | z_t, w) = \\mathcal{N}(z_{t-1} | \\mu(z_t, w, t), \\beta_t I) \\)<br>Neural network \\( \\mu(z_t, w, t) \\) predicts the mean of denoised distribution.<br>One network handles all timesteps (t is an input).",
      "tags": [
        "ch20",
        "reverse-process",
        "denoising"
      ]
    },
    {
      "uid": "20-005",
      "front": "Why does the reverse process work for small noise steps?",
      "back": "When \\( \\beta_t \\ll 1 \\):<br><ul><li>Forward step changes z only slightly</li><li>True reverse \\( q(z_{t-1}|z_t) \\) is approximately Gaussian</li></ul>Neural network only needs to model Gaussian mean.<br>Trade-off: Small steps → many steps needed (T often 1000+).",
      "tags": [
        "ch20",
        "small-steps",
        "approximation"
      ]
    },
    {
      "uid": "20-006",
      "front": "How is the diffusion model trained?",
      "back": "Maximize the <b>ELBO</b> (evidence lower bound):<br>\\( L(w) = E_q[\\ln p(x|z_1)] - \\sum_{t=2}^{T} D_{KL}(q(z_{t-1}|z_t,x) || p(z_{t-1}|z_t,w)) \\)<br>Two parts:<br><ol><li><b>Reconstruction</b>: Generate x from \\( z_1 \\)</li><li><b>Consistency</b>: Match reverse to true posterior at each step</li></ol>",
      "tags": [
        "ch20",
        "elbo",
        "training"
      ]
    },
    {
      "uid": "20-007",
      "front": "Why predict noise instead of the denoised image?",
      "back": "Instead of network \\( \\mu \\) predicting denoised mean, network g predicts <b>total noise added</b>:<br>\\( g(z_t, w, t) \\approx \\epsilon_t \\)<br>Advantages:<br><ul><li>Simpler objective function</li><li>Better empirical results</li><li>Unified handling of all timesteps</li></ul>",
      "tags": [
        "ch20",
        "noise-prediction",
        "training"
      ]
    },
    {
      "uid": "20-008",
      "front": "What is the simplified diffusion training loss?",
      "back": "\\( L(w) = -\\sum_t ||g(\\sqrt{\\alpha_t} x + \\sqrt{1-\\alpha_t} \\epsilon, w, t) - \\epsilon||^2 \\)<br>Simple interpretation:<br><ol><li>Create noisy version of x using sampled noise \\( \\epsilon \\)</li><li>Train network to predict what noise was added</li><li>Squared error between predicted and actual noise</li></ol>",
      "tags": [
        "ch20",
        "loss-function",
        "training"
      ]
    },
    {
      "uid": "20-009",
      "front": "How does stochastic gradient descent work for diffusion training?",
      "back": "For each training step:<br><ol><li>Sample data point x from training set</li><li>Sample random timestep t from \\( \\{1, ..., T\\} \\)</li><li>Sample noise \\( \\epsilon \\sim \\mathcal{N}(0, I) \\)</li><li>Compute noisy \\( z_t \\) using diffusion kernel</li><li>Train network to predict \\( \\epsilon \\)</li></ol>Built-in data augmentation: Same x gets different noise each time.",
      "tags": [
        "ch20",
        "sgd",
        "training"
      ]
    },
    {
      "uid": "20-010",
      "front": "How do you generate samples from a trained diffusion model?",
      "back": "Starting from \\( z_T \\sim \\mathcal{N}(0, I) \\):<br>For t = T to 1:<br><ol><li>Predict noise: \\( \\hat{\\epsilon} = g(z_t, w, t) \\)</li><li>Compute mean: \\( \\mu = \\frac{1}{\\sqrt{1-\\beta_t}}(z_t - \\frac{\\beta_t}{\\sqrt{1-\\alpha_t}} \\hat{\\epsilon}) \\)</li><li>Sample: \\( z_{t-1} = \\mu + \\sqrt{\\beta_t} \\epsilon \\) (except t=1)</li></ol>Final \\( z_0 \\) is the generated sample.",
      "tags": [
        "ch20",
        "sampling",
        "generation"
      ]
    },
    {
      "uid": "20-011",
      "front": "What is the main computational drawback of diffusion models?",
      "back": "Sampling requires <b>multiple sequential passes</b> through the network.<br>Typically T = 1000+ steps.<br>Much slower than:<br><ul><li>GANs (single forward pass)</li><li>VAEs (single forward pass)</li><li>Normalizing flows (single forward pass)</li></ul>Mitigation: Faster sampling methods, continuous-time formulations.",
      "tags": [
        "ch20",
        "sampling",
        "cost"
      ]
    },
    {
      "uid": "20-012",
      "front": "What is a U-Net in the context of diffusion models?",
      "back": "A common neural network architecture for the denoising network.<br>Structure:<br><ul><li>Encoder: Progressively downsample</li><li>Decoder: Progressively upsample</li><li>Skip connections: Connect encoder to decoder at each scale</li></ul>Output has same dimensionality as input (predicts noise).",
      "tags": [
        "ch20",
        "unet",
        "architecture"
      ]
    },
    {
      "uid": "20-013",
      "front": "How does the noise schedule \\( \\{\\beta_t\\} \\) affect diffusion models?",
      "back": "The schedule controls how fast noise is added:<br><ul><li><b>Linear</b>: \\( \\beta_t \\) increases linearly</li><li><b>Cosine</b>: Smoother noise addition, often works better</li></ul>Key property: After T steps, data should be approximately standard Gaussian.<br>Typical: \\( \\beta_1 \\approx 10^{-4} \\), \\( \\beta_T \\approx 0.02 \\).",
      "tags": [
        "ch20",
        "noise-schedule",
        "hyperparameter"
      ]
    },
    {
      "uid": "20-014",
      "front": "What is the relationship between diffusion models and score matching?",
      "back": "The denoising network implicitly learns the <b>score function</b>:<br>\\( \\nabla_z \\ln p(z) \\)<br>Noise prediction ≈ negative score.<br>Connects to:<br><ul><li>Score-based generative models</li><li>Langevin dynamics sampling</li><li>Denoising score matching</li></ul>",
      "tags": [
        "ch20",
        "score-matching",
        "theory"
      ]
    },
    {
      "uid": "20-015",
      "front": "What are the advantages of diffusion models over GANs?",
      "back": "<ol><li><b>Stable training</b>: No adversarial dynamics</li></ol><ol><li><b>Mode coverage</b>: Don't suffer from mode collapse</li></ol><ol><li><b>Likelihood</b>: Can compute ELBO (approximate likelihood)</li></ol><ol><li><b>Quality</b>: State-of-the-art image generation</li></ol>Trade-off: Slower sampling than GANs.",
      "tags": [
        "ch20",
        "diffusion-vs-gan",
        "comparison"
      ]
    },
    {
      "uid": "20-016",
      "front": "Why do diffusion models have the same latent and data dimensionality?",
      "back": "The forward process adds noise to x, creating latent \\( z_t \\) of same size.<br>No compression like VAE bottleneck.<br>Similar to normalizing flows in this respect.<br>The 'latent' is just a noisy version of data, not a compact representation.",
      "tags": [
        "ch20",
        "dimensionality",
        "architecture"
      ]
    },
    {
      "uid": "20-017",
      "front": "How does conditioning work in diffusion models?",
      "back": "Add conditioning information c to the denoising network:<br>\\( g(z_t, w, t, c) \\)<br>Examples:<br><ul><li><b>Text-to-image</b>: c = text embedding</li><li><b>Class-conditional</b>: c = class label</li><li><b>Image editing</b>: c = input image + mask</li></ul>Enables controllable generation.",
      "tags": [
        "ch20",
        "conditioning",
        "control"
      ]
    },
    {
      "uid": "20-018",
      "front": "What is classifier-free guidance in diffusion models?",
      "back": "A technique to strengthen conditioning without a separate classifier.<br>Train both:<br><ul><li>Conditional model: \\( g(z_t, w, t, c) \\)</li><li>Unconditional model: \\( g(z_t, w, t) \\)</li></ul>At sampling: \\( \\hat{g} = g_{uncond} + s \\cdot (g_{cond} - g_{uncond}) \\)<br>Scale s > 1 strengthens conditioning effect.",
      "tags": [
        "ch20",
        "guidance",
        "conditioning"
      ]
    }
  ]
}
