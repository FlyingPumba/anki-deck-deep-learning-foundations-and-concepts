{
  "id": "13",
  "title": "Lesson 13: Graph Neural Networks",
  "lesson_title": "Graph Neural Networks",
  "objectives": [
    "Understand graph neural networks and their applications",
    "Learn about permutation invariance and equivariance",
    "Master message-passing neural networks",
    "Understand inductive vs transductive learning",
    "Learn about graph attention networks"
  ],
  "cards": [
    {
      "uid": "13-001",
      "front": "What are graph neural networks (GNNs)?",
      "back": "Deep learning models for <b>graph-structured data</b>.<br>Key requirement: Ensure <b>equivariance or invariance</b> with respect to node ordering.<br>Applications:<br><ul><li>Molecular property prediction</li><li>Social network analysis</li><li>Knowledge graphs</li><li>Traffic prediction</li></ul>",
      "tags": [
        "ch13",
        "gnn",
        "graphs"
      ]
    },
    {
      "uid": "13-002",
      "front": "What types of predictions can graph neural networks make?",
      "back": "Three levels:<br><ol><li><b>Node-level</b>: Predict properties of individual nodes</li><li><b>Edge-level</b>: Predict properties of edges/relationships</li><li><b>Graph-level</b>: Predict properties of the entire graph</li></ol>All must respect permutation symmetry.",
      "tags": [
        "ch13",
        "gnn",
        "prediction-levels"
      ]
    },
    {
      "uid": "13-003",
      "front": "What is the difference between inductive and transductive learning in GNNs?",
      "back": "<b>Inductive</b>: Learn from training graphs, generalize to new unseen graphs<br><b>Transductive</b>: Given the entire graph structure, predict labels for unlabeled nodes<br>Transductive nodes participate in message passing but aren't labeled during training.",
      "tags": [
        "ch13",
        "inductive",
        "transductive"
      ]
    },
    {
      "uid": "13-004",
      "front": "What is graph representation learning?",
      "back": "Learning <b>embedding vectors</b> for graph components (nodes, edges, or entire graph).<br>Goal: Represent graph structure in a form useful for downstream tasks.<br>Similar to word embeddings but for graph elements.",
      "tags": [
        "ch13",
        "representation-learning",
        "embeddings"
      ]
    },
    {
      "uid": "13-005",
      "front": "What is the adjacency matrix of a graph?",
      "back": "Matrix A where \\( A_{nm} = 1 \\) if edge exists between nodes n and m, else 0.<br>Problem: The matrix <b>depends on arbitrary node ordering</b>.<br>Permuting node labels gives different matrix for same graph.<br>GNNs must handle this permutation ambiguity.",
      "tags": [
        "ch13",
        "adjacency-matrix",
        "representation"
      ]
    },
    {
      "uid": "13-006",
      "front": "How does permuting node labels affect the adjacency matrix?",
      "back": "If P is a permutation matrix:<br><ul><li>Rows permuted by: \\( PA \\)</li><li>Columns permuted by: \\( AP^T \\)</li><li>Both: \\( PAP^T \\)</li></ul>This represents the same graph with different node labeling.",
      "tags": [
        "ch13",
        "permutation",
        "adjacency-matrix"
      ]
    },
    {
      "uid": "13-007",
      "front": "What permutation properties must GNN predictions satisfy?",
      "back": "<b>Graph-level predictions</b>: Must be <b>invariant</b> (same output regardless of node ordering)<br><b>Node-level predictions</b>: Must be <b>equivariant</b> (if inputs permuted, outputs permute the same way)<br>Network structure must enforce this.",
      "tags": [
        "ch13",
        "invariance",
        "equivariance"
      ]
    },
    {
      "uid": "13-008",
      "front": "What is a message-passing neural network (MPNN)?",
      "back": "A GNN architecture where nodes <b>exchange messages</b> with neighbors:<br><ol><li>Each node has embedding vector</li><li>Messages aggregated from neighbors</li><li>Node embedding updated based on messages</li><li>Repeat for multiple layers</li></ol>General framework for many GNN variants.",
      "tags": [
        "ch13",
        "mpnn",
        "message-passing"
      ]
    },
    {
      "uid": "13-009",
      "front": "Why must message aggregation be order-independent?",
      "back": "The aggregation function must depend <b>only on the set of inputs, not their ordering</b>.<br>Otherwise, results would depend on arbitrary node labeling.<br>Typical choices: Sum, mean, max (all permutation invariant).",
      "tags": [
        "ch13",
        "aggregation",
        "permutation-invariance"
      ]
    },
    {
      "uid": "13-010",
      "front": "How does summation vs. mean aggregation differ in GNNs?",
      "back": "<b>Summation</b>: Nodes with many neighbors have stronger influence<br><b>Mean</b>: Normalizes by neighbor count<br>Summation preserves degree information; mean treats all neighborhoods equally.<br>Choice depends on application.",
      "tags": [
        "ch13",
        "aggregation",
        "comparison"
      ]
    },
    {
      "uid": "13-011",
      "front": "What is the receptive field in a GNN?",
      "back": "The set of nodes that can influence a given node's output.<br>After L layers: Receptive field includes nodes up to L hops away.<br>Sparse graphs may require many layers for global information flow.",
      "tags": [
        "ch13",
        "receptive-field",
        "depth"
      ]
    },
    {
      "uid": "13-012",
      "front": "What is a super-node in GNNs?",
      "back": "A virtual node connected to <b>all other nodes</b> in the graph.<br>Allows global information exchange in one hop.<br>Helps with:<br><ul><li>Long-range dependencies</li><li>Sparse graph connectivity</li><li>Graph-level predictions</li></ul>",
      "tags": [
        "ch13",
        "super-node",
        "global-aggregation"
      ]
    },
    {
      "uid": "13-013",
      "front": "How do GNNs achieve permutation equivariance?",
      "back": "By using <b>shared parameters</b> across all nodes and <b>order-invariant aggregation</b>.<br>The same transformation is applied to every node.<br>If inputs are permuted, the same permutation applies to outputs.",
      "tags": [
        "ch13",
        "equivariance",
        "weight-sharing"
      ]
    },
    {
      "uid": "13-014",
      "front": "What is deep sets?",
      "back": "Learning functions over <b>unstructured sets</b> of variables.<br>Special case of GNNs where graph is fully connected (or has no edges).<br>Must be permutation invariant/equivariant.<br>Foundation for understanding set-based neural networks.",
      "tags": [
        "ch13",
        "deep-sets",
        "sets"
      ]
    },
    {
      "uid": "13-015",
      "front": "What is a readout layer in GNNs?",
      "back": "The <b>output layer</b> that produces final predictions from node embeddings.<br>For graph-level tasks: Aggregates all node embeddings (e.g., sum, mean) into single vector.<br>Must be permutation invariant for graph-level outputs.",
      "tags": [
        "ch13",
        "readout",
        "output-layer"
      ]
    },
    {
      "uid": "13-016",
      "front": "What are the three types of nodes in semi-supervised GNN learning?",
      "back": "<ol><li><b>Training nodes</b> (\\( V_{train} \\)): Labeled, used for loss</li></ol><ol><li><b>Transductive nodes</b> (\\( V_{trans} \\)): Unlabeled, participate in message passing during training</li></ol><ol><li><b>Inductive nodes</b> (\\( V_{induct} \\)): Only seen at inference time</li></ol>",
      "tags": [
        "ch13",
        "semi-supervised",
        "node-types"
      ]
    },
    {
      "uid": "13-017",
      "front": "What is a graph attention network (GAT)?",
      "back": "GNN where messages are <b>weighted by learned attention coefficients</b>:<br>\\( \\vec{h}_n^{(l+1)} = \\sum_m A_{nm} \\text{message}(\\vec{h}_m^{(l)}) \\)<br>Inductive bias: Some neighbors are more important than others.<br>With full connectivity, becomes similar to transformer.",
      "tags": [
        "ch13",
        "gat",
        "attention"
      ]
    },
    {
      "uid": "13-018",
      "front": "How do graph attention networks relate to transformers?",
      "back": "For a <b>fully-connected graph</b>, multi-head graph attention becomes equivalent to a <b>transformer encoder</b>.<br>Both use attention to weight contributions from other elements.<br>GAT generalizes attention to arbitrary graph structures.",
      "tags": [
        "ch13",
        "gat",
        "transformers"
      ]
    },
    {
      "uid": "13-019",
      "front": "What are general message-passing equations in GNNs?",
      "back": "Update rules that can maintain:<br><ul><li><b>Node embeddings</b> \\( \\vec{h}_n^{(l)} \\)</li><li><b>Edge embeddings</b> \\( \\vec{e}_{nm}^{(l)} \\)</li><li><b>Global embedding</b> \\( \\vec{g}^{(l)} \\)</li></ul>All three can be updated based on each other, enabling rich interactions.",
      "tags": [
        "ch13",
        "message-passing",
        "general"
      ]
    },
    {
      "uid": "13-020",
      "front": "What is over-smoothing in GNNs?",
      "back": "Problem where node embeddings become <b>very similar</b> after many layers.<br>Information propagates and mixes, losing node-specific features.<br>Mitigations:<br><ul><li>Residual connections</li><li>Fewer layers</li><li>Careful normalization</li></ul>",
      "tags": [
        "ch13",
        "over-smoothing",
        "problem"
      ]
    },
    {
      "uid": "13-021",
      "front": "How is dropout applied in GNNs?",
      "back": "<b>Randomly omitting edges</b> during training (edge dropout).<br>Different from standard dropout on activations.<br>Acts as regularization by encouraging robustness to missing connections.",
      "tags": [
        "ch13",
        "dropout",
        "regularization"
      ]
    },
    {
      "uid": "13-022",
      "front": "What is geometric deep learning?",
      "back": "A framework unifying neural networks that respect <b>geometric symmetries</b>:<br><ul><li>GNNs: Permutation symmetry</li><li>CNNs: Translation symmetry</li><li>Spherical CNNs: Rotation symmetry</li></ul>Design networks based on the symmetry group of the domain.",
      "tags": [
        "ch13",
        "geometric-deep-learning",
        "symmetry"
      ]
    }
  ]
}
